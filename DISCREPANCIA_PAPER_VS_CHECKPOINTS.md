# üîç Discrepancia Cr√≠tica: Paper vs Checkpoints

**Fecha:** 19 de Octubre, 2025  
**Estado:** üî¥ Discrepancia Confirmada

---

## üìã Resumen Ejecutivo

Hemos identificado una **discrepancia importante** entre las especificaciones arquitecturales publicadas en el paper y los pesos (checkpoints) reales del modelo XS.

### ‚ö° Hallazgo Principal

El modelo **XS** seg√∫n el paper deber√≠a usar backbone **Atto**, pero el checkpoint real contiene un backbone **Femto-L**.

---

## üìä Comparaci√≥n Detallada

### Modelo XS - Paper vs Checkpoint

| Aspecto | Paper (Tabla 5) | Checkpoint Real (snapshot_67) |
|---------|----------------|-------------------------------|
| **Backbone** | Atto (2,2,6,2) | Femto-L (3,3,9,3) |
| **Dims** | [40, 80, 160, 320] | [48, 96, 192, 384] |
| **Upsampling** | 3-UP [320, 128, 128] | 3-UP [384, 256, 256] |
| **Head Channels** | 128 | 256 |
| **Params** | 3.53M | ~7.45M |
| **MPJPE** | 56.61 mm | 56.61 mm (esperado) |

### Modelos S, M, L

‚úÖ **Sin discrepancias**: Todos coinciden con las especificaciones del paper (Femto-L backbone).

---

## üî¨ Evidencia del An√°lisis de Checkpoint

### An√°lisis de State Dict (snapshot_67.pth)

```python
# Evidencia 1: Dimensiones del primer stage del backbone
Key: module.backbone.stages.0.0.dwconv.weight
Shape: torch.Size([48, 1, 7, 7])  # 48 channels
Paper esperaba: [40, 1, 7, 7]     # 40 channels para Atto

# Evidencia 2: N√∫mero de bloques en stage 2
Checkpoint tiene: stages.2.0 hasta stages.2.8  (9 bloques)
Paper Atto esperaba: 6 bloques
Paper Femto-L especifica: 9 bloques ‚úÖ

# Evidencia 3: Final layer del head
Key: module.head.final_layer.weight
Shape: torch.Size([2304, 64, 1, 1])
C√°lculo: 2304 / 64 = 36 joints
Input desde √∫ltima capa deconv: 64 channels

Paper Atto esperaba: entrada desde 128 channels
Checkpoint muestra: entrada desde 256 channels (3x256=768 ‚Üí 64 ‚Üí final)
```

### Error al Intentar Cargar con Config del Paper

```python
RuntimeError: Error(s) in loading state_dict for DataParallel:
    size mismatch for module.backbone.stages.0.0.dwconv.weight: 
    copying a param with shape torch.Size([48, 1, 7, 7]) from checkpoint, 
    the shape in current model is torch.Size([40, 1, 7, 7]).
    
    Missing key(s) in state_dict: "module.backbone.stages.2.6.*", 
                                   "module.backbone.stages.2.7.*",
                                   "module.backbone.stages.2.8.*"
```

---

## üéØ Tabla del Paper (Referencia)

**Tabla 5 del Paper:**

| Modelo | Backbone | Upsampling | Params | MPJPE |
|--------|----------|------------|--------|-------|
| **XS** | **Atto (2,2,6,2)** | **3-UP [320,128,128]** | **3.53M** | **56.61mm** |
| S | Femto-L (3,3,9,3) | 3-UP [384,256,256] | 7.45M | 51.80mm |
| M | Femto-L (3,3,9,3) | 3-UP [384,256,256] | 7.60M | 51.05mm |
| L | Femto-L (3,3,9,3) | 3-UP [384,512,512] | 8.39M | 49.75mm |

---

## üí° Configuraciones Correctas

### Para Usar con Checkpoints Proporcionados

```python
# XS - Configuraci√≥n que FUNCIONA con snapshot_67.pth
cfg.backbone = 'Femto-L'  # NO 'Atto'
cfg.backbone_cfg = ([3,3,9,3], [48,96,192,384])  # NO ([2,2,6,2], [40,80,160,320])
cfg.depth = 256  # NO 128
cfg.depth_dim = 64
cfg.head_cfg = None  # Modo Legacy
```

### Para Replicar Paper (Requerir√≠a Re-entrenar)

```python
# XS - Configuraci√≥n seg√∫n Paper
cfg.backbone = 'Atto'
cfg.backbone_cfg = ([2,2,6,2], [40,80,160,320])
cfg.depth = 128
cfg.depth_dim = 64
cfg.head_cfg = None
```

---

## ü§î Posibles Explicaciones

### 1. **Error en Nomenclatura de Checkpoints**
   - El archivo `snapshot_67.pth` podr√≠a no corresponder al modelo XS del paper
   - Posible confusi√≥n en el proceso de release de modelos
   - **Probabilidad:** Media

### 2. **Optimizaci√≥n Post-Publicaci√≥n**
   - El modelo XS podr√≠a haber sido mejorado despu√©s de la publicaci√≥n
   - Upgrade de Atto ‚Üí Femto-L para mejor rendimiento
   - El MPJPE se mantiene similar (56.61mm)
   - **Probabilidad:** Media-Alta

### 3. **Versi√≥n de Desarrollo vs Publicaci√≥n**
   - Los checkpoints podr√≠an ser de una versi√≥n de desarrollo diferente
   - Paper documenta arquitectura final, checkpoints son versi√≥n intermedia
   - **Probabilidad:** Baja

### 4. **Configuraci√≥n Modificada Durante Entrenamiento**
   - Posible switch de Atto ‚Üí Femto-L durante fine-tuning
   - Checkpoint final refleja arquitectura modificada
   - **Probabilidad:** Media

---

## üìà Impacto en Resultados

### Par√°metros del Modelo

```
Paper XS:          3.53M params (Atto)
Checkpoint XS:    ~7.45M params (Femto-L)
Diferencia:       +3.92M params (+111%)
```

### Rendimiento Esperado

- **MPJPE**: Ambos deber√≠an lograr ~56.61mm seg√∫n paper
- **Capacidad**: Femto-L tiene mayor capacidad de representaci√≥n
- **Velocidad**: Atto ser√≠a m√°s r√°pido (menos params)

---

## ‚úÖ Estrategia de Testing Recomendada

### Enfoque Dual

```python
# 1. Intentar primero con configuraci√≥n del paper
try:
    model = create_model_atto()  # Seg√∫n paper
    load_checkpoint('snapshot_67.pth')
except RuntimeError:
    # 2. Si falla, usar configuraci√≥n de checkpoint
    model = create_model_femto_l()  # Seg√∫n an√°lisis
    load_checkpoint('snapshot_67.pth')  # ‚úÖ Funciona
```

### Documentar Resultados

Para cada modelo, registrar:
- ‚úÖ Configuraci√≥n que funcion√≥ (paper o checkpoint)
- ‚úÖ N√∫mero de par√°metros real
- ‚úÖ MPJPE obtenido vs esperado
- ‚úÖ Diferencias arquitecturales identificadas

---

## üîß Script de Verificaci√≥n

```python
def verify_checkpoint_architecture(checkpoint_path):
    """
    Verifica la arquitectura real contenida en un checkpoint
    """
    checkpoint = torch.load(checkpoint_path, map_location='cpu')
    state_dict = checkpoint['network']
    
    # Verificar dimensiones del backbone
    first_stage_key = 'module.backbone.stages.0.0.dwconv.weight'
    if first_stage_key in state_dict:
        dims = state_dict[first_stage_key].shape[0]
        if dims == 40:
            print("‚úÖ Checkpoint contiene Atto backbone")
        elif dims == 48:
            print("‚ö†Ô∏è  Checkpoint contiene Femto-L backbone")
        else:
            print(f"‚ùì Dimensi√≥n desconocida: {dims}")
    
    # Verificar n√∫mero de bloques en stage 2
    stage2_blocks = [k for k in state_dict.keys() if 'stages.2.' in k]
    max_block = max([int(k.split('.')[3]) for k in stage2_blocks if k.split('.')[3].isdigit()])
    print(f"   Stage 2 tiene {max_block + 1} bloques")
    if max_block + 1 == 6:
        print("   ‚úÖ Coincide con Atto (2,2,6,2)")
    elif max_block + 1 == 9:
        print("   ‚ö†Ô∏è  Coincide con Femto-L (3,3,9,3)")
    
    # Verificar head channels
    final_layer_key = 'module.head.final_layer.weight'
    if final_layer_key in state_dict:
        in_channels = state_dict[final_layer_key].shape[1]
        print(f"   Head input channels: {in_channels}")
        if in_channels == 32:  # 128 / 4 (upsampling)
            print("   ‚úÖ Coincide con depth=128")
        elif in_channels == 64:  # 256 / 4 (upsampling)
            print("   ‚ö†Ô∏è  Coincide con depth=256")

# Uso
verify_checkpoint_architecture('snapshot_67.pth')
```

---

## üìù Recomendaciones

### Para Testing Inmediato

1. ‚úÖ **Usar configuraciones de checkpoint** (Femto-L para XS)
2. ‚úÖ **Documentar discrepancias** encontradas
3. ‚úÖ **Comparar MPJPE** obtenido vs paper

### Para Investigaci√≥n Futura

1. üìß **Contactar autores** para clarificar nomenclatura
2. üîç **Verificar otros checkpoints** (S, M, L) por consistencia
3. üìä **Comparar rendimiento** Atto vs Femto-L para XS
4. üèóÔ∏è **Re-entrenar XS con Atto** seg√∫n especificaciones originales

### Para Documentaci√≥n

1. üìù Actualizar README con configuraciones reales
2. ‚ö†Ô∏è  Agregar warnings sobre discrepancias
3. üìä Incluir tabla comparativa en gu√≠as
4. üîó Referenciar este documento en gu√≠as principales

---

## üìö Referencias

- **Paper Original**: ConvNeXtPose (Tabla 5)
- **Checkpoints**: snapshot_67.pth (XS), snapshot_68.pth (S), etc.
- **An√°lisis Previo**: `HALLAZGO_ARQUITECTURAS_REALES.md`
- **Configuraci√≥n C√≥digo**: `main/config.py`, `main/model.py`

---

## üéØ Conclusi√≥n

La discrepancia entre paper y checkpoints es **real y verificable**. Para trabajar con los checkpoints proporcionados, debemos usar configuraciones que difieren del paper en el caso del modelo XS.

**Estrategia recomendada:**
1. Probar primero con config del paper
2. Si falla, usar config encontrada en checkpoint
3. Documentar qu√© configuraci√≥n funcion√≥
4. Reportar hallazgos a los autores

---

**Documentado por:** An√°lisis t√©cnico de checkpoints  
**Verificado con:** PyTorch state_dict inspection  
**Fecha:** 19 de Octubre, 2025  
**Estado:** Pendiente de confirmaci√≥n por autores
