# ğŸ“Š GuÃ­a RÃ¡pida: Paper vs Checkpoints

**Ãšltima actualizaciÃ³n:** 19 de Octubre, 2025

---

## ğŸ¯ InformaciÃ³n Clave del Paper

SegÃºn la **Tabla 5** del paper de ConvNeXtPose:

| Modelo | Backbone | Dims Backbone | Upsampling | Head Channels | Params | MPJPE |
|--------|----------|---------------|------------|---------------|--------|-------|
| **XS** | **Atto** | **(2,2,6,2)** <br> **[40,80,160,320]** | **3-UP** <br> **[320,128,128]** | **128** | **3.53M** | **56.61mm** |
| **S** | Femto-L | (3,3,9,3) <br> [48,96,192,384] | 3-UP <br> [384,256,256] | 256 | 7.45M | 51.80mm |
| **M** | Femto-L | (3,3,9,3) <br> [48,96,192,384] | 3-UP <br> [384,256,256] | 256 | 7.60M | 51.05mm |
| **L** | Femto-L | (3,3,9,3) <br> [48,96,192,384] | 3-UP <br> [384,512,512] | 512 | 8.39M | 49.75mm |

---

## âš ï¸ Discrepancia Descubierta: Modelo XS

### Lo Que Encontramos

Al analizar el checkpoint `snapshot_67.pth` (que deberÃ­a ser XS), descubrimos:

```python
# Checkpoint snapshot_67.pth contiene:
Backbone: Femto-L (3,3,9,3)  â† NO es Atto
Dims: [48, 96, 192, 384]     â† NO es [40,80,160,320]
Head: depth=256              â† NO es depth=128
Params: ~7.45M               â† NO es 3.53M
```

### Evidencia del AnÃ¡lisis

```python
# VerificaciÃ³n de state_dict:
checkpoint['network']['module.backbone.stages.0.0.dwconv.weight'].shape
# Resultado: torch.Size([48, 1, 7, 7])
# Paper Atto esperaba: torch.Size([40, 1, 7, 7])

# VerificaciÃ³n de bloques:
checkpoint tiene stages.2.0 hasta stages.2.8  (9 bloques)
# Paper Atto esperaba: stages.2.0 hasta stages.2.5 (6 bloques)
# Femto-L especifica: 9 bloques âœ…
```

---

## ğŸ”§ Configuraciones para CÃ³digo

### ConfiguraciÃ³n 1: SegÃºn Paper (XS = Atto)

```python
# Para modelo XS segÃºn paper
cfg.backbone = 'Atto'
cfg.backbone_cfg = ([2, 2, 6, 2], [40, 80, 160, 320])
cfg.depth = 128  # Head channels
cfg.depth_dim = 64
cfg.head_cfg = None  # Modo Legacy

# Esta configuraciÃ³n:
# âœ… Coincide con paper
# âŒ NO funciona con snapshot_67.pth
```

### ConfiguraciÃ³n 2: SegÃºn Checkpoint (XS = Femto-L)

```python
# Para modelo XS segÃºn checkpoint real
cfg.backbone = 'Femto-L'
cfg.backbone_cfg = ([3, 3, 9, 3], [48, 96, 192, 384])
cfg.depth = 256  # Head channels
cfg.depth_dim = 64
cfg.head_cfg = None  # Modo Legacy

# Esta configuraciÃ³n:
# âŒ NO coincide con paper
# âœ… Funciona con snapshot_67.pth
```

### Configuraciones S, M, L (Sin Discrepancias)

```python
# Modelo S
cfg.backbone = 'Femto-L'
cfg.backbone_cfg = ([3, 3, 9, 3], [48, 96, 192, 384])
cfg.depth = 256
cfg.depth_dim = 64
cfg.head_cfg = None

# Modelo M
cfg.backbone = 'Femto-L'
cfg.backbone_cfg = ([3, 3, 9, 3], [48, 96, 192, 384])
cfg.depth = 256  # Igual que S
cfg.depth_dim = 64
cfg.head_cfg = None

# Modelo L
cfg.backbone = 'Femto-L'
cfg.backbone_cfg = ([3, 3, 9, 3], [48, 96, 192, 384])
cfg.depth = 512  # â† Diferencia principal con S/M
cfg.depth_dim = 64
cfg.head_cfg = None
```

---

## ğŸ“Š ComparaciÃ³n Visual

### Arquitectura XS: Paper vs Checkpoint

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ MODELO XS - PAPER                                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Input (256x256x3)                                       â”‚
â”‚    â†“                                                    â”‚
â”‚ [Atto Backbone]                                         â”‚
â”‚  Stage 0: 2 blocks Ã— 40 channels                       â”‚
â”‚  Stage 1: 2 blocks Ã— 80 channels                       â”‚
â”‚  Stage 2: 6 blocks Ã— 160 channels                      â”‚
â”‚  Stage 3: 2 blocks Ã— 320 channels                      â”‚
â”‚    â†“                                                    â”‚
â”‚ [Head - 3 Deconv Layers]                               â”‚
â”‚  Deconv 1: 320 channels                                â”‚
â”‚  Deconv 2: 128 channels                                â”‚
â”‚  Deconv 3: 128 channels                                â”‚
â”‚    â†“                                                    â”‚
â”‚ Output (64x64 heatmaps)                                â”‚
â”‚                                                         â”‚
â”‚ Total Params: 3.53M                                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ MODELO XS - CHECKPOINT REAL                             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Input (256x256x3)                                       â”‚
â”‚    â†“                                                    â”‚
â”‚ [Femto-L Backbone]                                      â”‚
â”‚  Stage 0: 3 blocks Ã— 48 channels  â† MÃ¡s bloques        â”‚
â”‚  Stage 1: 3 blocks Ã— 96 channels  â† MÃ¡s bloques        â”‚
â”‚  Stage 2: 9 blocks Ã— 192 channels â† MÃ¡s bloques        â”‚
â”‚  Stage 3: 3 blocks Ã— 384 channels â† MÃ¡s bloques        â”‚
â”‚    â†“                                                    â”‚
â”‚ [Head - 3 Deconv Layers]                               â”‚
â”‚  Deconv 1: 384 channels   â† MÃ¡s canales                â”‚
â”‚  Deconv 2: 256 channels   â† MÃ¡s canales                â”‚
â”‚  Deconv 3: 256 channels   â† MÃ¡s canales                â”‚
â”‚    â†“                                                    â”‚
â”‚ Output (64x64 heatmaps)                                â”‚
â”‚                                                         â”‚
â”‚ Total Params: ~7.45M  â† Casi el doble                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ¤” Â¿Por QuÃ© Esta Discrepancia?

### TeorÃ­as Posibles

1. **Error en Nomenclatura**
   - El checkpoint `snapshot_67.pth` podrÃ­a no ser el modelo XS original
   - Posible confusiÃ³n al subir los modelos

2. **OptimizaciÃ³n Post-Paper**
   - El modelo XS pudo ser mejorado despuÃ©s de la publicaciÃ³n
   - Upgrade de Atto â†’ Femto-L para mejor accuracy
   - El MPJPE se mantiene ~56.61mm

3. **VersiÃ³n de Desarrollo**
   - Los checkpoints podrÃ­an ser de una versiÃ³n diferente
   - Paper documenta versiÃ³n final, checkpoints son versiÃ³n intermedia

4. **ConfiguraciÃ³n Durante Training**
   - Posible cambio de arquitectura durante fine-tuning
   - Checkpoint refleja configuraciÃ³n modificada

---

## âœ… QuÃ© Hacer en la PrÃ¡ctica

### Estrategia Recomendada

El notebook de benchmark ahora implementa una **estrategia dual**:

```python
# Para modelo XS:
try:
    # Intento 1: ConfiguraciÃ³n del paper (Atto)
    load_model_with_config_paper()
except RuntimeError:
    # Intento 2: ConfiguraciÃ³n de checkpoint (Femto-L)
    load_model_with_config_checkpoint()
    # âš ï¸ Reportar discrepancia

# Para S, M, L:
# Usar configuraciÃ³n del paper directamente
```

### Pasos a Seguir

1. âœ… **Ejecutar benchmark** con estrategia dual
2. ğŸ“ **Documentar** quÃ© configuraciÃ³n funcionÃ³ para XS
3. ğŸ“Š **Comparar MPJPE** obtenido vs paper (56.61mm)
4. ğŸ“§ **Considerar contactar** autores si discrepancia confirmada

---

## ğŸ“ˆ Resultados Esperados

### Si XS Usa Config del Paper (Atto)

```
âœ… Arquitectura coincide con documentaciÃ³n
âœ… Params: 3.53M
âœ… MPJPE: ~56.61mm
âœ… Todo como se esperaba
```

### Si XS Usa Config de Checkpoint (Femto-L)

```
âš ï¸ Arquitectura difiere del paper
âš ï¸ Params: ~7.45M (111% mÃ¡s)
âœ… MPJPE: ~56.61mm (accuracy preservada)
âš ï¸ Discrepancia confirmada
```

---

## ğŸ” CÃ³mo Verificar TÃº Mismo

### Script de VerificaciÃ³n RÃ¡pida

```python
import torch

# Cargar checkpoint
checkpoint = torch.load('snapshot_67.pth', map_location='cpu')
sd = checkpoint['network']

# Verificar primera capa del backbone
first_conv = sd['module.backbone.stages.0.0.dwconv.weight']
print(f"Primera capa dims: {first_conv.shape[0]}")
# Si es 40 â†’ Atto âœ…
# Si es 48 â†’ Femto-L âš ï¸

# Verificar bloques en stage 2
stage2_keys = [k for k in sd.keys() if 'stages.2.' in k]
max_block = max([int(k.split('.')[3]) for k in stage2_keys 
                 if k.split('.')[3].isdigit()])
print(f"Stage 2 tiene {max_block + 1} bloques")
# Si es 6 â†’ Atto âœ…
# Si es 9 â†’ Femto-L âš ï¸

# Verificar head input channels
final_layer = sd['module.head.final_layer.weight']
print(f"Head input: {final_layer.shape[1]} channels")
# Si es 32 â†’ depth=128 âœ…
# Si es 64 â†’ depth=256 âš ï¸
```

---

## ğŸ“š Documentos Relacionados

- ğŸ“„ **DISCREPANCIA_PAPER_VS_CHECKPOINTS.md** - AnÃ¡lisis completo
- ğŸ““ **convnextpose (5).ipynb** - Benchmark con estrategia dual
- ğŸ“‹ **GUIA_COMPLETA_ACTUALIZADA.md** - Configuraciones verificadas
- ğŸ“Š **RESUMEN_ACTUALIZACION_DOCS.md** - Resumen de arquitecturas

---

## ğŸ¯ ConclusiÃ³n

**Para el modelo XS:** Existe una discrepancia entre paper y checkpoint que debe ser verificada mediante testing.

**Para S, M, L:** No hay discrepancias, usar configuraciones del paper directamente.

**Siguiente paso:** Ejecutar el benchmark y ver quÃ© configuraciÃ³n funciona para XS.

---

**Creado:** 19 de Octubre, 2025  
**Basado en:** AnÃ¡lisis de checkpoint snapshot_67.pth y Tabla 5 del paper  
**Estado:** Pendiente de validaciÃ³n experimental
