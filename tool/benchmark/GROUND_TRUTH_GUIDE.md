# Ground Truth en ConvNeXtPose: GuÃ­a Completa

## ğŸ¯ Â¿QuÃ© es Ground Truth?

**Ground Truth (GT)** = Poses 3D reales capturadas con Motion Capture (MoCap)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Ground Truth: La "verdad absoluta"                         â”‚
â”‚  - Capturado con sistemas MoCap profesionales               â”‚
â”‚  - PrecisiÃ³n submilimÃ©trica                                 â”‚
â”‚  - Sirve como referencia para evaluar modelos               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â†“
         pred = model(imagen)
         error = ||pred - GT||  â† MPJPE
```

## ğŸ“¦ Â¿De DÃ³nde Viene el Ground Truth?

### En Human3.6M:

```
data/Human36M/
â”œâ”€â”€ annotations/
â”‚   â”œâ”€â”€ Human36M_subject9_joint_3d.json   â† GROUND TRUTH (MoCap)
â”‚   â”œâ”€â”€ Human36M_subject9_camera.json     â† ParÃ¡metros de cÃ¡mara
â”‚   â”œâ”€â”€ Human36M_subject9_data.json       â† Anotaciones COCO
â”‚   â”œâ”€â”€ Human36M_subject11_joint_3d.json  â† GROUND TRUTH (MoCap)
â”‚   â””â”€â”€ ...
â””â”€â”€ images/
    â””â”€â”€ s_*.jpg                            â† Fotos del dataset
```

**Formato del JSON:**
```json
{
  "1": {              // action_idx
    "1": {            // subaction_idx
      "12345": [      // frame_idx
        [x, y, z],    // Pelvis (joint 0)
        [x, y, z],    // R_Hip (joint 1)
        ...           // 17 joints total
      ]
    }
  }
}
```

## âœ… El Repo Ya Carga el GT AutomÃ¡ticamente

**NO necesitas crear archivos NPZ manualmente.** El cÃ³digo ya lo hace:

```python
# En data/Human36M/Human36M.py:

class Human36M:
    def load_data(self):
        # 1. Carga GT desde JSON
        with open('Human36M_subject9_joint_3d.json') as f:
            joints[str(subject)] = json.load(f)  # â† GT aquÃ­
        
        # 2. Para cada frame, extrae GT
        joint_world = np.array(joints[str(subject)][action][subaction][frame])
        joint_cam = world2cam(joint_world, R, t)  # Transforma a coords cÃ¡mara
        
        # 3. Guarda en self.data
        data.append({
            'joint_cam': joint_cam,  # â† GT en (17, 3) mm
            'img_path': ...,
            ...
        })
        
        return data

    def evaluate(self, preds, result_dir):
        for n in range(len(preds)):
            gt_3d_kpt = self.data[n]['joint_cam']  # â† GT cargado automÃ¡ticamente
            pred_3d_kpt = preds[n]
            
            # Calcular error
            error[n] = np.sqrt(np.sum((pred_3d_kpt - gt_3d_kpt)**2, 1))  # MPJPE
```

## ğŸ”„ Flujo Completo

### Durante ENTRENAMIENTO:
```python
dataset = Human36M(data_split='train')
for sample in dataset:
    image = load_image(sample['img_path'])
    gt_3d = sample['joint_cam']  # â† GT del JSON
    
    pred = model(image)
    loss = mse(pred, gt_3d)  # Entrenar contra GT
    loss.backward()
```

### Durante TESTING:
```python
dataset = Human36M(data_split='test')  # S9, S11, stride=64
preds = []

for sample in dataset:
    image = load_image(sample['img_path'])
    pred = model(image)
    preds.append(pred)

# Evaluar contra GT (ya cargado en dataset.data)
mpjpe = dataset.evaluate(preds, output_dir)
```

### Durante BENCHMARK:
```python
# El benchmark runner usa el mismo flujo:
from tool.benchmark.runner_params import prep_data_from_dataset

# Carga GT automÃ¡ticamente desde JSON
gt_3d, meta = prep_data_from_dataset("Human3.6M", protocol=2)
# gt_3d: (N, 17, 3) array en mm
# meta: info del dataset

# Compara mÃºltiples modelos contra el mismo GT
for model_name in ['rootnet', 'convnextpose', 'integral_pose']:
    pred = model.infer(images)
    mpjpe = compute_mpjpe(pred, gt_3d)  # â† ComparaciÃ³n contra GT
```

## â“ Preguntas Frecuentes

### Q: Â¿Necesito crear un NPZ con ground truth?
**A:** âŒ No. El GT ya estÃ¡ en los archivos JSON y se carga automÃ¡ticamente.

### Q: Â¿QuÃ© es el archivo `all_3d_poses.npz` en el repo?
**A:** Son **predicciones** del modelo ConvNeXtPose sobre un video. NO es ground truth.

```python
# all_3d_poses.npz contiene:
{
  'poses': (123, 18, 3),  # Predicciones del modelo (no GT)
  'depths': (123,),        # Profundidades predichas
  'frames': (123,)         # NÃºmeros de frame del video
}
```

### Q: Â¿CuÃ¡ndo necesito ground truth?
**A:** 
- âœ… Entrenar el modelo â†’ necesitas GT
- âœ… Evaluar en dataset â†’ necesitas GT
- âœ… Benchmark multi-modelo â†’ necesitas GT
- âŒ Inferencia en video nuevo â†’ NO necesitas GT
- âŒ Demo visual â†’ NO necesitas GT

### Q: Â¿Se borra el GT despuÃ©s de testing?
**A:** âŒ No. El GT es parte permanente del dataset (archivos JSON).

Lo que SÃ se puede borrar:
```
output/result/bbox_root_pose_human36m_output.json  â† Predicciones
output/vis/                                        â† Visualizaciones
output/log/                                        â† Logs
```

### Q: Â¿DÃ³nde estÃ¡n los archivos JSON con GT?
**A:** Debes descargar el dataset Human3.6M completo de:
- http://vision.imar.ro/human3.6m/
- Seguir las instrucciones en `README.md` del repo

### Q: Â¿Puedo usar solo imÃ¡genes sin GT?
**A:** SÃ­, pero **solo para inferencia** (demo/visualizaciÃ³n). No podrÃ¡s calcular MPJPE.

## ğŸ› ï¸ Herramientas Incluidas

### 1. Verificar si tienes los archivos JSON:
```bash
python3 tool/benchmark/extract_gt.py --verify
```

Salida esperada si tienes el dataset:
```
âœ“ Found annotation directory: data/Human36M/annotations
âœ“ Human36M_subject9_joint_3d.json    (XX.XX MB) - Ground Truth 3D
âœ“ Human36M_subject11_joint_3d.json   (XX.XX MB) - Ground Truth 3D
```

### 2. Extraer estadÃ­sticas del GT (opcional):
```bash
python3 tool/benchmark/extract_gt.py --protocol 2
```

Muestra:
- NÃºmero de samples
- Shape del GT: (N, 17, 3)
- Rango de valores (mm)
- EstadÃ­sticas por joint

### 3. Guardar GT en NPZ (opcional, no recomendado):
```bash
python3 tool/benchmark/extract_gt.py --protocol 2 --save --out gt_p2.npz
```

**Nota:** Esto es opcional. El benchmark carga GT directamente del JSON.

## ğŸ“Š Para Benchmark en Kaggle

### OpciÃ³n A: Dataset Completo (Recomendado)
```python
# 1. Sube Human3.6M completo como Kaggle Dataset
#    Incluye: images/ y annotations/ (con los JSON de GT)

# 2. En el notebook:
from tool.benchmark.runner_params import prep_data_from_dataset

gt_3d, meta = prep_data_from_dataset("Human3.6M", protocol=2)
# â†‘ Carga GT automÃ¡ticamente desde los JSON
```

### OpciÃ³n B: Subset Pre-procesado (MÃ¡s RÃ¡pido)
```python
# 1. Local: extrae subset con GT
python3 tool/benchmark/extract_gt.py --protocol 2 --save --out gt_h36m_p2_subset.npz

# 2. Sube solo el NPZ a Kaggle Dataset (mucho mÃ¡s ligero)

# 3. En notebook:
from tool.benchmark.runner_params import prep_data_from_npz

gt_3d, meta = prep_data_from_npz('gt_h36m_p2_subset.npz')
```

## ğŸ”‘ Resumen Clave

| Aspecto | Respuesta |
|---------|-----------|
| **Â¿QuÃ© es GT?** | Poses 3D reales de MoCap (referencia perfecta) |
| **Â¿DÃ³nde estÃ¡?** | `data/Human36M/annotations/*_joint_3d.json` |
| **Â¿Necesito NPZ?** | âŒ No, se carga automÃ¡ticamente del JSON |
| **Â¿CuÃ¡ndo lo necesito?** | Entrenar, evaluar, benchmark (NO para demo) |
| **Â¿Se borra post-test?** | âŒ No, es parte permanente del dataset |
| **Â¿all_3d_poses.npz es GT?** | âŒ No, son predicciones de un video |
| **Â¿CÃ³mo lo usa el repo?** | `dataset.data[i]['joint_cam']` automÃ¡ticamente |

## ğŸ“š Referencias en el CÃ³digo

- Carga de GT: `data/Human36M/Human36M.py:86` (lÃ­nea `json.load()`)
- Uso en evaluaciÃ³n: `data/Human36M/Human36M.py:160` (funciÃ³n `evaluate()`)
- Carga automÃ¡tica en benchmark: `tool/benchmark/runner_params.py:prep_data_from_dataset()`
- Formato MPJPE: `tool/benchmark/metrics.py:mpjpe()`
