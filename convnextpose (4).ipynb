{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":5116687,"sourceType":"datasetVersion","datasetId":2200515},{"sourceId":13341353,"sourceType":"datasetVersion","datasetId":8460258}],"dockerImageVersionId":30839,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# üéØ ConvNeXtPose Testing en Kaggle - Modelos L y M\n\nEste notebook eval√∫a los modelos ConvNeXtPose L y M en Human3.6M Protocol 2.\n\n**Datasets requeridos:**\n- Human3.6M Dataset (con S9_ACT2_16, S11_ACT2_16, annotations)\n- ConvNeXtPose Pre-trained Models (checkpoints .tar)\n\n**GPU recomendada:** T4 x2 o P100","metadata":{}},{"cell_type":"markdown","source":"## üì¶ PASO 1: Setup Inicial","metadata":{}},{"cell_type":"code","source":"!pwd","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-14T16:45:35.465747Z","iopub.execute_input":"2025-10-14T16:45:35.466043Z","iopub.status.idle":"2025-10-14T16:45:35.582735Z","shell.execute_reply.started":"2025-10-14T16:45:35.466020Z","shell.execute_reply":"2025-10-14T16:45:35.581668Z"}},"outputs":[{"name":"stdout","text":"/kaggle/working\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"#!rm -r ConvNeXtPose ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-14T16:45:52.497540Z","iopub.execute_input":"2025-10-14T16:45:52.497867Z","iopub.status.idle":"2025-10-14T16:45:52.857792Z","shell.execute_reply.started":"2025-10-14T16:45:52.497837Z","shell.execute_reply":"2025-10-14T16:45:52.856598Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"# Clonar repositorio\n!git clone https://github.com/EstebanCabreraArbizu/ConvNeXtPose.git\n%cd ConvNeXtPose\n!git fetch origin\n\n# Verificar versiones\nimport torch\nprint(f\"PyTorch: {torch.__version__}\")\nprint(f\"CUDA disponible: {torch.cuda.is_available()}\")\nif torch.cuda.is_available():\n    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n    print(f\"Memoria GPU: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-14T16:46:28.543379Z","iopub.execute_input":"2025-10-14T16:46:28.543669Z","iopub.status.idle":"2025-10-14T16:46:45.055415Z","shell.execute_reply.started":"2025-10-14T16:46:28.543644Z","shell.execute_reply":"2025-10-14T16:46:45.054509Z"}},"outputs":[{"name":"stdout","text":"Cloning into 'ConvNeXtPose'...\nremote: Enumerating objects: 1166, done.\u001b[K\nremote: Counting objects: 100% (288/288), done.\u001b[K\nremote: Compressing objects: 100% (225/225), done.\u001b[K\nremote: Total 1166 (delta 103), reused 222 (delta 62), pack-reused 878 (from 1)\u001b[K\nReceiving objects: 100% (1166/1166), 318.73 MiB | 39.40 MiB/s, done.\nResolving deltas: 100% (341/341), done.\nUpdating files: 100% (207/207), done.\n/kaggle/working/ConvNeXtPose\nPyTorch: 2.5.1+cu121\nCUDA disponible: True\nGPU: Tesla T4\nMemoria GPU: 15.83 GB\n","output_type":"stream"}],"execution_count":3},{"cell_type":"markdown","source":"## üóÇÔ∏è PASO 2: Configurar Dataset Human3.6M\n\nUsamos enlaces simb√≥licos para evitar copiar ~30GB de datos.","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# IMPORTANTE: Ajusta este path al nombre real de tu dataset en Kaggle\nKAGGLE_DATASET_PATH = '/kaggle/input/human3-6m-for-convnextpose-and-3dmpee-pose-net'  # ‚Üê CAMBIAR SEG√öN TU DATASET\n\n# Verificar que el dataset existe\nimport os\nif not os.path.exists(KAGGLE_DATASET_PATH):\n    print(f\"‚ùå Dataset no encontrado en {KAGGLE_DATASET_PATH}\")\n    print(\"\\nüìÇ Datasets disponibles:\")\n    !ls /kaggle/input/\n    raise FileNotFoundError(\"Verifica el nombre del dataset y actualiza KAGGLE_DATASET_PATH\")\nelse:\n    print(f\"‚úì Dataset encontrado en {KAGGLE_DATASET_PATH}\")\n    print(\"\\nüìÇ Contenido:\")\n    !ls {KAGGLE_DATASET_PATH}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-14T16:46:45.056873Z","iopub.execute_input":"2025-10-14T16:46:45.057533Z","iopub.status.idle":"2025-10-14T16:46:45.198008Z","shell.execute_reply.started":"2025-10-14T16:46:45.057495Z","shell.execute_reply":"2025-10-14T16:46:45.197242Z"}},"outputs":[{"name":"stdout","text":"‚úì Dataset encontrado en /kaggle/input/human3-6m-for-convnextpose-and-3dmpee-pose-net\n\nüìÇ Contenido:\n'annotations (1)'   S11_ACT2_16   S9_ACT2_16\n","output_type":"stream"}],"execution_count":4},{"cell_type":"markdown","source":"### (Opcional) Diagnosticar Estructura del Dataset\n\nSi tienes dudas sobre la estructura de tu dataset, ejecuta esta celda primero para ver c√≥mo est√°n organizadas las carpetas.","metadata":{}},{"cell_type":"code","source":"# Diagnosticar estructura del dataset (√∫til para identificar carpetas anidadas)\n# !python diagnose_kaggle_dataset.py {KAGGLE_DATASET_PATH}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-14T16:46:45.199795Z","iopub.execute_input":"2025-10-14T16:46:45.200032Z","iopub.status.idle":"2025-10-14T16:46:45.203182Z","shell.execute_reply.started":"2025-10-14T16:46:45.200009Z","shell.execute_reply":"2025-10-14T16:46:45.202381Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"# Ejecutar script de configuraci√≥n\n# IMPORTANTE: Esto enlaza el dataset DENTRO de data/Human36M/, NO reemplaza data/\n# El script detecta autom√°ticamente carpetas anidadas (ej: annotations (1)/annotations/)\n!python setup_kaggle_dataset.py --kaggle-input {KAGGLE_DATASET_PATH} --project-root /kaggle/working/ConvNeXtPose\n\nprint(\"\\n‚úÖ Dataset enlazado en data/Human36M/\")\nprint(\"‚úÖ M√≥dulos Python originales intactos en data/\")\nprint(\"‚úÖ Carpetas anidadas detectadas autom√°ticamente\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-14T16:46:45.204536Z","iopub.execute_input":"2025-10-14T16:46:45.204788Z","iopub.status.idle":"2025-10-14T16:46:45.470004Z","shell.execute_reply.started":"2025-10-14T16:46:45.204768Z","shell.execute_reply":"2025-10-14T16:46:45.469225Z"}},"outputs":[{"name":"stdout","text":"\n======================================================================\n  Configuraci√≥n de Dataset Human3.6M para ConvNeXtPose\n======================================================================\nüìÇ Dataset Kaggle:     /kaggle/input/human3-6m-for-convnextpose-and-3dmpee-pose-net\nüìÇ Proyecto ConvNeXt:  /kaggle/working/ConvNeXtPose\n\n‚úì Directorio del proyecto encontrado: /kaggle/working/ConvNeXtPose/data/Human36M\n‚úì Manteniendo m√≥dulos Python originales en /kaggle/working/ConvNeXtPose/data\n\nüìÅ [1/3] Configurando annotations...\n  ‚úì Encontrado: annotations (1)/annotations\n    (21 archivos JSON detectados)\n  ‚úì Creado: annotations -> /kaggle/input/human3-6m-for-convnextpose-and-3dmpee-pose-net/annotations (1)/annotations\n\nüë• [2/3] Configurando sujetos S9 y S11...\n  ‚úì Creado: S9 -> /kaggle/input/human3-6m-for-convnextpose-and-3dmpee-pose-net/S9_ACT2_16\n  ‚úì Creado: S11 -> /kaggle/input/human3-6m-for-convnextpose-and-3dmpee-pose-net/S11_ACT2_16\n\nüì¶ [3/3] Configurando bbox_root...\n  ‚ö†Ô∏è  No se encontr√≥ bbox_root (opcional)\n\n======================================================================\n  ‚úÖ Configuraci√≥n Completada\n======================================================================\n\nüìÇ Estructura creada en: /kaggle/working/ConvNeXtPose/data/Human36M\n\nContenido:\n  üìÅ __pycache__/\n  üîó annotations -> annotations\n  üìÅ bbox_root/\n  üìÅ bbox_root/Subject 9,11 (trained on subject 1,5,6,7,8)/\n  üìÅ images/\n  üîó images/S11 -> S11_ACT2_16\n  üîó images/S9 -> S9_ACT2_16\n\n======================================================================\n  ‚úÖ LISTO - NO necesitas configurar CONVNEXPOSE_DATA_DIR\n======================================================================\n\n‚úÖ Los m√≥dulos Python originales est√°n intactos en:\n   /kaggle/working/ConvNeXtPose/data/dataset.py\n   /kaggle/working/ConvNeXtPose/data/Human36M/Human36M.py\n\n‚úÖ El dataset de Kaggle est√° enlazado en:\n   /kaggle/working/ConvNeXtPose/data/Human36M/images\n   /kaggle/working/ConvNeXtPose/data/Human36M/annotations\n   /kaggle/working/ConvNeXtPose/data/Human36M/bbox_root (si existe)\n\nüöÄ Puedes ejecutar el testing directamente:\n   %cd /kaggle/working/ConvNeXtPose/main\n   !python test.py --gpu 0 --epochs 70 --variant L\n\n\nüéâ Setup completado exitosamente!\n\nüí° Tip: Ejecuta con --verify para verificar la estructura:\n   !python setup_kaggle_dataset.py --verify /kaggle/working/ConvNeXtPose\n\n‚úÖ Dataset enlazado en data/Human36M/\n‚úÖ M√≥dulos Python originales intactos en data/\n‚úÖ Carpetas anidadas detectadas autom√°ticamente\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"from pathlib import Path\n\nimages_dir = Path('/kaggle/working/ConvNeXtPose/data/Human36M/images')\n\nfor subject in ['S9', 'S11']:\n    subj_dir = images_dir / subject\n    if not subj_dir.exists():\n        continue\n    for seq_dir in subj_dir.iterdir():\n        target = images_dir / seq_dir.name\n        if target.exists():\n            continue      # ya estaba creado\n        target.symlink_to(seq_dir, target_is_directory=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-14T16:46:45.471014Z","iopub.execute_input":"2025-10-14T16:46:45.471373Z","iopub.status.idle":"2025-10-14T16:46:45.527148Z","shell.execute_reply.started":"2025-10-14T16:46:45.471326Z","shell.execute_reply":"2025-10-14T16:46:45.526568Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"!find /kaggle/working/ConvNeXtPose/data/Human36M/images -maxdepth 1 -type l | head","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-14T16:46:45.527869Z","iopub.execute_input":"2025-10-14T16:46:45.528074Z","iopub.status.idle":"2025-10-14T16:46:45.657425Z","shell.execute_reply.started":"2025-10-14T16:46:45.528042Z","shell.execute_reply":"2025-10-14T16:46:45.656651Z"}},"outputs":[{"name":"stdout","text":"/kaggle/working/ConvNeXtPose/data/Human36M/images/s_09_act_05_subact_01_ca_04\n/kaggle/working/ConvNeXtPose/data/Human36M/images/s_09_act_13_subact_02_ca_02\n/kaggle/working/ConvNeXtPose/data/Human36M/images/s_09_act_15_subact_02_ca_03\n/kaggle/working/ConvNeXtPose/data/Human36M/images/s_09_act_10_subact_02_ca_02\n/kaggle/working/ConvNeXtPose/data/Human36M/images/s_09_act_02_subact_01_ca_04\n/kaggle/working/ConvNeXtPose/data/Human36M/images/s_11_act_16_subact_02_ca_01\n/kaggle/working/ConvNeXtPose/data/Human36M/images/s_11_act_08_subact_02_ca_04\n/kaggle/working/ConvNeXtPose/data/Human36M/images/s_11_act_06_subact_01_ca_01\n/kaggle/working/ConvNeXtPose/data/Human36M/images/s_11_act_12_subact_01_ca_02\n/kaggle/working/ConvNeXtPose/data/Human36M/images/s_11_act_03_subact_02_ca_03\nfind: ‚Äòstandard output‚Äô: Broken pipe\nfind: write error\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"# Verificar que la estructura es correcta\n!python setup_kaggle_dataset.py --verify /kaggle/working/ConvNeXtPose","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-14T16:46:45.658266Z","iopub.execute_input":"2025-10-14T16:46:45.658486Z","iopub.status.idle":"2025-10-14T16:46:45.869096Z","shell.execute_reply.started":"2025-10-14T16:46:45.658465Z","shell.execute_reply":"2025-10-14T16:46:45.868370Z"}},"outputs":[{"name":"stdout","text":"\n======================================================================\n  üîç Verificaci√≥n de Estructura\n======================================================================\n  ‚úì data/dataset.py\n  ‚úì data/Human36M/Human36M.py\n  ‚úì data/Human36M/annotations\n  ‚úì data/Human36M/images\n  ‚úì data/Human36M/images/S9\n  ‚úì data/Human36M/images/S11\n  ‚úì data/Human36M/bbox_root (optional)\n\n  ‚úÖ Estructura verificada correctamente\n  ‚úÖ M√≥dulos Python intactos en data/\n  ‚úÖ Dataset enlazado en data/Human36M/\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"# Verificar que la estructura es correcta\n!python verify_kaggle_structure.py","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-14T16:46:45.870061Z","iopub.execute_input":"2025-10-14T16:46:45.870293Z","iopub.status.idle":"2025-10-14T16:46:46.073253Z","shell.execute_reply.started":"2025-10-14T16:46:45.870271Z","shell.execute_reply":"2025-10-14T16:46:46.072470Z"}},"outputs":[{"name":"stdout","text":"\n======================================================================\n  üîç VERIFICACI√ìN R√ÅPIDA - Estructura del Proyecto\n======================================================================\n\nüìÑ M√≥dulos Python originales:\n  ‚úì data/dataset.py\n  ‚úì data/multiple_datasets.py\n  ‚úì data/Human36M/Human36M.py\n  ‚úì common/base.py\n  ‚úì main/config.py\n\nüìÇ Dataset de Kaggle (enlaces):\n  ‚úì data/Human36M/images/S9\n  ‚úì data/Human36M/images/S11\n  ‚úì data/Human36M/annotations\n  ‚úì data/Human36M/bbox_root\n\n======================================================================\n‚úÖ ESTRUCTURA CORRECTA - Listo para testing\n\nüöÄ Siguiente paso:\n   %cd /kaggle/working/ConvNeXtPose/main\n   !python test.py --gpu 0 --epochs 83 --variant L\n======================================================================\n\n","output_type":"stream"}],"execution_count":10},{"cell_type":"markdown","source":"## üéØ PASO 3: Preparar Checkpoints\n\nExtraer los modelos pre-entrenados.","metadata":{}},{"cell_type":"code","source":"!ls","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-14T16:46:46.075813Z","iopub.execute_input":"2025-10-14T16:46:46.076030Z","iopub.status.idle":"2025-10-14T16:46:46.198934Z","shell.execute_reply.started":"2025-10-14T16:46:46.076009Z","shell.execute_reply":"2025-10-14T16:46:46.198211Z"}},"outputs":[{"name":"stdout","text":"ARCHITECTURE_ADAPTATION_COMPLETE.md  KAGGLE_TESTING_GUIDE.md\nassets\t\t\t\t     kaggle_testing_notebook.ipynb\nAUTHOR_CONTACT_GUIDE.md\t\t     LICENSE\nCHECKLIST_TESTING.md\t\t     list_google_drive_contents.py\nCHECKPOINT_EXTRACTION_FIX.md\t     log2.txt\nCHECKPOINT_INVESTIGATION_REPORT.md   log.txt\nCHECKPOINT_MISLABELING_ISSUE.md      main\ncommon\t\t\t\t     NESTED_FOLDERS_SOLUTION.md\nCORRECCION_CONFIG_S.md\t\t     output\ndata\t\t\t\t     PASOS_TESTING.md\ndemo\t\t\t\t     PLAN_ACCION_INMEDIATO.md\ndiagnose_kaggle_dataset.py\t     quick_start.sh\nEMAIL_TEMPLATE_AUTHORS.md\t     README.md\nESTADO_PROYECTO.md\t\t     README_TESTING.md\nEXPLICACION_DIMS_INCORRECTOS.md      requirements.txt\nexports\t\t\t\t     RESUMEN_EJECUTIVO.md\nGITHUB_ISSUE_TEMPLATE.md\t     RESUMEN_RETESTING.md\nGUIA_TESTING_MODELOS_L_M.md\t     setup_kaggle_dataset.py\nidentify_model_variant.py\t     tool\nKAGGLE_DATASET_FIX.md\t\t     UBUNTU_QUICKSTART.md\nKAGGLE_EXECUTION_GUIDE.md\t     ubuntu_quickstart.sh\nKAGGLE_QUICK_SOLUTION.md\t     verify_kaggle_structure.py\nKAGGLE_QUICKSTART.md\t\t     vis\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"!pip install gdown","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-14T16:46:46.200943Z","iopub.execute_input":"2025-10-14T16:46:46.201220Z","iopub.status.idle":"2025-10-14T16:46:50.416654Z","shell.execute_reply.started":"2025-10-14T16:46:46.201195Z","shell.execute_reply":"2025-10-14T16:46:50.415623Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: gdown in /usr/local/lib/python3.10/dist-packages (5.2.0)\nRequirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from gdown) (4.12.3)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from gdown) (3.16.1)\nRequirement already satisfied: requests[socks] in /usr/local/lib/python3.10/dist-packages (from gdown) (2.32.3)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from gdown) (4.67.1)\nRequirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->gdown) (2.6)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (3.4.0)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (2.2.3)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (2024.12.14)\nRequirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (1.7.1)\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"import gdown\n\nfolder_id = \"12H7zkLvmJtrkCmAUAPkQ6788WAnO60gI\"\noutput = \"models_tar\"\n\ngdown.download_folder(id=folder_id, output=output, quiet=False, use_cookies=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-14T16:46:50.418192Z","iopub.execute_input":"2025-10-14T16:46:50.418560Z","iopub.status.idle":"2025-10-14T16:47:34.444852Z","shell.execute_reply.started":"2025-10-14T16:46:50.418522Z","shell.execute_reply":"2025-10-14T16:47:34.444173Z"}},"outputs":[{"name":"stderr","text":"Retrieving folder contents\n","output_type":"stream"},{"name":"stdout","text":"Processing file 1eIaMqTYG-30CuPULs9LzSfeouAzYYHQW ConvNeXtPose_L.tar\nProcessing file 1X_H-6S4xrQjW9GhJ3yWB-AXqUHddvkOI ConvNeXtPose_M.tar\nProcessing file 1OriQPQ3uRY8MWPHP9KnwPaKawzrontqH ConvNeXtPose_S.tar\nProcessing file 165D0rU2GImmRe7u7DNe6eKJG8voBIcGh ConvNeXtPose_XS.tar\n","output_type":"stream"},{"name":"stderr","text":"Retrieving folder contents completed\nBuilding directory structure\nBuilding directory structure completed\nDownloading...\nFrom: https://drive.google.com/uc?id=1eIaMqTYG-30CuPULs9LzSfeouAzYYHQW\nTo: /kaggle/working/ConvNeXtPose/models_tar/ConvNeXtPose_L.tar\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 101M/101M [00:02<00:00, 49.1MB/s] \nDownloading...\nFrom: https://drive.google.com/uc?id=1X_H-6S4xrQjW9GhJ3yWB-AXqUHddvkOI\nTo: /kaggle/working/ConvNeXtPose/models_tar/ConvNeXtPose_M.tar\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 91.3M/91.3M [00:01<00:00, 68.3MB/s]\nDownloading...\nFrom: https://drive.google.com/uc?id=1OriQPQ3uRY8MWPHP9KnwPaKawzrontqH\nTo: /kaggle/working/ConvNeXtPose/models_tar/ConvNeXtPose_S.tar\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 89.6M/89.6M [00:00<00:00, 110MB/s] \nDownloading...\nFrom: https://drive.google.com/uc?id=165D0rU2GImmRe7u7DNe6eKJG8voBIcGh\nTo: /kaggle/working/ConvNeXtPose/models_tar/ConvNeXtPose_XS.tar\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 42.5M/42.5M [00:00<00:00, 172MB/s]\nDownload completed\n","output_type":"stream"},{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"['models_tar/ConvNeXtPose_L.tar',\n 'models_tar/ConvNeXtPose_M.tar',\n 'models_tar/ConvNeXtPose_S.tar',\n 'models_tar/ConvNeXtPose_XS.tar']"},"metadata":{}}],"execution_count":13},{"cell_type":"code","source":"!ls","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-14T16:47:34.445619Z","iopub.execute_input":"2025-10-14T16:47:34.446033Z","iopub.status.idle":"2025-10-14T16:47:34.570001Z","shell.execute_reply.started":"2025-10-14T16:47:34.446009Z","shell.execute_reply":"2025-10-14T16:47:34.569036Z"}},"outputs":[{"name":"stdout","text":"ARCHITECTURE_ADAPTATION_COMPLETE.md  kaggle_testing_notebook.ipynb\nassets\t\t\t\t     LICENSE\nAUTHOR_CONTACT_GUIDE.md\t\t     list_google_drive_contents.py\nCHECKLIST_TESTING.md\t\t     log2.txt\nCHECKPOINT_EXTRACTION_FIX.md\t     log.txt\nCHECKPOINT_INVESTIGATION_REPORT.md   main\nCHECKPOINT_MISLABELING_ISSUE.md      models_tar\ncommon\t\t\t\t     NESTED_FOLDERS_SOLUTION.md\nCORRECCION_CONFIG_S.md\t\t     output\ndata\t\t\t\t     PASOS_TESTING.md\ndemo\t\t\t\t     PLAN_ACCION_INMEDIATO.md\ndiagnose_kaggle_dataset.py\t     quick_start.sh\nEMAIL_TEMPLATE_AUTHORS.md\t     README.md\nESTADO_PROYECTO.md\t\t     README_TESTING.md\nEXPLICACION_DIMS_INCORRECTOS.md      requirements.txt\nexports\t\t\t\t     RESUMEN_EJECUTIVO.md\nGITHUB_ISSUE_TEMPLATE.md\t     RESUMEN_RETESTING.md\nGUIA_TESTING_MODELOS_L_M.md\t     setup_kaggle_dataset.py\nidentify_model_variant.py\t     tool\nKAGGLE_DATASET_FIX.md\t\t     UBUNTU_QUICKSTART.md\nKAGGLE_EXECUTION_GUIDE.md\t     ubuntu_quickstart.sh\nKAGGLE_QUICK_SOLUTION.md\t     verify_kaggle_structure.py\nKAGGLE_QUICKSTART.md\t\t     vis\nKAGGLE_TESTING_GUIDE.md\n","output_type":"stream"}],"execution_count":14},{"cell_type":"code","source":"!ls src","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-14T16:47:34.570917Z","iopub.execute_input":"2025-10-14T16:47:34.571123Z","iopub.status.idle":"2025-10-14T16:47:34.692625Z","shell.execute_reply.started":"2025-10-14T16:47:34.571104Z","shell.execute_reply":"2025-10-14T16:47:34.691851Z"}},"outputs":[{"name":"stdout","text":"ls: cannot access 'src': No such file or directory\n","output_type":"stream"}],"execution_count":15},{"cell_type":"code","source":"!pwd","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-14T16:47:34.693548Z","iopub.execute_input":"2025-10-14T16:47:34.693756Z","iopub.status.idle":"2025-10-14T16:47:34.812981Z","shell.execute_reply.started":"2025-10-14T16:47:34.693736Z","shell.execute_reply":"2025-10-14T16:47:34.812254Z"}},"outputs":[{"name":"stdout","text":"/kaggle/working/ConvNeXtPose\n","output_type":"stream"}],"execution_count":16},{"cell_type":"code","source":"!ls -lh /kaggle/working/ConvNeXtPose/models_tar","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-14T16:47:34.813836Z","iopub.execute_input":"2025-10-14T16:47:34.814060Z","iopub.status.idle":"2025-10-14T16:47:34.935416Z","shell.execute_reply.started":"2025-10-14T16:47:34.814038Z","shell.execute_reply":"2025-10-14T16:47:34.934458Z"}},"outputs":[{"name":"stdout","text":"total 310M\n-rw-r--r-- 1 root root 97M Oct  4  2023 ConvNeXtPose_L.tar\n-rw-r--r-- 1 root root 88M Oct  4  2023 ConvNeXtPose_M.tar\n-rw-r--r-- 1 root root 86M Oct  4  2023 ConvNeXtPose_S.tar\n-rw-r--r-- 1 root root 41M Oct  4  2023 ConvNeXtPose_XS.tar\n","output_type":"stream"}],"execution_count":17},{"cell_type":"code","source":"import os\n# IMPORTANTE: Ajusta seg√∫n el nombre de tu dataset de modelos\nMODELS_DATASET_PATH = '/kaggle/working/ConvNeXtPose/models_tar'\n# Verificar modelos disponibles\nif os.path.exists(MODELS_DATASET_PATH):\n    print(\"üì¶ Modelos disponibles:\")\n    !ls -lh {MODELS_DATASET_PATH}\nelse:\n    print(f\"‚ùå Dataset de modelos no encontrado: {MODELS_DATASET_PATH}\")\n    print(\"\\nüìÇ Datasets disponibles:\")\n    !ls /kaggle/input/","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-14T16:47:34.936434Z","iopub.execute_input":"2025-10-14T16:47:34.936717Z","iopub.status.idle":"2025-10-14T16:47:35.059418Z","shell.execute_reply.started":"2025-10-14T16:47:34.936689Z","shell.execute_reply":"2025-10-14T16:47:35.058668Z"}},"outputs":[{"name":"stdout","text":"üì¶ Modelos disponibles:\ntotal 310M\n-rw-r--r-- 1 root root 97M Oct  4  2023 ConvNeXtPose_L.tar\n-rw-r--r-- 1 root root 88M Oct  4  2023 ConvNeXtPose_M.tar\n-rw-r--r-- 1 root root 86M Oct  4  2023 ConvNeXtPose_S.tar\n-rw-r--r-- 1 root root 41M Oct  4  2023 ConvNeXtPose_XS.tar\n","output_type":"stream"}],"execution_count":18},{"cell_type":"code","source":"import os\nimport zipfile\nimport shutil\nimport tarfile\nimport torch\n\n# Crear directorio para modelos\nos.makedirs('output/model_dump', exist_ok=True)\n\n# Funci√≥n para extraer y convertir checkpoints\ndef extract_checkpoint(tar_path, model_name, expected_epoch=None):\n    \"\"\"Extrae checkpoint desde .tar/.zip y lo convierte en archivo .pth v√°lido\n    \n    Los archivos .tar de ConvNeXtPose son ZIP con estructura de directorio en formato legacy.\n    Usamos una conversi√≥n simple: empaquetar el directorio como TAR que PyTorch puede leer.\n    \"\"\"\n    if not os.path.exists(tar_path):\n        print(f\"‚ö†Ô∏è  Modelo {model_name} no encontrado en {tar_path}\")\n        return None\n    \n    print(f\"üì¶ Extrayendo modelo {model_name} desde {tar_path}...\")\n    \n    # Verificar tama√±o del archivo\n    file_size = os.path.getsize(tar_path)\n    print(f\"   üìè Tama√±o del archivo: {file_size / (1024*1024):.2f} MB\")\n    \n    # Leer primeros bytes para diagnosticar el formato\n    with open(tar_path, 'rb') as f:\n        header = f.read(512)\n        print(f\"   üîç Primeros bytes (hex): {header[:50].hex()}\")\n    \n    # Si el archivo es muy peque√±o o empieza con HTML, es un error de descarga\n    if file_size < 1000 or header.startswith(b'<!DOCTYPE') or header.startswith(b'<html'):\n        print(f\"   ‚ùå ERROR: El archivo parece ser HTML (error de descarga)\")\n        print(f\"   üí° Soluci√≥n: Revisa que el folder de Google Drive sea p√∫blico\")\n        return None\n    \n    # Extraer a directorio temporal\n    temp_dir = 'output/model_dump/temp_extract'\n    os.makedirs(temp_dir, exist_ok=True)\n    \n    # Detectar formato: intentar ZIP primero, luego TAR\n    extracted = False\n    try:\n        with zipfile.ZipFile(tar_path, 'r') as zip_ref:\n            zip_ref.extractall(temp_dir)\n            files = zip_ref.namelist()\n            print(f\"   ‚úì Formato: ZIP - Extra√≠do: {len(files)} archivos\")\n            extracted = True\n    except zipfile.BadZipFile:\n        try:\n            with tarfile.open(tar_path, 'r') as tar_ref:\n                tar_ref.extractall(temp_dir)\n                files = tar_ref.getnames()\n                print(f\"   ‚úì Formato: TAR - Extra√≠do: {len(files)} archivos\")\n                extracted = True\n        except (tarfile.ReadError, Exception) as e:\n            print(f\"   ‚ùå ERROR: No se pudo extraer el archivo\")\n            print(f\"   üí° Formato no reconocido: {type(e).__name__}: {e}\")\n            shutil.rmtree(temp_dir, ignore_errors=True)\n            return None\n    \n    if not extracted:\n        shutil.rmtree(temp_dir, ignore_errors=True)\n        return None\n    \n    # Buscar la carpeta del checkpoint (snapshot_XX.pth/ o archive/)\n    found_checkpoints = []\n    for item in os.listdir(temp_dir):\n        item_path = os.path.join(temp_dir, item)\n        if os.path.isdir(item_path):\n            # Verificar que tenga data.pkl (indicador de checkpoint PyTorch)\n            if os.path.exists(os.path.join(item_path, 'data.pkl')):\n                found_checkpoints.append((item, item_path))\n                print(f\"   ‚úì Checkpoint encontrado: {item}/\")\n    \n    if not found_checkpoints:\n        print(f\"   ‚ùå No se encontr√≥ estructura de checkpoint v√°lida\")\n        print(f\"   üìÇ Contenido extra√≠do:\")\n        for item in os.listdir(temp_dir)[:10]:\n            print(f\"      - {item}\")\n        shutil.rmtree(temp_dir, ignore_errors=True)\n        return None\n    \n    # Convertir el checkpoint legacy a formato moderno\n    epoch = None\n    for ckpt_name, ckpt_path in found_checkpoints:\n        # Determinar el epoch desde el nombre\n        import re\n        match = re.search(r'snapshot_(\\d+)', ckpt_name)\n        if match:\n            epoch = match.group(1)\n            final_name = f'snapshot_{epoch}.pth'\n        else:\n            # Si no tiene snapshot_XX, usar archive/ con epoch esperado\n            if expected_epoch:\n                epoch = str(expected_epoch)\n                final_name = f'snapshot_{epoch}.pth'\n            else:\n                epoch = '0'\n                final_name = f'{model_name}_checkpoint.pth'\n        \n        dest_path = os.path.join('output/model_dump', final_name)\n        \n        print(f\"   üîÑ Convirtiendo formato legacy ‚Üí formato moderno...\")\n        \n        try:\n            # SOLUCI√ìN DEFINITIVA: Cargar manualmente el formato legacy y guardar como moderno\n            import pickle\n            import io\n            \n            # Cargar data.pkl\n            data_pkl_path = os.path.join(ckpt_path, 'data.pkl')\n            data_dir = os.path.join(ckpt_path, 'data')\n            \n            # Crear un unpickler personalizado que resuelve persistent IDs\n            class LegacyUnpickler(pickle.Unpickler):\n                def __init__(self, file, data_dir):\n                    super().__init__(file)\n                    self.data_dir = data_dir\n                    self.storage_cache = {}\n                \n                def persistent_load(self, pid):\n                    # pid es una tupla: ('storage', <type>, <key>, <location>, <size>)\n                    if isinstance(pid, tuple) and len(pid) >= 2 and pid[0] == 'storage':\n                        typename, key = pid[1:3]\n                        location = pid[3] if len(pid) > 3 else None\n                        \n                        # Cachear storages para evitar recargar\n                        if key in self.storage_cache:\n                            return self.storage_cache[key]\n                        \n                        # Leer el archivo de storage\n                        storage_file = os.path.join(self.data_dir, str(key))\n                        \n                        # Leer el contenido como raw binary\n                        with open(storage_file, 'rb') as f:\n                            raw_data = f.read()\n                            \n                            # Crear UntypedStorage desde el buffer\n                            untyped_storage = torch.UntypedStorage.from_buffer(raw_data, dtype=torch.uint8)\n                            \n                            # Mapear typename a dtype de PyTorch\n                            dtype_map = {\n                                'FloatStorage': torch.float32,\n                                'DoubleStorage': torch.float64,\n                                'HalfStorage': torch.float16,\n                                'LongStorage': torch.int64,\n                                'IntStorage': torch.int32,\n                                'ShortStorage': torch.int16,\n                                'CharStorage': torch.int8,\n                                'ByteStorage': torch.uint8,\n                                'BoolStorage': torch.bool,\n                            }\n                            \n                            # Obtener dtype desde typename\n                            # typename puede ser un objeto _LegacyStorageMeta, extraer el nombre\n                            type_str = str(typename).split('.')[-1].replace(\"'>\", \"\")\n                            dtype = dtype_map.get(type_str, torch.float32)\n                            \n                            # Crear TypedStorage desde UntypedStorage\n                            typed_storage = torch.storage.TypedStorage(\n                                wrap_storage=untyped_storage,\n                                dtype=dtype\n                            )\n                            \n                            self.storage_cache[key] = typed_storage\n                            return typed_storage\n                    \n                    raise pickle.UnpicklingError(f\"unsupported persistent id: {pid}\")\n            \n            # Cargar el checkpoint usando el unpickler personalizado\n            with open(data_pkl_path, 'rb') as f:\n                unpickler = LegacyUnpickler(f, data_dir)\n                checkpoint = unpickler.load()\n            \n            # Guardar en formato moderno\n            torch.save(checkpoint, dest_path)\n            \n            # Verificar tama√±o\n            size_mb = os.path.getsize(dest_path) / (1024 * 1024)\n            print(f\"   ‚úì Archivo creado: {final_name} ({size_mb:.1f} MB)\")\n            \n            # Verificar que se puede cargar\n            test_load = torch.load(dest_path, map_location='cpu', weights_only=False)\n            keys = list(test_load.keys())\n            print(f\"   ‚úì Verificaci√≥n exitosa - Keys: {keys}\")\n            \n        except Exception as e:\n            print(f\"   ‚ùå ERROR al convertir checkpoint: {type(e).__name__}\")\n            print(f\"      {str(e)[:200]}\")\n            import traceback\n            traceback.print_exc()\n            shutil.rmtree(temp_dir, ignore_errors=True)\n            return None\n    \n    # Limpiar temporal\n    shutil.rmtree(temp_dir, ignore_errors=True)\n    print(f\"   ‚úì Conversi√≥n completada\")\n    \n    return epoch\n\n# Extraer modelo L (epoch 83 seg√∫n el paper)\nprint(\"=\"*60)\nmodel_l_path = f'{MODELS_DATASET_PATH}/ConvNeXtPose_L.tar'\nepoch_l = extract_checkpoint(model_l_path, 'L', expected_epoch=83)\n\nprint()\n\n# Extraer modelo M (epoch 70 seg√∫n el paper)\nmodel_m_path = f'{MODELS_DATASET_PATH}/ConvNeXtPose_M.tar'\nepoch_m = extract_checkpoint(model_m_path, 'M', expected_epoch=70)\nprint(\"=\"*60)\n\n# Verificar checkpoints extra√≠dos\nprint(\"\\nüìÇ Checkpoints disponibles:\")\ncheckpoints = [f for f in os.listdir('output/model_dump') \n               if f.endswith('.pth') and os.path.isfile(os.path.join('output/model_dump', f))]\n\nif checkpoints:\n    for ckpt in sorted(checkpoints):\n        ckpt_path = os.path.join('output/model_dump', ckpt)\n        size_mb = os.path.getsize(ckpt_path) / (1024 * 1024)\n        print(f\"  ‚úì {ckpt} ({size_mb:.1f} MB)\")\n        \n        # Verificar contenido\n        try:\n            test_load = torch.load(ckpt_path, map_location='cpu', weights_only=False)\n            keys = list(test_load.keys())\n            print(f\"    ‚Üí Keys: {keys}\")\n            if 'network' in test_load:\n                # Contar par√°metros\n                num_params = sum(p.numel() for p in test_load['network'].values() \n                                 if isinstance(p, torch.Tensor))\n                print(f\"    ‚Üí ‚úÖ Formato v√°lido ({num_params:,} par√°metros)\")\n            else:\n                print(f\"    ‚Üí ‚ö†Ô∏è  Falta key 'network'\")\n        except Exception as e:\n            print(f\"    ‚Üí ‚ùå Error: {type(e).__name__}: {str(e)[:80]}\")\n\n# Mostrar informaci√≥n de epochs\nif epoch_l:\n    print(f\"\\nüí° Modelo L: Usa CHECKPOINT_EPOCH = {epoch_l}\")\nif epoch_m:\n    print(f\"üí° Modelo M: Usa CHECKPOINT_EPOCH = {epoch_m}\")\n\nif epoch_l or epoch_m:\n    print(\"\\n‚úÖ Los checkpoints est√°n listos para torch.load()\")\n    print(\"   Formato: PyTorch moderno (.pth)\")\nelse:\n    print(\"\\n‚ùå No se pudieron extraer los checkpoints\")\n    print(\"üí° Verifica que los archivos .tar se descargaron correctamente\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-14T16:47:35.060407Z","iopub.execute_input":"2025-10-14T16:47:35.060688Z","iopub.status.idle":"2025-10-14T16:47:36.438155Z","shell.execute_reply.started":"2025-10-14T16:47:35.060664Z","shell.execute_reply":"2025-10-14T16:47:36.437433Z"}},"outputs":[{"name":"stdout","text":"============================================================\nüì¶ Extrayendo modelo L desde /kaggle/working/ConvNeXtPose/models_tar/ConvNeXtPose_L.tar...\n   üìè Tama√±o del archivo: 96.19 MB\n   üîç Primeros bytes (hex): 504b03040000080800000000000000000000000000000000000018000a00736e617073686f745f38332e7074682f64617461\n   ‚úì Formato: ZIP - Extra√≠do: 808 archivos\n   ‚úì Checkpoint encontrado: snapshot_83.pth/\n   üîÑ Convirtiendo formato legacy ‚Üí formato moderno...\n","output_type":"stream"},{"name":"stderr","text":"<ipython-input-19-2ab6ed486f7e>:162: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n  typed_storage = torch.storage.TypedStorage(\n","output_type":"stream"},{"name":"stdout","text":"   ‚úì Archivo creado: snapshot_83.pth (96.2 MB)\n   ‚úì Verificaci√≥n exitosa - Keys: ['epoch', 'network', 'optimizer']\n   ‚úì Conversi√≥n completada\n\nüì¶ Extrayendo modelo M desde /kaggle/working/ConvNeXtPose/models_tar/ConvNeXtPose_M.tar...\n   üìè Tama√±o del archivo: 87.10 MB\n   üîç Primeros bytes (hex): 504b03040000080800000000000000000000000000000000000010001200617263686976652f646174612e706b6c46420e00\n   ‚úì Formato: ZIP - Extra√≠do: 808 archivos\n   ‚úì Checkpoint encontrado: archive/\n   üîÑ Convirtiendo formato legacy ‚Üí formato moderno...\n   ‚úì Archivo creado: snapshot_70.pth (87.1 MB)\n   ‚úì Verificaci√≥n exitosa - Keys: ['epoch', 'network', 'optimizer']\n   ‚úì Conversi√≥n completada\n============================================================\n\nüìÇ Checkpoints disponibles:\n  ‚úì snapshot_70.pth (87.1 MB)\n    ‚Üí Keys: ['epoch', 'network', 'optimizer']\n    ‚Üí ‚úÖ Formato v√°lido (7,596,986 par√°metros)\n  ‚úì snapshot_83.pth (96.2 MB)\n    ‚Üí Keys: ['epoch', 'network', 'optimizer']\n    ‚Üí ‚úÖ Formato v√°lido (8,391,354 par√°metros)\n\nüí° Modelo L: Usa CHECKPOINT_EPOCH = 83\nüí° Modelo M: Usa CHECKPOINT_EPOCH = 70\n\n‚úÖ Los checkpoints est√°n listos para torch.load()\n   Formato: PyTorch moderno (.pth)\n","output_type":"stream"}],"execution_count":19},{"cell_type":"markdown","source":"### ‚ö†Ô∏è Nota sobre Extracci√≥n de Checkpoints\n\nLos archivos `.tar` de ConvNeXtPose son en realidad **archivos ZIP** con estructura anidada:\n```\nConvNeXtPose_L.tar (archivo zip)\n‚îî‚îÄ‚îÄ snapshot_83.pth/        ‚Üê Directorio\n    ‚îú‚îÄ‚îÄ data.pkl\n    ‚îú‚îÄ‚îÄ version\n    ‚îî‚îÄ‚îÄ data/               ‚Üê Carpeta con el checkpoint real\n        ‚îî‚îÄ‚îÄ 0, 1, 2...      ‚Üê Archivos binarios\n```\n\nLa siguiente celda extrae y reorganiza correctamente el archivo `.pth` real.","metadata":{}},{"cell_type":"code","source":"!pwd","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-14T16:47:36.438928Z","iopub.execute_input":"2025-10-14T16:47:36.439193Z","iopub.status.idle":"2025-10-14T16:47:36.566305Z","shell.execute_reply.started":"2025-10-14T16:47:36.439158Z","shell.execute_reply":"2025-10-14T16:47:36.565355Z"}},"outputs":[{"name":"stdout","text":"/kaggle/working/ConvNeXtPose\n","output_type":"stream"}],"execution_count":20},{"cell_type":"code","source":"# Verificar checkpoints extra√≠dos y detectar epoch\nimport glob\nimport re\nimport os\nimport torch\n\nprint(\"üîç Verificaci√≥n de Checkpoints:\\n\")\n\n# Buscar ARCHIVOS .pth\ncheckpoint_dir = 'output/model_dump'\nif os.path.exists(checkpoint_dir):\n    checkpoints = [f for f in os.listdir(checkpoint_dir) \n                   if f.endswith('.pth') and os.path.isfile(os.path.join(checkpoint_dir, f))]\nelse:\n    checkpoints = []\n\nif checkpoints:\n    print(\"‚úÖ Checkpoints encontrados:\\n\")\n    for ckpt in sorted(checkpoints):\n        ckpt_path = os.path.join(checkpoint_dir, ckpt)\n        size_mb = os.path.getsize(ckpt_path) / (1024 * 1024)\n        \n        # Extraer epoch\n        match = re.search(r'snapshot_(\\d+)', ckpt)\n        if match:\n            epoch = match.group(1)\n            print(f\"  ‚úì {ckpt} ({size_mb:.1f} MB)\")\n            print(f\"    ‚Üí Usa CHECKPOINT_EPOCH = {epoch}\")\n            \n            # Verificar contenido del checkpoint\n            try:\n                test_load = torch.load(ckpt_path, map_location='cpu', weights_only=False)\n                keys = list(test_load.keys())\n                print(f\"    ‚Üí Keys: {keys}\")\n                \n                if 'network' in test_load:\n                    # Contar par√°metros del modelo\n                    num_params = sum(p.numel() for p in test_load['network'].values() \n                                     if isinstance(p, torch.Tensor))\n                    print(f\"    ‚Üí ‚úÖ Formato v√°lido ({num_params:,} par√°metros)\")\n                else:\n                    print(f\"    ‚Üí ‚ö†Ô∏è  Falta key 'network'\")\n                    \n            except Exception as e:\n                print(f\"    ‚Üí ‚ùå Error al verificar: {type(e).__name__}: {str(e)[:50]}\")\n            print()\nelse:\n    print(\"‚ùå No se encontraron checkpoints v√°lidos\\n\")\n    \n    # Diagnosticar problema\n    if os.path.exists(checkpoint_dir):\n        all_items = os.listdir(checkpoint_dir)\n        if all_items:\n            print(\"üìÇ Contenido de output/model_dump/:\")\n            for item in all_items[:15]:\n                item_path = os.path.join(checkpoint_dir, item)\n                if os.path.isdir(item_path):\n                    print(f\"    üìÅ {item}/ (directorio - no v√°lido)\")\n                else:\n                    size_mb = os.path.getsize(item_path) / (1024 * 1024)\n                    print(f\"    üìÑ {item} ({size_mb:.1f} MB)\")\n            \n            print(\"\\nüí° Soluci√≥n: Re-ejecuta la celda anterior de extracci√≥n\")\n        else:\n            print(\"üí° Directorio vac√≠o - verifica MODELS_DATASET_PATH\")\n    else:\n        print(\"üí° Soluci√≥n: Verifica que MODELS_DATASET_PATH es correcto\")\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"üí° IMPORTANTE: Los checkpoints son archivos .pth modernos\")\nprint(\"   Convertidos desde formato legacy a formato PyTorch est√°ndar\")\nprint(\"=\"*60)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-14T16:47:36.567521Z","iopub.execute_input":"2025-10-14T16:47:36.567775Z","iopub.status.idle":"2025-10-14T16:47:36.742784Z","shell.execute_reply.started":"2025-10-14T16:47:36.567753Z","shell.execute_reply":"2025-10-14T16:47:36.742101Z"}},"outputs":[{"name":"stdout","text":"üîç Verificaci√≥n de Checkpoints:\n\n‚úÖ Checkpoints encontrados:\n\n  ‚úì snapshot_70.pth (87.1 MB)\n    ‚Üí Usa CHECKPOINT_EPOCH = 70\n    ‚Üí Keys: ['epoch', 'network', 'optimizer']\n    ‚Üí ‚úÖ Formato v√°lido (7,596,986 par√°metros)\n\n  ‚úì snapshot_83.pth (96.2 MB)\n    ‚Üí Usa CHECKPOINT_EPOCH = 83\n    ‚Üí Keys: ['epoch', 'network', 'optimizer']\n    ‚Üí ‚úÖ Formato v√°lido (8,391,354 par√°metros)\n\n\n============================================================\nüí° IMPORTANTE: Los checkpoints son archivos .pth modernos\n   Convertidos desde formato legacy a formato PyTorch est√°ndar\n============================================================\n","output_type":"stream"}],"execution_count":21},{"cell_type":"markdown","source":"**‚ö†Ô∏è IMPORTANTE:** Ajusta el valor de `CHECKPOINT_EPOCH` en las siguientes celdas seg√∫n el epoch de tu checkpoint extra√≠do. En este caso detectamos `snapshot_83.pth`, as√≠ que usa `CHECKPOINT_EPOCH = 83`.","metadata":{}},{"cell_type":"markdown","source":"## üöÄ PASO 4: Ejecutar Testing\n\n### Modelo L (Large)","metadata":{}},{"cell_type":"code","source":"# Diagn√≥stico: Verificar que todos los componentes est√°n listos\nimport sys\nimport os\n\nprint(\"üîç Verificaci√≥n de Componentes:\\n\")\n\n# 1. Dataset - Verificar estructura DENTRO del proyecto\nproject_root = '/kaggle/working/ConvNeXtPose'\ndata_dir = os.path.join(project_root, 'data')\nh36m_path = os.path.join(data_dir, 'Human36M')\n\nprint(f\"1. Dataset (estructura del proyecto):\")\nprint(f\"   Project root: {project_root}\")\nprint(f\"   data/ exists: {os.path.exists(data_dir)}\")\nprint(f\"   data/dataset.py: {os.path.exists(os.path.join(data_dir, 'dataset.py'))}\")\nprint(f\"   data/Human36M/ exists: {os.path.exists(h36m_path)}\")\n\nif os.path.exists(h36m_path):\n    print(f\"   - Human36M.py: {os.path.exists(os.path.join(h36m_path, 'Human36M.py'))}\")\n    print(f\"   - annotations: {os.path.exists(os.path.join(h36m_path, 'annotations'))}\")\n    print(f\"   - images/S9: {os.path.exists(os.path.join(h36m_path, 'images', 'S9'))}\")\n    print(f\"   - images/S11: {os.path.exists(os.path.join(h36m_path, 'images', 'S11'))}\")\n\n# 2. Checkpoints\ncheckpoint_dir = os.path.join(project_root, 'output/model_dump')\nprint(f\"\\n2. Checkpoints: {checkpoint_dir}\")\nif os.path.exists(checkpoint_dir):\n    checkpoints = [f for f in os.listdir(checkpoint_dir) if f.startswith('snapshot_')]\n    if checkpoints:\n        print(f\"   Disponibles: {', '.join(checkpoints)}\")\n        # Extraer epoch del checkpoint\n        import re\n        for ckpt in checkpoints:\n            match = re.search(r'snapshot_(\\d+)', ckpt)\n            if match:\n                epoch = match.group(1)\n                print(f\"   üí° Usa CHECKPOINT_EPOCH = {epoch} en la siguiente celda\")\n    else:\n        print(\"   ‚ö†Ô∏è  No se encontraron checkpoints\")\nelse:\n    print(\"   ‚ùå Directorio no existe\")\n\n# 3. Estructura del proyecto\nprint(f\"\\n3. Estructura del proyecto:\")\ncritical_files = ['main/config.py', 'common/base.py', 'data/dataset.py', 'data/Human36M/Human36M.py']\nfor file_path in critical_files:\n    full_path = os.path.join(project_root, file_path)\n    exists = os.path.exists(full_path)\n    status = \"‚úì\" if exists else \"‚ùå\"\n    print(f\"   {status} {file_path}\")\n\nprint(\"\\n\" + \"=\"*60)\nif all(os.path.exists(os.path.join(project_root, f)) for f in critical_files):\n    print(\"‚úÖ Todos los checks pasaron - Listo para testing\")\nelse:\n    print(\"‚ùå Algunos archivos faltan - Revisa la configuraci√≥n\")\nprint(\"=\"*60)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-14T16:47:36.743601Z","iopub.execute_input":"2025-10-14T16:47:36.743876Z","iopub.status.idle":"2025-10-14T16:47:36.760980Z","shell.execute_reply.started":"2025-10-14T16:47:36.743840Z","shell.execute_reply":"2025-10-14T16:47:36.760168Z"}},"outputs":[{"name":"stdout","text":"üîç Verificaci√≥n de Componentes:\n\n1. Dataset (estructura del proyecto):\n   Project root: /kaggle/working/ConvNeXtPose\n   data/ exists: True\n   data/dataset.py: True\n   data/Human36M/ exists: True\n   - Human36M.py: True\n   - annotations: True\n   - images/S9: True\n   - images/S11: True\n\n2. Checkpoints: /kaggle/working/ConvNeXtPose/output/model_dump\n   Disponibles: snapshot_70.pth, snapshot_83.pth\n   üí° Usa CHECKPOINT_EPOCH = 70 en la siguiente celda\n   üí° Usa CHECKPOINT_EPOCH = 83 en la siguiente celda\n\n3. Estructura del proyecto:\n   ‚úì main/config.py\n   ‚úì common/base.py\n   ‚úì data/dataset.py\n   ‚úì data/Human36M/Human36M.py\n\n============================================================\n‚úÖ Todos los checks pasaron - Listo para testing\n============================================================\n","output_type":"stream"}],"execution_count":22},{"cell_type":"markdown","source":"### Ejecutar Testing desde Python\n\n**Estructura correcta:**\n- ‚úÖ M√≥dulos Python originales en `data/` (dataset.py, Human36M.py)\n- ‚úÖ Dataset de Kaggle enlazado en `data/Human36M/` (images, annotations)\n- ‚úÖ `config.py` configura autom√°ticamente los paths","metadata":{}},{"cell_type":"markdown","source":"### ‚ö†Ô∏è IMPORTANTE: Verificar GPU Antes de Testing\n\n**El modelo REQUIERE GPU para correr en tiempo razonable.**\n\n- ‚úÖ **Con GPU T4 x2**: ~10-20 minutos\n- ‚ùå **Con CPU**: ~10-20 HORAS (no recomendado)\n\n**C√≥mo activar GPU en Kaggle:**\n1. Panel derecho ‚Üí **Settings**\n2. **Accelerator** ‚Üí Selecciona **GPU T4 x2** o **GPU P100**\n3. Click **Save**\n4. El notebook se reiniciar√° con GPU habilitada","metadata":{}},{"cell_type":"code","source":"# Verificar disponibilidad de GPU\nimport torch\n\nprint(\"üîç Verificando hardware disponible...\\n\")\nif torch.cuda.is_available():\n    print(f\"‚úÖ GPU disponible: {torch.cuda.get_device_name(0)}\")\n    print(f\"   Memoria: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n    print(f\"\\nüí° Tiempo estimado: 10-20 minutos\")\n    USE_GPU = True\nelse:\n    print(\"‚ùå GPU NO disponible - usando CPU\")\n    print(\"\\n‚ö†Ô∏è  ADVERTENCIA: El testing en CPU puede tomar HORAS\")\n    print(\"   Se recomienda activar GPU T4 x2 en Kaggle\")\n    print(\"\\n¬øContinuar de todas formas? (no recomendado)\")\n    USE_GPU = False","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-14T16:47:36.761881Z","iopub.execute_input":"2025-10-14T16:47:36.762180Z","iopub.status.idle":"2025-10-14T16:47:36.769521Z","shell.execute_reply.started":"2025-10-14T16:47:36.762127Z","shell.execute_reply":"2025-10-14T16:47:36.768709Z"}},"outputs":[{"name":"stdout","text":"üîç Verificando hardware disponible...\n\n‚úÖ GPU disponible: Tesla T4\n   Memoria: 15.8 GB\n\nüí° Tiempo estimado: 10-20 minutos\n","output_type":"stream"}],"execution_count":23},{"cell_type":"code","source":"# Testing con estructura correcta del proyecto\nimport sys\nimport os\n\nos.chdir('/kaggle/working/ConvNeXtPose/main')\nfrom config import cfg\n\n# No llamar cfg.load_variant_config(...) por ahora\ncfg.head_cfg = None\ncfg.backbone_cfg = ([3,3,9,3],[48,96,192,384])\ncfg.variant = 'XS'\ncfg.depth_dim = 64\n\n# cfg.load_variant_config(VARIANT)\ncfg.set_args('0')\nCHECKPOINT_EPOCH = 83  # ‚Üê AJUSTAR seg√∫n tu checkpoint\n\nprint(f\"\\n{'='*60}\")\nprint(f\"  Testing ConvNeXtPose-{cfg.variant}\")\nprint(f\"{'='*60}\\n\")\n\n# 4. AHORA importar los dem√°s m√≥dulos (config ya configur√≥ sys.path)\nimport torch\nimport torch.backends.cudnn as cudnn\nfrom base import Tester\nimport numpy as np\nfrom tqdm import tqdm\n\n# Configurar CUDA solo si est√° disponible\nif torch.cuda.is_available():\n    cudnn.benchmark = True\n    cudnn.deterministic = False\n    cudnn.enabled = True\n    print(\"‚úÖ CUDA habilitado\")\nelse:\n    print(\"‚ö†Ô∏è  Ejecutando en CPU (ser√° muy lento)\")\nfrom collections import OrderedDict\nimport torch\n\ndef map_legacy_head_keys(state_dict):\n    new_state = OrderedDict()\n    for k, v in state_dict.items():\n        if k.startswith('module.head.deconv_layers_'):\n            layer = k.split('.')[2]           # deconv_layers_1 / 2 / 3\n            suffix = k.split('.', 3)[-1]      # 0.weight, 1.bias, 2.weight‚Ä¶\n            if suffix.startswith('0.'):\n                new_k = k.replace('.0.', '.dwconv.')\n            elif suffix.startswith('1.'):\n                new_k = k.replace('.1.', '.norm.')\n            elif suffix.startswith('2.'):\n                new_k = k.replace('.2.', '.pwconv.')\n            else:\n                new_k = k\n            new_state[new_k] = v\n        else:\n            new_state[k] = v\n    return new_state\norig_make_model = Tester._make_model\n\ndef legacy_make_model(self, test_epoch):\n    import os\n    from torch.nn.parallel import DataParallel\n    from model import get_pose_net\n    from config import cfg\n\n    self.test_epoch = test_epoch\n    base = os.path.join(cfg.model_dir, f'snapshot_{test_epoch}')\n    model_path = base + '.pth'\n    if not os.path.exists(model_path):\n        model_path = base + '.pth.tar'\n    self.logger.info(f'Load checkpoint from {model_path}')\n\n    model = get_pose_net(cfg, False, self.joint_num)\n    model = DataParallel(model).cuda()\n\n    ckpt = torch.load(model_path, map_location='cpu')\n    state_dict = ckpt['network']\n    state_dict = map_legacy_head_keys(state_dict)  # <-- renombrar claves\n\n    model.load_state_dict(state_dict)\n    self.logger.info('‚úì Checkpoint cargado (modo legacy)')\n    model.eval()\n    self.model = model\n\n# 5. Crear tester y ejecutar\nTester._make_model = legacy_make_model\ntester = Tester()\ntester._make_batch_generator()\ntester._make_model(CHECKPOINT_EPOCH)\nprint(f\"\\nüöÄ Ejecutando testing en epoch {CHECKPOINT_EPOCH}...\\n\")\n\npreds = []\nwith torch.no_grad():\n    for itr, input_img in enumerate(tqdm(tester.batch_generator)):\n        coord_out = tester.model(input_img)\n        \n        if cfg.flip_test:\n            from utils.pose_utils import flip\n            flipped_input_img = flip(input_img, dims=3)\n            flipped_coord_out = tester.model(flipped_input_img)\n            flipped_coord_out[:, :, 0] = cfg.output_shape[1] - flipped_coord_out[:, :, 0] - 1\n            for pair in tester.flip_pairs:\n                flipped_coord_out[:, pair[0], :], flipped_coord_out[:, pair[1], :] = \\\n                    flipped_coord_out[:, pair[1], :].clone(), flipped_coord_out[:, pair[0], :].clone()\n            coord_out = (coord_out + flipped_coord_out)/2.\n        \n        coord_out = coord_out.cpu().numpy()\n        preds.append(coord_out)\n\n# Evaluar\npreds = np.concatenate(preds, axis=0)\nprint(f\"\\nüìä Evaluando {len(preds)} predicciones...\\n\")\ntester._evaluate(preds, cfg.result_dir)\n\nprint(f\"\\n‚úÖ Testing completado!\")\nprint(f\"üìÇ Resultados guardados en: {cfg.result_dir}\")\nprint(f\"\\nüí° MPJPE esperado para XS: ~52.0 mm (Protocol 2)\")\nprint(f\"üí° PA-MPJPE esperado para XS: ~36.5 mm (Protocol 1)\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-14T16:56:12.167343Z","iopub.execute_input":"2025-10-14T16:56:12.167675Z","iopub.status.idle":"2025-10-14T16:56:29.506354Z","shell.execute_reply.started":"2025-10-14T16:56:12.167652Z","shell.execute_reply":"2025-10-14T16:56:29.504962Z"}},"outputs":[{"name":"stderr","text":"\u001b[92m10-14 16:56:12\u001b[0m Creating dataset...\n","output_type":"stream"},{"name":"stdout","text":">>> Using GPU: 0\n\n============================================================\n  Testing ConvNeXtPose-XS\n============================================================\n\n‚úÖ CUDA habilitado\nLoad data of H36M Protocol 2\ncreating index...\nindex created!\nGet bounding box and root from groundtruth\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 20 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(\n\u001b[92m10-14 16:56:28\u001b[0m Load checkpoint from /kaggle/working/ConvNeXtPose/main/../output/model_dump/snapshot_83.pth\n","output_type":"stream"},{"name":"stdout","text":"üìê Arquitectura: ConvNeXtPose-XS\n   Backbone: 384 canales de salida\n   HeadNet: Legacy (2-UP + 1 sin upsampling)\n","output_type":"stream"},{"name":"stderr","text":"<ipython-input-25-bb4940fbf4cf>:76: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  ckpt = torch.load(model_path, map_location='cpu')\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m<ipython-input-25-bb4940fbf4cf>\u001b[0m in \u001b[0;36m<cell line: 89>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[0mtester\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTester\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[0mtester\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_batch_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m \u001b[0mtester\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCHECKPOINT_EPOCH\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     90\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"\\nüöÄ Ejecutando testing en epoch {CHECKPOINT_EPOCH}...\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-25-bb4940fbf4cf>\u001b[0m in \u001b[0;36mlegacy_make_model\u001b[0;34m(self, test_epoch)\u001b[0m\n\u001b[1;32m     78\u001b[0m     \u001b[0mstate_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmap_legacy_head_keys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# <-- renombrar claves\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 80\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     81\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'‚úì Checkpoint cargado (modo legacy)'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mload_state_dict\u001b[0;34m(self, state_dict, strict, assign)\u001b[0m\n\u001b[1;32m   2582\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2583\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_msgs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2584\u001b[0;31m             raise RuntimeError(\n\u001b[0m\u001b[1;32m   2585\u001b[0m                 \"Error(s) in loading state_dict for {}:\\n\\t{}\".format(\n\u001b[1;32m   2586\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"\\n\\t\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_msgs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for DataParallel:\n\tsize mismatch for module.head.deconv_layers_1.pwconv.weight: copying a param with shape torch.Size([512, 384, 1, 1]) from checkpoint, the shape in current model is torch.Size([256, 384, 1, 1]).\n\tsize mismatch for module.head.deconv_layers_1.pwconv.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for module.head.deconv_layers_2.dwconv.weight: copying a param with shape torch.Size([512, 1, 3, 3]) from checkpoint, the shape in current model is torch.Size([256, 1, 3, 3]).\n\tsize mismatch for module.head.deconv_layers_2.dwconv.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for module.head.deconv_layers_2.norm.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for module.head.deconv_layers_2.norm.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for module.head.deconv_layers_2.norm.running_mean: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for module.head.deconv_layers_2.norm.running_var: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for module.head.deconv_layers_2.pwconv.weight: copying a param with shape torch.Size([512, 512, 1, 1]) from checkpoint, the shape in current model is torch.Size([256, 256, 1, 1]).\n\tsize mismatch for module.head.deconv_layers_2.pwconv.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for module.head.deconv_layers_3.dwconv.weight: copying a param with shape torch.Size([512, 1, 3, 3]) from checkpoint, the shape in current model is torch.Size([256, 1, 3, 3]).\n\tsize mismatch for module.head.deconv_layers_3.dwconv.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for module.head.deconv_layers_3.norm.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for module.head.deconv_layers_3.norm.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for module.head.deconv_layers_3.norm.running_mean: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for module.head.deconv_layers_3.norm.running_var: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for module.head.deconv_layers_3.pwconv.weight: copying a param with shape torch.Size([512, 512, 1, 1]) from checkpoint, the shape in current model is torch.Size([256, 256, 1, 1]).\n\tsize mismatch for module.head.deconv_layers_3.pwconv.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for module.head.final_layer.weight: copying a param with shape torch.Size([1152, 512, 1, 1]) from checkpoint, the shape in current model is torch.Size([1152, 256, 1, 1])."],"ename":"RuntimeError","evalue":"Error(s) in loading state_dict for DataParallel:\n\tsize mismatch for module.head.deconv_layers_1.pwconv.weight: copying a param with shape torch.Size([512, 384, 1, 1]) from checkpoint, the shape in current model is torch.Size([256, 384, 1, 1]).\n\tsize mismatch for module.head.deconv_layers_1.pwconv.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for module.head.deconv_layers_2.dwconv.weight: copying a param with shape torch.Size([512, 1, 3, 3]) from checkpoint, the shape in current model is torch.Size([256, 1, 3, 3]).\n\tsize mismatch for module.head.deconv_layers_2.dwconv.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for module.head.deconv_layers_2.norm.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for module.head.deconv_layers_2.norm.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for module.head.deconv_layers_2.norm.running_mean: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for module.head.deconv_layers_2.norm.running_var: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for module.head.deconv_layers_2.pwconv.weight: copying a param with shape torch.Size([512, 512, 1, 1]) from checkpoint, the shape in current model is torch.Size([256, 256, 1, 1]).\n\tsize mismatch for module.head.deconv_layers_2.pwconv.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for module.head.deconv_layers_3.dwconv.weight: copying a param with shape torch.Size([512, 1, 3, 3]) from checkpoint, the shape in current model is torch.Size([256, 1, 3, 3]).\n\tsize mismatch for module.head.deconv_layers_3.dwconv.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for module.head.deconv_layers_3.norm.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for module.head.deconv_layers_3.norm.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for module.head.deconv_layers_3.norm.running_mean: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for module.head.deconv_layers_3.norm.running_var: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for module.head.deconv_layers_3.pwconv.weight: copying a param with shape torch.Size([512, 512, 1, 1]) from checkpoint, the shape in current model is torch.Size([256, 256, 1, 1]).\n\tsize mismatch for module.head.deconv_layers_3.pwconv.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for module.head.final_layer.weight: copying a param with shape torch.Size([1152, 512, 1, 1]) from checkpoint, the shape in current model is torch.Size([1152, 256, 1, 1]).","output_type":"error"}],"execution_count":25},{"cell_type":"markdown","source":"### Modelo M (Medium) - Opcional","metadata":{}},{"cell_type":"code","source":"# Si tienes el checkpoint del modelo M, ejecuta esto:\n# !python test.py --gpu 0 --epochs {CHECKPOINT_EPOCH} --variant M","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-14T16:47:43.280996Z","iopub.status.idle":"2025-10-14T16:47:43.281277Z","shell.execute_reply":"2025-10-14T16:47:43.281153Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## üìä PASO 5: Verificar Resultados","metadata":{}},{"cell_type":"code","source":"# Ver resultados generados\n%cd ..\n!ls -lh output/result/\n\n# Leer log de resultados\nimport glob\nlog_files = glob.glob('output/log/*.log')\nif log_files:\n    latest_log = max(log_files, key=os.path.getctime)\n    print(f\"\\nüìÑ √öltimas l√≠neas del log ({os.path.basename(latest_log)}):\")\n    !tail -n 20 {latest_log}\nelse:\n    print(\"‚ö†Ô∏è  No se encontraron logs\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-14T16:47:43.282247Z","iopub.status.idle":"2025-10-14T16:47:43.282557Z","shell.execute_reply":"2025-10-14T16:47:43.282399Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## üìà PASO 6: An√°lisis de Resultados","metadata":{}},{"cell_type":"code","source":"import re\n\ndef extract_mpjpe_from_log(log_path):\n    \"\"\"Extrae el MPJPE del log\"\"\"\n    with open(log_path, 'r') as f:\n        content = f.read()\n    \n    # Buscar patr√≥n de MPJPE\n    pattern = r'MPJPE.*?(\\d+\\.\\d+)'\n    matches = re.findall(pattern, content)\n    \n    if matches:\n        return float(matches[-1])  # √öltimo valor\n    return None\n\n# Extraer resultados\nlog_files = glob.glob('output/log/*.log')\nif log_files:\n    latest_log = max(log_files, key=os.path.getctime)\n    mpjpe = extract_mpjpe_from_log(latest_log)\n    \n    print(\"\\n\" + \"=\"*60)\n    print(\"  üìä RESULTADOS FINALES\")\n    print(\"=\"*60)\n    \n    if mpjpe:\n        print(f\"\\n  MPJPE (Protocol 2): {mpjpe:.2f} mm\")\n        \n        # Comparar con paper\n        expected = {\n            'L': 42.3,\n            'M': 44.6\n        }\n        \n        # Determinar variante\n        for variant, expected_val in expected.items():\n            diff = abs(mpjpe - expected_val)\n            if diff < 5:\n                print(f\"\\n  Variante detectada: {variant}\")\n                print(f\"  Valor del paper: {expected_val:.2f} mm\")\n                print(f\"  Diferencia: {mpjpe - expected_val:+.2f} mm\")\n                \n                if diff < 2:\n                    print(\"  ‚úÖ Resultado excelente (dentro de ¬±2mm)\")\n                elif diff < 5:\n                    print(\"  ‚úì Resultado aceptable (dentro de ¬±5mm)\")\n                break\n    else:\n        print(\"\\n‚ö†Ô∏è  No se pudo extraer MPJPE del log\")\n        print(\"Revisa el log manualmente en output/log/\")\n    \n    print(\"\\n\" + \"=\"*60)\nelse:\n    print(\"‚ùå No se encontraron logs de testing\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-14T16:47:43.283332Z","iopub.status.idle":"2025-10-14T16:47:43.283568Z","shell.execute_reply":"2025-10-14T16:47:43.283470Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## üíæ PASO 7: Guardar Outputs\n\nKaggle guarda autom√°ticamente todo en `/kaggle/working/`. Opcionalmente puedes copiar resultados espec√≠ficos:","metadata":{}},{"cell_type":"code","source":"# Crear resumen de resultados\nimport json\nfrom datetime import datetime\n\nsummary = {\n    'timestamp': datetime.now().isoformat(),\n    'model': 'ConvNeXtPose-L',  # Cambiar seg√∫n modelo testeado\n    'checkpoint_epoch': CHECKPOINT_EPOCH,\n    'dataset': 'Human3.6M Protocol 2',\n    'mpjpe_mm': mpjpe if 'mpjpe' in locals() else None,\n    'pytorch_version': torch.__version__,\n    'cuda_version': torch.version.cuda if torch.cuda.is_available() else None\n}\n\n# Guardar resumen\nwith open('output/result/test_summary.json', 'w') as f:\n    json.dump(summary, f, indent=2)\n\nprint(\"‚úì Resumen guardado en output/result/test_summary.json\")\nprint(\"\\nüìÑ Contenido:\")\nprint(json.dumps(summary, indent=2))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-14T16:47:43.284405Z","iopub.status.idle":"2025-10-14T16:47:43.284663Z","shell.execute_reply":"2025-10-14T16:47:43.284552Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## üéâ Testing Completado!\n\nLos resultados est√°n en:\n- **Logs**: `output/log/`\n- **Resultados**: `output/result/`\n- **Resumen JSON**: `output/result/test_summary.json`\n\nTodos los archivos en `/kaggle/working/ConvNeXtPose/output/` se guardar√°n autom√°ticamente cuando hagas commit del notebook.","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}