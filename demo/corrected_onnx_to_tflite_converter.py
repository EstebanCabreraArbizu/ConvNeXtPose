#!/usr/bin/env python3
"""
corrected_onnx_to_tflite_converter.py - Convertidor ONNXâ†’TFLite usando onnx-tf (correcto)

Este convertidor usa exclusivamente onnx-tf para la conversiÃ³n real 
ONNXâ†’TensorFlowâ†’TFLite, preservando los pesos del modelo original.

Estrategias implementadas:
1. onnx-tf SavedModel: ONNX â†’ TensorFlow SavedModel â†’ TFLite (principal)
2. onnx-tf direct: ONNX â†’ TensorFlow en memoria â†’ TFLite (alternativa)

Todas las estrategias preservan los pesos reales del modelo ONNX original.
No se usan modelos simplificados ni genÃ©ricos.
"""

import os
import sys
import tempfile
import shutil
import logging
from pathlib import Path
from typing import Dict, Any, Optional, Tuple

# Configure logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# Check dependencies
TENSORFLOW_AVAILABLE = False
ONNX_TF_AVAILABLE = False
ONNX_AVAILABLE = False

try:
    import tensorflow as tf
    TENSORFLOW_AVAILABLE = True
    logger.info(f"âœ… TensorFlow available: {tf.__version__}")
except ImportError:
    logger.error("âŒ TensorFlow not available")

try:
    import onnx_tf
    from onnx_tf.backend import prepare
    ONNX_TF_AVAILABLE = True
    logger.info(f"âœ… onnx-tf available: {onnx_tf.__version__}")
except ImportError:
    logger.warning("âš ï¸ onnx-tf not available")

try:
    import onnx
    ONNX_AVAILABLE = True
    logger.info(f"âœ… ONNX available: {onnx.__version__}")
except ImportError:
    logger.warning("âš ï¸ ONNX not available")

class CorrectedONNXToTFLiteConverter:
    """
    Convertidor ONNXâ†’TFLite usando onnx-tf (mÃ©todo correcto)
    """
    
    def __init__(self, temp_dir: Optional[str] = None):
        self.temp_dir = temp_dir or tempfile.mkdtemp(prefix="onnx_tflite_")
        self.cleanup_temp = temp_dir is None
        
        if not TENSORFLOW_AVAILABLE:
            raise RuntimeError("TensorFlow is required but not available")
        
        if not ONNX_TF_AVAILABLE:
            raise RuntimeError("onnx-tf is required but not available")
        
        if not ONNX_AVAILABLE:
            raise RuntimeError("ONNX is required but not available")
        
        logger.info(f"ğŸ”§ Initialized converter with temp dir: {self.temp_dir}")
    
    def __enter__(self):
        return self
    
    def __exit__(self, exc_type, exc_val, exc_tb):
        if self.cleanup_temp and os.path.exists(self.temp_dir):
            shutil.rmtree(self.temp_dir, ignore_errors=True)
            logger.info(f"ğŸ§¹ Cleaned up temp directory: {self.temp_dir}")
    
    def convert(self, onnx_path: str, tflite_path: str, 
                optimization: str = "default") -> Dict[str, Any]:
        """
        Convierte ONNX a TFLite usando onnx-tf (mÃ©todo correcto)
        
        Args:
            onnx_path: Ruta al modelo ONNX
            tflite_path: Ruta de salida para el modelo TFLite
            optimization: Tipo de optimizaciÃ³n ('default', 'size', 'latency', 'none')
        
        Returns:
            Dict con resultados de la conversiÃ³n
        """
        result = {
            'success': False,
            'tflite_path': None,
            'strategy_used': None,
            'file_size_mb': 0,
            'error': None
        }
        
        if not os.path.exists(onnx_path):
            result['error'] = f"ONNX model not found: {onnx_path}"
            return result
        
        # Crear directorio de salida si no existe
        os.makedirs(os.path.dirname(os.path.abspath(tflite_path)), exist_ok=True)
        
        # Estrategias de conversiÃ³n con onnx-tf (preservan pesos reales)
        strategies = [
            ("onnx_tf_savedmodel", self._convert_via_onnx_tf_savedmodel),
            ("onnx_tf_direct", self._convert_via_onnx_tf_direct),
        ]
        
        for strategy_name, strategy_func in strategies:
            logger.info(f"ğŸ”„ Trying strategy: {strategy_name}")
            try:
                success = strategy_func(onnx_path, tflite_path, optimization)
                if success and os.path.exists(tflite_path):
                    result['success'] = True
                    result['tflite_path'] = tflite_path
                    result['strategy_used'] = strategy_name
                    result['file_size_mb'] = os.path.getsize(tflite_path) / (1024 * 1024)
                    
                    logger.info(f"âœ… Conversion successful with {strategy_name}")
                    logger.info(f"ğŸ“Š TFLite model size: {result['file_size_mb']:.2f} MB")
                    break
                    
            except Exception as e:
                logger.warning(f"âš ï¸ Strategy {strategy_name} failed: {str(e)}")
                continue
        
        if not result['success']:
            result['error'] = "All conversion strategies failed"
            logger.error("âŒ All conversion strategies failed")
        
        return result
    
    def _convert_via_onnx_tf_savedmodel(self, onnx_path: str, tflite_path: str, 
                                        optimization: str) -> bool:
        """
        Estrategia principal: ONNX â†’ TF SavedModel â†’ TFLite usando onnx-tf
        """
        logger.info("ğŸ”„ Using onnx-tf SavedModel strategy")
        
        try:
            import onnx
            import onnx_tf
            from onnx_tf.backend import prepare
            
            # Paso 1: Cargar modelo ONNX
            logger.info("ğŸ“¥ Loading ONNX model...")
            onnx_model = onnx.load(onnx_path)
            
            # Paso 2: Convertir ONNX â†’ TensorFlow usando onnx-tf
            logger.info("ğŸ”„ Converting ONNX â†’ TensorFlow using onnx-tf...")
            tf_rep = prepare(onnx_model)
            
            # Paso 3: Exportar como SavedModel
            saved_model_dir = os.path.join(self.temp_dir, "saved_model")
            logger.info(f"ğŸ’¾ Exporting TensorFlow SavedModel to {saved_model_dir}...")
            tf_rep.export_graph(saved_model_dir)
            
            # Verificar que el SavedModel se creÃ³
            if not os.path.exists(saved_model_dir):
                raise RuntimeError("SavedModel directory not created")
            
            # Paso 4: SavedModel â†’ TFLite
            logger.info("ğŸ”„ Converting SavedModel â†’ TFLite...")
            return self._savedmodel_to_tflite(saved_model_dir, tflite_path, optimization)
            
        except Exception as e:
            logger.error(f"onnx-tf SavedModel strategy failed: {e}")
            raise
    
    def _convert_via_onnx_tf_direct(self, onnx_path: str, tflite_path: str, 
                                    optimization: str) -> bool:
        """
        Estrategia alternativa: ONNX â†’ TensorFlow en memoria â†’ TFLite
        """
        logger.info("ğŸ”„ Using onnx-tf direct strategy")
        
        try:
            import onnx
            import onnx_tf
            from onnx_tf.backend import prepare
            
            # Cargar modelo ONNX
            onnx_model = onnx.load(onnx_path)
            
            # Convertir ONNX â†’ TensorFlow en memoria
            tf_rep = prepare(onnx_model)
            
            # Obtener el modelo TensorFlow
            tf_graph = tf_rep.graph
            
            # Crear un modelo Keras temporal para TFLite
            # Nota: Este mÃ©todo puede requerir adaptaciÃ³n segÃºn la estructura del modelo
            
            # Para modelos complejos, mejor usar SavedModel
            saved_model_dir = os.path.join(self.temp_dir, "saved_model_direct")
            tf_rep.export_graph(saved_model_dir)
            
            return self._savedmodel_to_tflite(saved_model_dir, tflite_path, optimization)
            
        except Exception as e:
            logger.error(f"onnx-tf direct strategy failed: {e}")
            raise
    
    def _savedmodel_to_tflite(self, saved_model_dir: str, tflite_path: str, 
                              optimization: str) -> bool:
        """
        Convierte SavedModel a TFLite
        """
        try:
            # Crear convertidor desde SavedModel
            converter = tf.lite.TFLiteConverter.from_saved_model(saved_model_dir)
            
            # Configurar optimizaciones
            if optimization == "default":
                converter.optimizations = [tf.lite.Optimize.DEFAULT]
            elif optimization == "size":
                converter.optimizations = [tf.lite.Optimize.OPTIMIZE_FOR_SIZE]
            elif optimization == "latency":
                converter.optimizations = [tf.lite.Optimize.OPTIMIZE_FOR_LATENCY]
            # 'none' no aÃ±ade optimizaciones
            
            # Configuraciones adicionales para compatibilidad
            converter.allow_custom_ops = True
            converter.experimental_new_converter = True
            
            # Realizar conversiÃ³n
            logger.info("ğŸ”„ Converting to TFLite...")
            tflite_model = converter.convert()
            
            # Guardar modelo
            with open(tflite_path, 'wb') as f:
                f.write(tflite_model)
            
            logger.info(f"âœ… SavedModelâ†’TFLite conversion successful")
            return True
            
        except Exception as e:
            logger.error(f"SavedModelâ†’TFLite conversion failed: {e}")
            return False

def convert_onnx_to_tflite_corrected(onnx_path: str, tflite_path: str, 
                                     optimization: str = "default") -> Dict[str, Any]:
    """
    FunciÃ³n de conveniencia para conversiÃ³n ONNXâ†’TFLite usando onnx-tf (correcto)
    
    Args:
        onnx_path: Ruta al modelo ONNX
        tflite_path: Ruta de salida para el modelo TFLite
        optimization: Tipo de optimizaciÃ³n ('default', 'size', 'latency', 'none')
    
    Returns:
        Dict con resultados de la conversiÃ³n
    """
    with CorrectedONNXToTFLiteConverter() as converter:
        return converter.convert(onnx_path, tflite_path, optimization)

def main():
    """Test del convertidor correcto"""
    import argparse
    
    parser = argparse.ArgumentParser(description='Corrected ONNX to TFLite Converter using onnx-tf')
    parser.add_argument('--onnx_path', type=str, required=True,
                        help='Path to ONNX model')
    parser.add_argument('--tflite_path', type=str, required=True,
                        help='Output path for TFLite model')
    parser.add_argument('--optimization', type=str, default='default',
                        choices=['default', 'size', 'latency', 'none'],
                        help='Optimization type')
    
    args = parser.parse_args()
    
    # Verificar dependencias
    if not ONNX_TF_AVAILABLE:
        logger.error("âŒ onnx-tf not available. Please install it first:")
        logger.error("   pip install onnx-tf==1.10.0")
        return 1
    
    # Ejecutar conversiÃ³n
    logger.info(f"ğŸš€ Starting corrected ONNXâ†’TFLite conversion")
    logger.info(f"ğŸ“¥ Input: {args.onnx_path}")
    logger.info(f"ğŸ“¤ Output: {args.tflite_path}")
    logger.info(f"âš™ï¸ Optimization: {args.optimization}")
    logger.info(f"ğŸ”§ Using onnx-tf backend (correct method)")
    
    result = convert_onnx_to_tflite_corrected(
        args.onnx_path, 
        args.tflite_path, 
        args.optimization
    )
    
    # Mostrar resultados
    if result['success']:
        logger.info("ğŸ‰ Conversion completed successfully!")
        logger.info(f"âœ… Strategy used: {result['strategy_used']}")
        logger.info(f"ğŸ“Š Output file size: {result['file_size_mb']:.2f} MB")
        logger.info(f"ğŸ“ TFLite model saved to: {result['tflite_path']}")
        logger.info("ğŸ” Model weights preserved from original ONNX")
    else:
        logger.error("âŒ Conversion failed!")
        if result['error']:
            logger.error(f"ğŸ’¥ Error: {result['error']}")
        return 1
    
    return 0

if __name__ == "__main__":
    exit(main())
