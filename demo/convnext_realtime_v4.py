#!/usr/bin/env python3
"""
convnext_realtime_v4_ULTRA_OPTIMIZED.py - VERSI√ìN ULTRA OPTIMIZADA

OBJETIVO: Modelos XS/S de ConvNeXtPose a tiempo real mediante:
1. ‚úÖ ConvNeXt ‚Üí ONNX ‚Üí TensorFlow Lite conversion
2. ‚úÖ Quantizaci√≥n INT8 para m√°ximo rendimiento  
3. ‚úÖ Pipeline completamente optimizado para XS/S
4. ‚úÖ Cache inteligente + funciones as√≠ncronas de v3
5. ‚úÖ Benchmark real contra modelos L/M
"""

import argparse
import time
import sys
from pathlib import Path
import os
import warnings
import logging
from typing import Optional, Dict, Any, Tuple, List

# Configurar logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)
warnings.filterwarnings("ignore", category=UserWarning)

# IMPORTS OPTIMIZADOS
import cv2
import numpy as np
import torch
import torchvision.transforms as transforms
import onnxruntime as ort
from collections import deque
import asyncio
from concurrent.futures import ThreadPoolExecutor

# TensorFlow Lite
try:
    import tensorflow as tf
    TFLITE_AVAILABLE = True
    logger.info("‚úÖ TensorFlow Lite disponible")
except ImportError:
    TFLITE_AVAILABLE = False
    logger.warning("‚ö†Ô∏è TensorFlow Lite no disponible")

# IMPORTS DEL PROYECTO
ROOT = Path(__file__).resolve().parent
PROJECT_ROOT = ROOT.parent
sys.path.extend([
    str(PROJECT_ROOT / 'main'),
    str(PROJECT_ROOT / 'data'),
    str(PROJECT_ROOT / 'common')
])

from config import cfg
from model import get_pose_net
from dataset import generate_patch_image
import utils.pose_utils as pose_utils

def detect_model_size(model_path: str) -> str:
    """Detectar tama√±o del modelo ConvNeXt"""
    model_name = Path(model_path).stem.lower()
    
    if 'xs' in model_name or 'nano' in model_name:
        return 'XS'
    elif 's' in model_name or 'small' in model_name:
        return 'S'
    elif 'm' in model_name or 'medium' in model_name:
        return 'M'
    elif 'l' in model_name or 'large' in model_name:
        return 'L'
    else:
        # Por defecto asumir S si no se puede determinar
        return 'S'

class ConvNeXtONNXConverter:
    """Conversor ConvNeXt ‚Üí ONNX optimizado"""
    
    def __init__(self, model_path: str, model_size: str):
        self.model_path = model_path
        self.model_size = model_size
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        
    def convert_to_onnx(self, output_path: Optional[str] = None) -> str:
        """Convertir ConvNeXt a ONNX optimizado"""
        
        if output_path is None:
            output_path = self.model_path.replace('.pth', f'_{self.model_size}_optimized.onnx')
        
        if os.path.exists(output_path):
            logger.info(f"‚úÖ ONNX ya existe: {output_path}")
            return output_path
            
        logger.info(f"üîÑ Convirtiendo ConvNeXt {self.model_size} a ONNX...")
        
        try:
            # Configurar ConvNeXt
            cfg.input_shape = (256, 256)
            cfg.output_shape = (32, 32) if self.model_size in ['XS', 'S'] else (64, 64)
            cfg.depth_dim = 32 if self.model_size in ['XS', 'S'] else 64
            cfg.bbox_3d_shape = (2000, 2000, 2000)
            
            # Cargar modelo
            model = get_pose_net(cfg, is_train=False, joint_num=18)
            state = torch.load(self.model_path, map_location=self.device)
            sd = state.get('network', state)
            model.load_state_dict(sd, strict=False)
            model = model.to(self.device).eval()
            
            # Crear entrada dummy
            dummy_input = torch.randn(1, 3, 256, 256).to(self.device)
            
            # Exportar a ONNX con optimizaciones
            torch.onnx.export(
                model,
                dummy_input,
                output_path,
                export_params=True,
                opset_version=12,
                do_constant_folding=True,
                input_names=['input'],
                output_names=['pose_coords'],
                dynamic_axes={
                    'input': {0: 'batch_size'},
                    'pose_coords': {0: 'batch_size'}
                }
            )
            
            logger.info(f"‚úÖ ONNX creado: {output_path}")
            return output_path
            
        except Exception as e:
            logger.error(f"‚ùå Error en conversi√≥n ONNX: {e}")
            raise

class ConvNeXtTFLiteConverter:
    """Conversor ONNX ‚Üí TensorFlow Lite con quantizaci√≥n"""
    
    def __init__(self, onnx_path: str, model_size: str):
        self.onnx_path = onnx_path
        self.model_size = model_size
        
    def convert_to_tflite(self, quantize: bool = True) -> str:
        """Convertir ONNX a TensorFlow Lite"""
        
        if not TFLITE_AVAILABLE:
            raise RuntimeError("TensorFlow Lite no disponible")
            
        tflite_path = self.onnx_path.replace('.onnx', '_quantized.tflite' if quantize else '.tflite')
        
        if os.path.exists(tflite_path):
            logger.info(f"‚úÖ TFLite ya existe: {tflite_path}")
            return tflite_path
            
        logger.info(f"üîÑ Convirtiendo a TensorFlow Lite (quantize={quantize})...")
        
        try:
            # Primero ONNX ‚Üí TensorFlow
            import onnx
            from onnx_tf.backend import prepare
            
            onnx_model = onnx.load(self.onnx_path)
            tf_rep = prepare(onnx_model)
            
            # Guardar modelo TensorFlow temporal
            tf_model_path = self.onnx_path.replace('.onnx', '_temp_tf')
            tf_rep.export_graph(tf_model_path)
            
            # Convertir a TFLite
            converter = tf.lite.TFLiteConverter.from_saved_model(tf_model_path)
            
            if quantize:
                # Quantizaci√≥n INT8 para m√°ximo rendimiento
                converter.optimizations = [tf.lite.Optimize.DEFAULT]
                converter.target_spec.supported_types = [tf.int8]
                
                # Dataset representativo para quantizaci√≥n
                def representative_dataset():
                    for _ in range(100):
                        yield [np.random.rand(1, 3, 256, 256).astype(np.float32)]
                
                converter.representative_dataset = representative_dataset
                converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]
                converter.inference_input_type = tf.uint8
                converter.inference_output_type = tf.uint8
            
            # Convertir
            tflite_model = converter.convert()
            
            # Guardar
            with open(tflite_path, 'wb') as f:
                f.write(tflite_model)
            
            # Limpiar archivos temporales
            import shutil
            if os.path.exists(tf_model_path):
                shutil.rmtree(tf_model_path)
                
            logger.info(f"‚úÖ TFLite creado: {tflite_path}")
            return tflite_path
            
        except Exception as e:
            logger.error(f"‚ùå Error en conversi√≥n TFLite: {e}")
            raise

class UltraOptimizedPoseProcessor:
    """Procesador ultra optimizado para modelos XS/S"""
    
    def __init__(self, model_path: str, use_tflite: bool = True):
        self.model_path = model_path
        self.model_size = detect_model_size(model_path)
        self.use_tflite = use_tflite and TFLITE_AVAILABLE
        
        # Cache de v3
        self.pose_cache = {}
        self.cache_timeout = 0.1  # M√°s agresivo para XS/S
        self.max_cache_size = 50
        
        # M√©tricas
        self.processing_times = deque(maxlen=100)
        self.cache_hits = 0
        self.cache_misses = 0
        
        self._setup_model()
    
    def _setup_model(self):
        """Configurar modelo optimizado"""
        logger.info(f"üöÄ Configurando modelo {self.model_size} ultra-optimizado...")
        
        if self.use_tflite:
            # Ruta completa de conversi√≥n
            onnx_converter = ConvNeXtONNXConverter(self.model_path, self.model_size)
            onnx_path = onnx_converter.convert_to_onnx()
            
            tflite_converter = ConvNeXtTFLiteConverter(onnx_path, self.model_size)
            self.tflite_path = tflite_converter.convert_to_tflite(quantize=True)
            
            # Cargar int√©rprete TFLite
            self.interpreter = tf.lite.Interpreter(model_path=self.tflite_path)
            self.interpreter.allocate_tensors()
            
            self.input_details = self.interpreter.get_input_details()
            self.output_details = self.interpreter.get_output_details()
            
            logger.info(f"‚úÖ TensorFlow Lite cargado: {self.tflite_path}")
            
        else:
            # Fallback a ONNX
            onnx_converter = ConvNeXtONNXConverter(self.model_path, self.model_size)
            onnx_path = onnx_converter.convert_to_onnx()
            
            # Configurar ONNX Runtime con optimizaciones m√°ximas
            providers = ['CUDAExecutionProvider', 'CPUExecutionProvider'] if torch.cuda.is_available() else ['CPUExecutionProvider']
            
            session_options = ort.SessionOptions()
            session_options.graph_optimization_level = ort.GraphOptimizationLevel.ORT_ENABLE_ALL
            session_options.execution_mode = ort.ExecutionMode.ORT_PARALLEL
            
            self.session = ort.InferenceSession(onnx_path, providers=providers, sess_options=session_options)
            
            logger.info(f"‚úÖ ONNX Runtime cargado: {onnx_path}")
    
    def _generate_cache_key(self, bbox: List[int], frame_time: float) -> str:
        """Cache key optimizado"""
        x1, y1, x2, y2 = bbox
        # Quantizaci√≥n m√°s agresiva para XS/S
        q = 25 if self.model_size == 'XS' else 20
        x1_q = int(x1 / q) * q
        y1_q = int(y1 / q) * q
        x2_q = int(x2 / q) * q
        y2_q = int(y2 / q) * q
        time_slot = int(frame_time / self.cache_timeout)
        return f"{x1_q}_{y1_q}_{x2_q}_{y2_q}_{time_slot}"
    
    async def process_pose_ultra_fast(self, frame: np.ndarray, bbox: List[int], frame_time: float) -> Optional[np.ndarray]:
        """Procesamiento ultra r√°pido para XS/S"""
        
        # Cache check
        cache_key = self._generate_cache_key(bbox, frame_time)
        if cache_key in self.pose_cache:
            cached_result, cached_time = self.pose_cache[cache_key]
            if frame_time - cached_time < self.cache_timeout:
                self.cache_hits += 1
                return cached_result
        
        self.cache_misses += 1
        start_time = time.time()
        
        try:
            # Preparar entrada
            x1, y1, x2, y2 = bbox
            bbox_array = np.array([x1, y1, x2 - x1, y2 - y1])
            
            proc_bbox = pose_utils.process_bbox(bbox_array, frame.shape[1], frame.shape[0])
            if proc_bbox is None:
                return None
            
            img_patch, img2bb_trans = generate_patch_image(
                frame, proc_bbox, False, 1.0, 0.0, False
            )
            
            # Preparar tensor de entrada
            transform = transforms.Compose([
                transforms.ToTensor(),
                transforms.Normalize(mean=cfg.pixel_mean, std=cfg.pixel_std),
            ])
            
            input_tensor = transform(img_patch).unsqueeze(0).numpy()
            
            # Inferencia optimizada
            if self.use_tflite:
                # TensorFlow Lite (m√°s r√°pido)
                self.interpreter.set_tensor(self.input_details[0]['index'], input_tensor.astype(np.float32))
                self.interpreter.invoke()
                pose_coords = self.interpreter.get_tensor(self.output_details[0]['index'])
            else:
                # ONNX Runtime
                pose_coords = self.session.run(None, {'input': input_tensor.astype(np.float32)})[0]
            
            # Post-procesamiento r√°pido
            if pose_coords is not None:
                pose_coords = pose_coords[0]  # Remove batch dimension
                
                # Transformaci√≥n b√°sica (simplificada para velocidad)
                pose_coords[:, 0] = pose_coords[:, 0] / cfg.output_shape[1] * cfg.input_shape[1]
                pose_coords[:, 1] = pose_coords[:, 1] / cfg.output_shape[0] * cfg.input_shape[0]
                
                # Transformaci√≥n af√≠n b√°sica
                pose_2d_coords = pose_coords[:, :2]
                pose_2d_homo = np.column_stack((pose_2d_coords, np.ones(len(pose_2d_coords))))
                img2bb_trans_full = np.vstack((img2bb_trans, [0, 0, 1]))
                
                try:
                    final_coords = np.linalg.solve(img2bb_trans_full, pose_2d_homo.T).T[:, :2]
                except:
                    final_coords = pose_2d_coords
                
                # Update cache
                processing_time = time.time() - start_time
                self.processing_times.append(processing_time)
                self.pose_cache[cache_key] = (final_coords, frame_time)
                
                # Limpieza de cache
                if len(self.pose_cache) > self.max_cache_size:
                    old_keys = [k for k, (_, t) in self.pose_cache.items() 
                               if frame_time - t > self.cache_timeout * 3]
                    for k in old_keys:
                        del self.pose_cache[k]
                
                return final_coords
            
        except Exception as e:
            logger.error(f"‚ùå Error en pose processing: {e}")
            return None
        
        return None
    
    def get_performance_stats(self) -> Dict[str, Any]:
        """Estad√≠sticas de rendimiento"""
        total_requests = self.cache_hits + self.cache_misses
        hit_rate = (self.cache_hits / total_requests * 100) if total_requests > 0 else 0
        
        avg_time = 0
        if self.processing_times:
            avg_time = sum(self.processing_times) / len(self.processing_times) * 1000
        
        return {
            'model_size': self.model_size,
            'engine': 'TensorFlow Lite' if self.use_tflite else 'ONNX Runtime',
            'cache_hit_rate': hit_rate,
            'avg_processing_time_ms': avg_time,
            'cache_size': len(self.pose_cache),
            'total_processed': total_requests
        }

def benchmark_model_performance(processor: UltraOptimizedPoseProcessor, test_frames: int = 100):
    """Benchmark de rendimiento del modelo"""
    logger.info(f"üèÅ Benchmarking modelo {processor.model_size}...")
    
    # Crear frames de prueba
    test_frame = np.random.randint(0, 255, (480, 640, 3), dtype=np.uint8)
    test_bbox = [100, 100, 200, 300]  # bbox t√≠pico
    
    times = []
    successful_inferences = 0
    
    for i in range(test_frames):
        start_time = time.time()
        
        # Simular procesamiento real
        result = asyncio.run(processor.process_pose_ultra_fast(test_frame, test_bbox, time.time()))
        
        end_time = time.time()
        
        if result is not None:
            successful_inferences += 1
            times.append((end_time - start_time) * 1000)  # ms
    
    if times:
        avg_time = sum(times) / len(times)
        min_time = min(times)
        max_time = max(times)
        fps_estimate = 1000 / avg_time if avg_time > 0 else 0
        
        logger.info(f"üìä BENCHMARK RESULTS:")
        logger.info(f"   Modelo: {processor.model_size}")
        logger.info(f"   Engine: {'TensorFlow Lite' if processor.use_tflite else 'ONNX Runtime'}")
        logger.info(f"   Successful inferences: {successful_inferences}/{test_frames}")
        logger.info(f"   Avg time: {avg_time:.2f}ms")
        logger.info(f"   Min time: {min_time:.2f}ms")
        logger.info(f"   Max time: {max_time:.2f}ms")
        logger.info(f"   Estimated FPS: {fps_estimate:.1f}")
        
        return {
            'model_size': processor.model_size,
            'avg_time_ms': avg_time,
            'estimated_fps': fps_estimate,
            'success_rate': successful_inferences / test_frames
        }
    
    return None

async def main_ultra_optimized():
    """Main ultra optimizado para modelos XS/S"""
    parser = argparse.ArgumentParser(description="ConvNeXt v4 ULTRA OPTIMIZADO - TensorFlow Lite")
    parser.add_argument('--pose-model', type=str, required=True, help='ConvNeXt XS/S checkpoint')
    parser.add_argument('--input', type=str, default='0', help='Video source')
    parser.add_argument('--use-tflite', action='store_true', default=True, help='Usar TensorFlow Lite')
    parser.add_argument('--benchmark', action='store_true', help='Ejecutar benchmark')
    args = parser.parse_args()
    
    # Configurar procesador ultra optimizado
    processor = UltraOptimizedPoseProcessor(args.pose_model, use_tflite=args.use_tflite)
    
    # Benchmark opcional
    if args.benchmark:
        benchmark_results = benchmark_model_performance(processor)
        return
    
    # YOLO simple (mantenemos el de v3)
    from convnext_realtime_v3 import ModernYOLODetector, convert_yolo_to_onnx_safe, detect_hardware_capabilities
    
    hardware_caps = detect_hardware_capabilities()
    onnx_path = convert_yolo_to_onnx_safe('yolov8n.pt')
    yolo_detector = ModernYOLODetector(onnx_path, hardware_caps)
    
    # Configurar captura
    if args.input == '0':
        cap = cv2.VideoCapture(0)
    elif args.input.isdigit():
        cap = cv2.VideoCapture(int(args.input))
    else:
        cap = cv2.VideoCapture(args.input)
    
    if not cap.isOpened():
        logger.error(f"‚ùå No se pudo abrir video: {args.input}")
        return
    
    cap.set(cv2.CAP_PROP_BUFFERSIZE, 1)
    
    # Esqueleto
    skeleton = [
        (10, 9), (9, 8), (8, 11), (8, 14),
        (11, 12), (12, 13), (14, 15), (15, 16),
        (11, 4), (14, 1), (0, 4), (0, 1),
        (4, 5), (5, 6), (1, 2), (2, 3)
    ]
    
    logger.info(f"üöÄ Demo v4 ULTRA iniciado - Modelo {processor.model_size}")
    
    frame_count = 0
    fps_counter = deque(maxlen=30)
    
    try:
        while True:
            frame_start = time.time()
            
            ret, frame = cap.read()
            if not ret:
                break
            
            try:
                # YOLO detection
                bboxes = await yolo_detector.detect_persons_async(frame)
                
                # Ultra fast pose processing
                if bboxes:
                    best_bbox = bboxes[0]
                    pose_coords = await processor.process_pose_ultra_fast(frame, best_bbox, time.time())
                    
                    if pose_coords is not None:
                        # Clip coordinates
                        pose_coords = np.clip(pose_coords, 0, [frame.shape[1]-1, frame.shape[0]-1])
                        
                        # Draw skeleton
                        for i, j in skeleton:
                            if i < len(pose_coords) and j < len(pose_coords):
                                pt1 = tuple(map(int, pose_coords[i]))
                                pt2 = tuple(map(int, pose_coords[j]))
                                cv2.line(frame, pt1, pt2, (0, 255, 0), 2)
                        
                        # Draw joints
                        for point in pose_coords:
                            cv2.circle(frame, tuple(map(int, point)), 3, (0, 255, 0), -1)
                    
                    # Draw bbox
                    x1, y1, x2, y2 = best_bbox
                    cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 0), 2)
                
            except Exception as e:
                logger.error(f"‚ùå Error: {e}")
            
            # FPS calculation
            frame_time = time.time() - frame_start
            fps_counter.append(1.0 / max(frame_time, 1e-6))
            
            if fps_counter:
                fps = sum(fps_counter) / len(fps_counter)
                
            # Stats display
            stats = processor.get_performance_stats()
            
            cv2.putText(frame, f"v4-ULTRA FPS: {fps:.1f}", (10, 30),
                       cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 255, 0), 2)
            cv2.putText(frame, f"Model: {stats['model_size']} ({stats['engine']})", (10, 70),
                       cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 0), 2)
            cv2.putText(frame, f"Proc: {stats['avg_processing_time_ms']:.1f}ms", (10, 110),
                       cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 0), 2)
            cv2.putText(frame, f"Cache: {stats['cache_hit_rate']:.1f}%", (10, 150),
                       cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 0), 2)
            
            cv2.imshow("ConvNeXt v4 ULTRA - TensorFlow Lite", frame)
            
            frame_count += 1
            
            # Log stats
            if frame_count % 60 == 0:
                logger.info(f"üìä Frame {frame_count}: FPS={fps:.1f}, "
                           f"Engine={stats['engine']}, "
                           f"Proc={stats['avg_processing_time_ms']:.1f}ms, "
                           f"Cache={stats['cache_hit_rate']:.1f}%")
            
            if cv2.waitKey(1) & 0xFF == ord('q'):
                break
                
    finally:
        cap.release()
        cv2.destroyAllWindows()
        
        # Final stats
        final_stats = processor.get_performance_stats()
        logger.info("üèÅ DEMO FINALIZADO")
        logger.info(f"üìä ESTAD√çSTICAS FINALES:")
        logger.info(f"   Modelo: {final_stats['model_size']}")
        logger.info(f"   Engine: {final_stats['engine']}")
        logger.info(f"   Frames procesados: {final_stats['total_processed']}")
        logger.info(f"   Cache hit rate: {final_stats['cache_hit_rate']:.1f}%")
        logger.info(f"   Tiempo promedio: {final_stats['avg_processing_time_ms']:.1f}ms")

def main():
    """Entry point"""
    try:
        asyncio.run(main_ultra_optimized())
    except KeyboardInterrupt:
        logger.info("\nüõë Detenido por usuario")
    except Exception as e:
        logger.error(f"‚ùå Error: {e}")

if __name__ == "__main__":
    main()