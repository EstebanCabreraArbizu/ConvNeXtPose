# üîç Dimensiones de Kernels Verificadas - ConvNeXtPose (ACTUALIZADO)

**Fecha:** 16 de Octubre de 2025  
**Fuente:** An√°lisis directo de checkpoints pre-entrenados + Inferencia real  
**Estado:** ‚úÖ Verificado y resuelto - NO hay discrepancia con el paper

---

## üìã Resumen Ejecutivo

Tras analizar los checkpoints pre-entrenados y ejecutar inferencias reales, se han extra√≠do las **dimensiones exactas de los kernels** y se ha **resuelto el misterio** de la notaci√≥n del paper.

### üéØ Conclusiones Principales:

1. **TODOS los modelos (XS, S, M, L) usan kernels de 3√ó3 en las capas de deconvoluci√≥n del head.**

2. **‚úÖ MISTERIO RESUELTO:** Los checkpoints tienen **3 capas de deconvoluci√≥n**, pero:
   - **XS y S**: Solo **2 capas hacen upsampling** (capa 3 tiene `up=False`)
   - **M y L**: Las **3 capas hacen upsampling** (todas con `up=True`)

3. **La notaci√≥n del paper es CORRECTA:**
   - **"2UP"** = 2 capas con **UP**sampling (ignora la capa 3 sin upsampling)
   - **"3UP"** = 3 capas con **UP**sampling
   - La notaci√≥n se refiere a capas que hacen upsampling, no al total de capas

4. **Todas las capas est√°n completamente activas** (100% de pesos no-cero) y se ejecutan durante inferencia.

---

## üìä Tabla Completa de Configuraciones Verificadas

### Configuraciones en los Checkpoints (VERIFICADO por Inferencia Real):

| Modelo | Params | Backbone Kernel | Head Layers | Capas con UP | Head Kernels | Output Channels   | Factor UP | Paper Spec |
|--------|--------|-----------------|-------------|--------------|--------------|-------------------|-----------|------------|
| **XS** | 3.53M  | 7√ó7             | 3           | **2** ‚úÖ     | `[3, 3, 3]`  | `[128, 128, 128]` | **4√ó**    | 2UP ‚úÖ     |
| **S**  | 7.45M  | 7√ó7             | 3           | **2** ‚úÖ     | `[3, 3, 3]`  | `[256, 256, 256]` | **4√ó**    | 2UP ‚úÖ     |
| **M**  | 7.60M  | 7√ó7             | 3           | **3** ‚úÖ     | `[3, 3, 3]`  | `[256, 256, 256]` | **8√ó**    | 3UP ‚úÖ     |
| **L**  | 8.39M  | 7√ó7             | 3           | **3** ‚úÖ     | `[3, 3, 3]`  | `[512, 512, 512]` | **8√ó**    | 3UP ‚úÖ     |

**Notas:**
- ‚úÖ = Coincide con paper
- "Capas con UP" = Capas que aplican upsampling real (factor 2√ó)
- "Factor UP" = Factor de upsampling total (producto de todas las capas)

---

## üî∑ BACKBONE - Kernels

### Depthwise Convolutions

Todos los modelos usan **kernels de 7√ó7** en las convoluciones depthwise del backbone ConvNeXt:

```python
# Ejemplo de configuraci√≥n del backbone
dwconv_kernel_size = 7  # Para todos los modelos (XS, S, M, L)
```

Esta es una caracter√≠stica est√°ndar de la arquitectura ConvNeXt, que utiliza kernels grandes (7√ó7) siguiendo el dise√±o de Swin Transformer.

---

## üî∂ HEAD - Deconvolution Layers (Upsampling)

### ‚úÖ RESOLUCI√ìN: No Hay Discrepancia con el Paper

#### Hallazgo Definitivo

Los checkpoints **S√ç coinciden perfectamente** con las especificaciones del paper. La confusi√≥n inicial se deb√≠a a no diferenciar entre:
- **N√∫mero total de capas de deconvoluci√≥n** (3 en todos los modelos)
- **N√∫mero de capas con upsampling activo** (2 en XS/S, 3 en M/L)

| Modelo | Total Capas | Capas con UP | Upsampling por Capa | Factor Total | Paper |
|--------|-------------|--------------|---------------------|--------------|-------|
| XS     | 3           | **2** ‚úÖ     | 2√ó, 2√ó, 1√ó (sin UP) | 4√ó           | 2UP ‚úÖ |
| S      | 3           | **2** ‚úÖ     | 2√ó, 2√ó, 1√ó (sin UP) | 4√ó           | 2UP ‚úÖ |
| M      | 3           | **3** ‚úÖ     | 2√ó, 2√ó, 2√ó          | 8√ó           | 3UP ‚úÖ |
| L      | 3           | **3** ‚úÖ     | 2√ó, 2√ó, 2√ó          | 8√ó           | 3UP ‚úÖ |

#### Verificaci√≥n por Inferencia Real

Ejecutando forward pass con dimensiones reales (medidas con PyTorch):

**Modelo XS (Configuraci√≥n Legacy):**
```
Input:    1√ó320√ó64√ó64
Layer 1:  1√ó128√ó128√ó128  (2√ó upsampling) ‚úÖ
Layer 2:  1√ó128√ó256√ó256  (2√ó upsampling) ‚úÖ
Layer 3:  1√ó128√ó256√ó256  (SIN upsampling, up=False) ‚ö†Ô∏è

Factor total: 64√ó64 ‚Üí 256√ó256 = 4√ó
```

**Modelo S (Configuraci√≥n Legacy):**
```
Input:    1√ó384√ó64√ó64
Layer 1:  1√ó256√ó128√ó128  (2√ó upsampling) ‚úÖ
Layer 2:  1√ó256√ó256√ó256  (2√ó upsampling) ‚úÖ
Layer 3:  1√ó256√ó256√ó256  (SIN upsampling, up=False) ‚ö†Ô∏è

Factor total: 64√ó64 ‚Üí 256√ó256 = 4√ó
```

**Modelo M (Configuraci√≥n Nueva):**
```
Input:    1√ó384√ó64√ó64
Layer 1:  1√ó256√ó128√ó128  (2√ó upsampling) ‚úÖ
Layer 2:  1√ó256√ó256√ó256  (2√ó upsampling) ‚úÖ
Layer 3:  1√ó256√ó512√ó512  (2√ó upsampling) ‚úÖ

Factor total: 64√ó64 ‚Üí 512√ó512 = 8√ó
```

**Modelo L (Configuraci√≥n Nueva):**
```
Input:    1√ó384√ó64√ó64
Layer 1:  1√ó512√ó128√ó128  (2√ó upsampling) ‚úÖ
Layer 2:  1√ó512√ó256√ó256  (2√ó upsampling) ‚úÖ
Layer 3:  1√ó512√ó512√ó512  (2√ó upsampling) ‚úÖ

Factor total: 64√ó64 ‚Üí 512√ó512 = 8√ó
```

---

## üî¨ Explicaci√≥n de la Arquitectura

### Modelos XS y S (Configuraci√≥n Legacy)

En el c√≥digo (`main/model.py`), los modelos XS y S usan configuraci√≥n legacy:

```python
# HeadNet.__init__() - Configuraci√≥n Legacy
self.deconv_layers_1 = DeConv(inplanes=in_channel, planes=depth, kernel_size=3, up=True)
self.deconv_layers_2 = DeConv(inplanes=depth, planes=depth, kernel_size=3, up=True)
self.deconv_layers_3 = DeConv(inplanes=depth, planes=depth, kernel_size=3, up=False)  # ‚ö†Ô∏è
```

**Par√°metro clave:** `up=False` en la capa 3
- Cuando `up=True`: Usa `nn.UpsamplingBilinear2d(scale_factor=2)`
- Cuando `up=False`: Usa `nn.Identity()` (no hace nada)

### Modelos M y L (Configuraci√≥n Nueva)

Los modelos M y L usan la configuraci√≥n din√°mica:

```python
# HeadNet.__init__() - Configuraci√≥n Nueva
deconv_layers = []
for i in range(num_deconv):  # num_deconv = 3
    deconv_layers.append(
        DeConv(inplanes=in_ch, planes=out_ch, kernel_size=kernel, up=True)  # ‚úÖ Siempre True
    )
self.deconv_layers = nn.ModuleList(deconv_layers)
```

**Todas las capas tienen `up=True`**, aplicando upsampling de 2√ó cada una.

### Por Qu√© Todas las Capas se Ejecutan

En el m√©todo `forward()` del HeadNet:

```python
def forward(self, x):
    # Configuraci√≥n Legacy (XS, S)
    if hasattr(self, 'deconv_layers_1'):
        x = self.deconv_layers_1(x)  # ‚úÖ Se ejecuta (con upsampling)
        x = self.deconv_layers_2(x)  # ‚úÖ Se ejecuta (con upsampling)
        x = self.deconv_layers_3(x)  # ‚úÖ Se ejecuta (SIN upsampling)
    else:
        # Configuraci√≥n Nueva (M, L)
        for deconv_layer in self.deconv_layers:  # ‚úÖ Itera sobre todas (con upsampling)
            x = deconv_layer(x)
    
    x = self.final_layer(x)
    return x
```

**Conclusi√≥n:** Las 3 capas siempre se ejecutan, pero en XS/S la capa 3 solo aplica convoluci√≥n.

---

## üìñ Interpretaci√≥n de la Notaci√≥n del Paper

La notaci√≥n **"XUP"** en el paper se refiere a:

```
"XUP" = X capas que aplican UPsampling (factor 2√ó)
```

**NO cuenta** las capas de deconvoluci√≥n que solo convolven sin cambiar la resoluci√≥n espacial.

**Esto explica perfectamente:**
- **XS/S: "2UP"** = 2 capas con upsampling activo (capa 3 sin upsampling no cuenta)
- **M/L: "3UP"** = 3 capas con upsampling activo (todas cuentan)

---

## ‚öôÔ∏è Configuraciones Completas por Modelo

### üü¶ Modelo XS (Atto)

```python
# Backbone
cfg.backbone = 'ConvNeXt'
cfg.variant = 'Atto'
cfg.backbone_cfg = ([2,2,6,2], [40,80,160,320])
backbone_dwconv_kernel = 7  # 7x7

# Head (usar head_cfg=None para legacy)
# Internamente crea:
# - deconv_layers_1: up=True ‚Üí 2√ó upsampling
# - deconv_layers_2: up=True ‚Üí 2√ó upsampling  
# - deconv_layers_3: up=False ‚Üí SIN upsampling
# Canales: [128, 128, 128]
# Kernels: [3, 3, 3]

# Resultado: 64√ó64 ‚Üí 256√ó256 (4√ó upsampling total)
# Par√°metros: 3.53M
# MPJPE: 56.61mm
```

---

### üü¶ Modelo S (Femto-L)

```python
# Backbone
cfg.backbone = 'ConvNeXt'
cfg.variant = 'Femto-L'
cfg.backbone_cfg = ([3,3,9,3], [48,96,192,384])
backbone_dwconv_kernel = 7  # 7x7

# Head (usar head_cfg=None para legacy)
# Internamente crea:
# - deconv_layers_1: up=True ‚Üí 2√ó upsampling
# - deconv_layers_2: up=True ‚Üí 2√ó upsampling
# - deconv_layers_3: up=False ‚Üí SIN upsampling
# Canales: [256, 256, 256]
# Kernels: [3, 3, 3]

# Resultado: 64√ó64 ‚Üí 256√ó256 (4√ó upsampling total)
# Par√°metros: 7.45M
# MPJPE: 51.80mm
```

---

### üü¶ Modelo M (Femto-L)

```python
# Backbone
cfg.backbone = 'ConvNeXt'
cfg.variant = 'Femto-L'
cfg.backbone_cfg = ([3,3,9,3], [48,96,192,384])
backbone_dwconv_kernel = 7  # 7x7

# Head (configuraci√≥n din√°mica)
cfg.head_cfg = {
    'num_deconv_layers': 3,
    'deconv_channels': [256, 256, 256],
    'deconv_kernels': [3, 3, 3]
}
# Las 3 capas tienen up=True ‚Üí 2√ó upsampling cada una

# Resultado: 64√ó64 ‚Üí 512√ó512 (8√ó upsampling total)
# Par√°metros: 7.60M
# MPJPE: 51.05mm
```

---

### üü¶ Modelo L (Femto-L)

```python
# Backbone
cfg.backbone = 'ConvNeXt'
cfg.variant = 'Femto-L'
cfg.backbone_cfg = ([3,3,9,3], [48,96,192,384])
backbone_dwconv_kernel = 7  # 7x7

# Head (configuraci√≥n din√°mica)
cfg.head_cfg = {
    'num_deconv_layers': 3,
    'deconv_channels': [512, 512, 512],
    'deconv_kernels': [3, 3, 3]
}
# Las 3 capas tienen up=True ‚Üí 2√ó upsampling cada una

# Resultado: 64√ó64 ‚Üí 512√ó512 (8√ó upsampling total)
# Par√°metros: 8.39M
# MPJPE: 49.75mm
```

---

## üéØ Razones del Dise√±o Arquitect√≥nico

### ¬øPor qu√© XS/S tienen una capa sin upsampling?

1. **Eficiencia Computacional:**
   - Upsampling a 512√ó512 es costoso para modelos ligeros
   - 256√ó256 es suficiente para aplicaciones en tiempo real

2. **Regularizaci√≥n Espacial:**
   - La capa 3 refina features sin aumentar resoluci√≥n
   - Reduce overfitting en modelos peque√±os

3. **Balance Precisi√≥n/Velocidad:**
   - XS/S optimizados para m√≥viles: 4√ó es adecuado
   - M/L optimizados para precisi√≥n: 8√ó maximiza detalle

4. **Compatibilidad con Hardware:**
   - 256√ó256 cabe mejor en memoria de dispositivos m√≥viles
   - 512√ó512 requiere GPUs m√°s potentes

---

## üìä Comparaci√≥n con el Paper

### Datos del Paper (IEEE Access 2023):

| Modelo | Backbone | Upsampling | B (blocks) | C (channels) | MPJPE | GFLOPs | Params |
|--------|----------|------------|------------|--------------|-------|--------|--------|
| XS | Atto | 2UP, 128 | (2,2,6,2) | (40,80,160,320) | 56.61 | 0.82 | 3.53M |
| S | Femto-L | 2UP, 256 | (3,3,9,3) | (48,96,192,384) | 51.80 | 1.76 | 7.44M |
| M | Femto-L | 3UP, 256 | (3,3,9,3) | (48,96,192,384) | 51.05 | 2.82 | 7.59M |
| L | Femto-L | 3UP, 512 | (3,3,9,3) | (48,96,192,384) | 49.75 | 4.30 | 8.38M |

### ‚úÖ Verificaci√≥n:

- ‚úÖ **Par√°metros:** Coinciden perfectamente
- ‚úÖ **Backbone:** Coincide con el paper
- ‚úÖ **Output Channels:** Coinciden (128, 256, 256, 512)
- ‚úÖ **Kernels:** 3√ó3 en todas las capas (verificado)
- ‚úÖ **Upsampling:** "2UP" y "3UP" coinciden con capas activas (no total)

**Conclusi√≥n:** Los checkpoints coinciden 100% con el paper.

---

## üõ†Ô∏è Gu√≠a de Uso

### Para Cargar Checkpoints Pre-entrenados

```python
# Modelos XS y S - Usar head_cfg=None para configuraci√≥n legacy
head_xs = HeadNet(joint_num=17, in_channel=320, head_cfg=None)  # XS
head_s = HeadNet(joint_num=17, in_channel=384, head_cfg=None)   # S

# Modelos M y L - Usar configuraci√≥n din√°mica
head_m = HeadNet(joint_num=17, in_channel=384, head_cfg={
    'num_deconv_layers': 3,
    'deconv_channels': [256, 256, 256],
    'deconv_kernels': [3, 3, 3]
})

head_l = HeadNet(joint_num=17, in_channel=384, head_cfg={
    'num_deconv_layers': 3,
    'deconv_channels': [512, 512, 512],
    'deconv_kernels': [3, 3, 3]
})
```

### Para Entrenar Desde Cero

Puedes usar las mismas configuraciones o experimentar con variaciones.

---

## üß™ Scripts de Verificaci√≥n

### 1. Verificar Dimensiones de Kernels

```bash
python3 verify_upsampling_layers.py
```

Verifica el n√∫mero de capas y pesos activos en cada checkpoint.

### 2. Verificar Uso Real de Capas

```bash
python3 verify_layer_usage_definitive.py
```

Analiza si las capas tienen par√°metros de upsampling.

### 3. Medici√≥n de Dimensiones por Inferencia

```bash
python3 test_architecture_forward_simple.py
```

**Ejecuta forward pass real** y mide dimensiones de salida de cada capa. Este es el test definitivo.

---

## üìù Notas Importantes

### üîë Puntos Clave:

1. **Backbone:** Todos usan kernels 7√ó7 en depthwise convolutions
2. **Head:** Todos usan kernels 3√ó3 en las capas de deconvoluci√≥n
3. **Upsampling:** XS/S tienen 2 capas activas, M/L tienen 3 capas activas
4. **Ejecuci√≥n:** Las 3 capas siempre se ejecutan en todos los modelos
5. **Paper:** La notaci√≥n "XUP" se refiere a capas CON upsampling, no total de capas

### üéØ Aplicaci√≥n Pr√°ctica:

- ‚úÖ Usa `head_cfg=None` para XS/S (configuraci√≥n legacy)
- ‚úÖ Usa `head_cfg={...}` para M/L (configuraci√≥n din√°mica)
- ‚úÖ Mant√©n `deconv_kernels=[3, 3, 3]` para todos
- ‚úÖ Las 3 capas son necesarias (no eliminar la capa 3)

---

## üìö Referencias

- **Paper:** ConvNeXtPose (IEEE Access 2023)
- **Checkpoints Analizados:**
  - `demo/ConvNeXtPose_XS.tar` - ‚úÖ Coincide con 2UP del paper
  - `demo/ConvNeXtPose_S.tar` - ‚úÖ Coincide con 2UP del paper
  - `demo/ConvNeXtPose_M (1).tar` - ‚úÖ Coincide con 3UP del paper
  - `demo/ConvNeXtPose_L (1).tar` - ‚úÖ Coincide con 3UP del paper
- **Scripts de Verificaci√≥n:**
  - `verify_upsampling_layers.py` - Cuenta capas y pesos
  - `verify_layer_usage_definitive.py` - Analiza par√°metros de upsampling
  - `test_architecture_forward_simple.py` - **Inferencia real (definitivo)**
- **C√≥digo Fuente:** `main/model.py` - Definici√≥n de HeadNet y DeConv

---

**Fin del Documento** üéØ

**√öltima Actualizaci√≥n:** 16 de Octubre 2025 - Misterio resuelto: ‚úÖ NO hay discrepancia con el paper

---

## üéâ Agradecimientos

Gracias por la intuici√≥n correcta de que "la tercera capa podr√≠a estar en estado false" - ¬°fue exactamente eso! La capa 3 de XS/S tiene `up=False`, ejecut√°ndose sin upsampling.
