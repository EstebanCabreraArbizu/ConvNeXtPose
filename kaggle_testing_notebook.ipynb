{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "17daf72a",
   "metadata": {},
   "source": [
    "# üéØ ConvNeXtPose Testing en Kaggle - Modelos L y M\n",
    "\n",
    "Este notebook eval√∫a los modelos ConvNeXtPose L y M en Human3.6M Protocol 2.\n",
    "\n",
    "**Datasets requeridos:**\n",
    "- Human3.6M Dataset (con S9_ACT2_i6, S11_ACT2_i6, annotations)\n",
    "- ConvNeXtPose Pre-trained Models (checkpoints .tar)\n",
    "\n",
    "**GPU recomendada:** T4 x2 o P100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9ee4c88",
   "metadata": {},
   "source": [
    "## üì¶ PASO 1: Setup Inicial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eba1510",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clonar repositorio\n",
    "!git clone https://github.com/EstebanCabreraArbizu/ConvNeXtPose.git\n",
    "%cd ConvNeXtPose\n",
    "\n",
    "# Verificar versiones\n",
    "import torch\n",
    "print(f\"PyTorch: {torch.__version__}\")\n",
    "print(f\"CUDA disponible: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"Memoria GPU: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1dc19b2",
   "metadata": {},
   "source": [
    "## üóÇÔ∏è PASO 2: Configurar Dataset Human3.6M\n",
    "\n",
    "**IMPORTANTE:** Solo enlazamos el contenido del dataset dentro de `data/Human36M/`.\n",
    "Los m√≥dulos Python originales (`dataset.py`, `Human36M.py`) permanecen intactos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59e3a6e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORTANTE: Ajusta este path al nombre real de tu dataset en Kaggle\n",
    "KAGGLE_DATASET_PATH = '/kaggle/input/human36m-dataset'  # ‚Üê CAMBIAR SEG√öN TU DATASET\n",
    "\n",
    "# Verificar que el dataset existe\n",
    "import os\n",
    "if not os.path.exists(KAGGLE_DATASET_PATH):\n",
    "    print(f\"‚ùå Dataset no encontrado en {KAGGLE_DATASET_PATH}\")\n",
    "    print(\"\\nüìÇ Datasets disponibles:\")\n",
    "    !ls /kaggle/input/\n",
    "    raise FileNotFoundError(\"Verifica el nombre del dataset y actualiza KAGGLE_DATASET_PATH\")\n",
    "else:\n",
    "    print(f\"‚úì Dataset encontrado en {KAGGLE_DATASET_PATH}\")\n",
    "    print(\"\\nüìÇ Contenido:\")\n",
    "    !ls {KAGGLE_DATASET_PATH}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "593fc699",
   "metadata": {},
   "source": [
    "### (Opcional) Diagnosticar Estructura del Dataset\n",
    "\n",
    "Si tienes dudas sobre la estructura de tu dataset, ejecuta esta celda primero para ver c√≥mo est√°n organizadas las carpetas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5a35b87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Diagnosticar estructura del dataset (√∫til para identificar carpetas anidadas)\n",
    "!python diagnose_kaggle_dataset.py {KAGGLE_DATASET_PATH}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43d76eb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejecutar script de configuraci√≥n\n",
    "# IMPORTANTE: Esto enlaza el dataset DENTRO de data/Human36M/, NO reemplaza data/\n",
    "# El script detecta autom√°ticamente carpetas anidadas (ej: annotations (1)/annotations/)\n",
    "!python setup_kaggle_dataset.py --kaggle-input {KAGGLE_DATASET_PATH} --project-root /kaggle/working/ConvNeXtPose\n",
    "\n",
    "print(\"\\n‚úÖ Dataset enlazado en data/Human36M/\")\n",
    "print(\"‚úÖ M√≥dulos Python originales intactos en data/\")\n",
    "print(\"‚úÖ Carpetas anidadas detectadas autom√°ticamente\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2fdaddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verificar que la estructura es correcta\n",
    "!python setup_kaggle_dataset.py --verify /kaggle/working/ConvNeXtPose"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79560116",
   "metadata": {},
   "source": [
    "## üéØ PASO 3: Preparar Checkpoints\n",
    "\n",
    "Extraer los modelos pre-entrenados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ace898e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORTANTE: Ajusta seg√∫n el nombre de tu dataset de modelos\n",
    "MODELS_DATASET_PATH = '/kaggle/input/convnextpose-models'  # ‚Üê CAMBIAR SEG√öN TU DATASET\n",
    "\n",
    "# Verificar modelos disponibles\n",
    "if os.path.exists(MODELS_DATASET_PATH):\n",
    "    print(\"üì¶ Modelos disponibles:\")\n",
    "    !ls -lh {MODELS_DATASET_PATH}/models_tar/\n",
    "else:\n",
    "    print(f\"‚ùå Dataset de modelos no encontrado: {MODELS_DATASET_PATH}\")\n",
    "    print(\"\\nüìÇ Datasets disponibles:\")\n",
    "    !ls /kaggle/input/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b531a3fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import zipfile\n",
    "import shutil\n",
    "\n",
    "# Crear directorio para modelos\n",
    "os.makedirs('output/model_dump', exist_ok=True)\n",
    "\n",
    "# Funci√≥n para extraer y organizar checkpoints\n",
    "def extract_checkpoint(tar_path, model_name, expected_epoch=None):\n",
    "    \"\"\"Extrae checkpoint desde .tar y organiza la estructura correctamente\n",
    "    \n",
    "    Los .tar contienen una carpeta snapshot_XX.pth/ o archive/ con:\n",
    "    - data.pkl\n",
    "    - data/0, data/1, ... (archivos binarios del tensor)\n",
    "    \n",
    "    Esta funci√≥n copia toda la estructura como snapshot_XX.pth/ que PyTorch puede cargar.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(tar_path):\n",
    "        print(f\"‚ö†Ô∏è  Modelo {model_name} no encontrado en {tar_path}\")\n",
    "        return None\n",
    "    \n",
    "    print(f\"üì¶ Extrayendo modelo {model_name} desde {tar_path}...\")\n",
    "    \n",
    "    # Extraer a directorio temporal\n",
    "    temp_dir = f'output/model_dump/temp_{model_name}'\n",
    "    os.makedirs(temp_dir, exist_ok=True)\n",
    "    \n",
    "    with zipfile.ZipFile(tar_path, 'r') as zip_ref:\n",
    "        zip_ref.extractall(temp_dir)\n",
    "        files = zip_ref.namelist()\n",
    "        print(f\"   ‚úì Extra√≠do: {len(files)} archivos\")\n",
    "    \n",
    "    # Buscar la carpeta del checkpoint (snapshot_XX.pth/ o archive/)\n",
    "    found_checkpoints = []\n",
    "    for item in os.listdir(temp_dir):\n",
    "        item_path = os.path.join(temp_dir, item)\n",
    "        if os.path.isdir(item_path):\n",
    "            # Verificar que tenga data.pkl (indicador de checkpoint PyTorch)\n",
    "            if os.path.exists(os.path.join(item_path, 'data.pkl')):\n",
    "                found_checkpoints.append((item, item_path))\n",
    "                print(f\"   ‚úì Checkpoint encontrado: {item}/\")\n",
    "    \n",
    "    if not found_checkpoints:\n",
    "        print(f\"   ‚ùå No se encontr√≥ estructura de checkpoint v√°lida\")\n",
    "        shutil.rmtree(temp_dir)\n",
    "        return None\n",
    "    \n",
    "    # Procesar cada checkpoint encontrado\n",
    "    epoch = None\n",
    "    for ckpt_name, ckpt_path in found_checkpoints:\n",
    "        # Determinar el nombre final\n",
    "        import re\n",
    "        match = re.search(r'snapshot_(\\d+)', ckpt_name)\n",
    "        if match:\n",
    "            epoch = match.group(1)\n",
    "            final_name = f'snapshot_{epoch}.pth.tar'\n",
    "        else:\n",
    "            # Si no tiene n√∫mero, usar el epoch esperado\n",
    "            if expected_epoch:\n",
    "                epoch = str(expected_epoch)\n",
    "                final_name = f'snapshot_{epoch}.pth.tar'\n",
    "            else:\n",
    "                final_name = f'{model_name}_checkpoint.pth.tar'\n",
    "        \n",
    "        dest_path = os.path.join('output/model_dump', final_name)\n",
    "        \n",
    "        # Copiar toda la carpeta del checkpoint\n",
    "        if os.path.exists(dest_path):\n",
    "            shutil.rmtree(dest_path)\n",
    "        shutil.copytree(ckpt_path, dest_path)\n",
    "        print(f\"   ‚úì Checkpoint copiado como: {final_name}\")\n",
    "        \n",
    "        # Verificar estructura\n",
    "        if os.path.exists(os.path.join(dest_path, 'data.pkl')):\n",
    "            data_files = len(os.listdir(os.path.join(dest_path, 'data')))\n",
    "            print(f\"     - data.pkl: ‚úì\")\n",
    "            print(f\"     - data/*: {data_files} archivos\")\n",
    "    \n",
    "    # Limpiar temporal\n",
    "    shutil.rmtree(temp_dir)\n",
    "    print(f\"   ‚úì Limpieza temporal completada\")\n",
    "    \n",
    "    return epoch\n",
    "\n",
    "# Extraer modelo L (epoch 83 seg√∫n el paper)\n",
    "model_l_path = f'{MODELS_DATASET_PATH}/models_tar/ConvNeXtPose_L.tar'\n",
    "epoch_l = extract_checkpoint(model_l_path, 'L', expected_epoch=83)\n",
    "\n",
    "# Extraer modelo M (epoch 70 seg√∫n el paper)\n",
    "model_m_path = f'{MODELS_DATASET_PATH}/models_tar/ConvNeXtPose_M.tar'\n",
    "epoch_m = extract_checkpoint(model_m_path, 'M', expected_epoch=70)\n",
    "\n",
    "# Verificar checkpoints extra√≠dos\n",
    "print(\"\\nüìÇ Checkpoints disponibles:\")\n",
    "!ls -lh output/model_dump/\n",
    "\n",
    "# Mostrar informaci√≥n de epochs\n",
    "if epoch_l:\n",
    "    print(f\"\\nüí° Modelo L: Usa CHECKPOINT_EPOCH = {epoch_l}\")\n",
    "if epoch_m:\n",
    "    print(f\"üí° Modelo M: Usa CHECKPOINT_EPOCH = {epoch_m}\")\n",
    "\n",
    "print(\"\\n‚ö†Ô∏è  NOTA: Los checkpoints est√°n en formato carpeta (snapshot_XX.pth.tar/)\")\n",
    "print(\"   PyTorch puede cargarlos usando torch.load('output/model_dump/snapshot_83.pth.tar')\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b098aaa5",
   "metadata": {},
   "source": [
    "### ‚ö†Ô∏è Nota sobre Extracci√≥n de Checkpoints\n",
    "\n",
    "Los archivos `.tar` de ConvNeXtPose son en realidad **archivos ZIP** con estructura anidada:\n",
    "```\n",
    "ConvNeXtPose_L.tar (archivo zip)\n",
    "‚îî‚îÄ‚îÄ snapshot_83.pth/        ‚Üê Directorio\n",
    "    ‚îú‚îÄ‚îÄ data.pkl\n",
    "    ‚îú‚îÄ‚îÄ version\n",
    "    ‚îî‚îÄ‚îÄ data/               ‚Üê Carpeta con el checkpoint real\n",
    "        ‚îî‚îÄ‚îÄ 0, 1, 2...      ‚Üê Archivos binarios\n",
    "```\n",
    "\n",
    "La siguiente celda extrae y reorganiza correctamente el archivo `.pth` real."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbe7254a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verificar checkpoints extra√≠dos y detectar epoch\n",
    "import glob\n",
    "import re\n",
    "\n",
    "print(\"üîç Verificaci√≥n de Checkpoints:\\n\")\n",
    "\n",
    "# Buscar archivos .pth (no directorios)\n",
    "checkpoints = [f for f in glob.glob('output/model_dump/snapshot_*.pth') \n",
    "               if os.path.isfile(f)]  # ‚Üê Solo archivos, no directorios\n",
    "\n",
    "if checkpoints:\n",
    "    print(\"‚úÖ Checkpoints encontrados:\\n\")\n",
    "    for ckpt in sorted(checkpoints):\n",
    "        ckpt_name = os.path.basename(ckpt)\n",
    "        size_mb = os.path.getsize(ckpt) / (1024 * 1024)\n",
    "        \n",
    "        # Extraer epoch\n",
    "        match = re.search(r'snapshot_(\\d+)', ckpt_name)\n",
    "        if match:\n",
    "            epoch = match.group(1)\n",
    "            print(f\"  ‚úì {ckpt_name} ({size_mb:.1f} MB)\")\n",
    "            print(f\"    ‚Üí Usa CHECKPOINT_EPOCH = {epoch}\\n\")\n",
    "else:\n",
    "    print(\"‚ùå No se encontraron checkpoints .pth v√°lidos\\n\")\n",
    "    \n",
    "    # Diagnosticar problema\n",
    "    all_items = glob.glob('output/model_dump/snapshot_*')\n",
    "    if all_items:\n",
    "        print(\"‚ö†Ô∏è  Se encontraron directorios en lugar de archivos:\")\n",
    "        for item in all_items:\n",
    "            if os.path.isdir(item):\n",
    "                print(f\"    üìÅ {os.path.basename(item)} (directorio)\")\n",
    "                # Buscar dentro del directorio\n",
    "                pth_inside = glob.glob(f\"{item}/**/*.pth\", recursive=True)\n",
    "                if pth_inside:\n",
    "                    print(f\"       Contiene: {[os.path.basename(p) for p in pth_inside]}\")\n",
    "        \n",
    "        print(\"\\nüí° Soluci√≥n: Re-ejecuta la celda anterior de extracci√≥n\")\n",
    "    else:\n",
    "        print(\"üí° Soluci√≥n: Verifica que MODELS_DATASET_PATH es correcto\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "177530d1",
   "metadata": {},
   "source": [
    "**‚ö†Ô∏è IMPORTANTE:** Ajusta el valor de `CHECKPOINT_EPOCH` en las siguientes celdas seg√∫n el epoch de tu checkpoint extra√≠do. En este caso detectamos `snapshot_83.pth`, as√≠ que usa `CHECKPOINT_EPOCH = 83`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f63761ce",
   "metadata": {},
   "source": [
    "## üöÄ PASO 4: Ejecutar Testing\n",
    "\n",
    "### Modelo L (Large)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58509a2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Diagn√≥stico: Verificar que todos los componentes est√°n listos\n",
    "import sys\n",
    "import os\n",
    "\n",
    "print(\"üîç Verificaci√≥n de Componentes:\\n\")\n",
    "\n",
    "# 1. Dataset - Verificar estructura DENTRO del proyecto\n",
    "project_root = '/kaggle/working/ConvNeXtPose'\n",
    "data_dir = os.path.join(project_root, 'data')\n",
    "h36m_path = os.path.join(data_dir, 'Human36M')\n",
    "\n",
    "print(f\"1. Dataset (estructura del proyecto):\")\n",
    "print(f\"   Project root: {project_root}\")\n",
    "print(f\"   data/ exists: {os.path.exists(data_dir)}\")\n",
    "print(f\"   data/dataset.py: {os.path.exists(os.path.join(data_dir, 'dataset.py'))}\")\n",
    "print(f\"   data/Human36M/ exists: {os.path.exists(h36m_path)}\")\n",
    "\n",
    "if os.path.exists(h36m_path):\n",
    "    print(f\"   - Human36M.py: {os.path.exists(os.path.join(h36m_path, 'Human36M.py'))}\")\n",
    "    print(f\"   - annotations: {os.path.exists(os.path.join(h36m_path, 'annotations'))}\")\n",
    "    print(f\"   - images/S9: {os.path.exists(os.path.join(h36m_path, 'images', 'S9'))}\")\n",
    "    print(f\"   - images/S11: {os.path.exists(os.path.join(h36m_path, 'images', 'S11'))}\")\n",
    "\n",
    "# 2. Checkpoints\n",
    "checkpoint_dir = os.path.join(project_root, 'output/model_dump')\n",
    "print(f\"\\n2. Checkpoints: {checkpoint_dir}\")\n",
    "if os.path.exists(checkpoint_dir):\n",
    "    checkpoints = [f for f in os.listdir(checkpoint_dir) if f.startswith('snapshot_')]\n",
    "    if checkpoints:\n",
    "        print(f\"   Disponibles: {', '.join(checkpoints)}\")\n",
    "        # Extraer epoch del checkpoint\n",
    "        import re\n",
    "        for ckpt in checkpoints:\n",
    "            match = re.search(r'snapshot_(\\d+)', ckpt)\n",
    "            if match:\n",
    "                epoch = match.group(1)\n",
    "                print(f\"   üí° Usa CHECKPOINT_EPOCH = {epoch} en la siguiente celda\")\n",
    "    else:\n",
    "        print(\"   ‚ö†Ô∏è  No se encontraron checkpoints\")\n",
    "else:\n",
    "    print(\"   ‚ùå Directorio no existe\")\n",
    "\n",
    "# 3. Estructura del proyecto\n",
    "print(f\"\\n3. Estructura del proyecto:\")\n",
    "critical_files = ['main/config.py', 'common/base.py', 'data/dataset.py', 'data/Human36M/Human36M.py']\n",
    "for file_path in critical_files:\n",
    "    full_path = os.path.join(project_root, file_path)\n",
    "    exists = os.path.exists(full_path)\n",
    "    status = \"‚úì\" if exists else \"‚ùå\"\n",
    "    print(f\"   {status} {file_path}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "if all(os.path.exists(os.path.join(project_root, f)) for f in critical_files):\n",
    "    print(\"‚úÖ Todos los checks pasaron - Listo para testing\")\n",
    "else:\n",
    "    print(\"‚ùå Algunos archivos faltan - Revisa la configuraci√≥n\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70c873e6",
   "metadata": {},
   "source": [
    "### Ejecutar Testing desde Python\n",
    "\n",
    "**Estructura correcta:**\n",
    "- ‚úÖ M√≥dulos Python originales en `data/` (dataset.py, Human36M.py)\n",
    "- ‚úÖ Dataset de Kaggle enlazado en `data/Human36M/` (images, annotations)\n",
    "- ‚úÖ `config.py` configura autom√°ticamente los paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6224a3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing con estructura correcta del proyecto\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# 1. Cambiar al directorio main (donde est√° config.py)\n",
    "os.chdir('/kaggle/working/ConvNeXtPose/main')\n",
    "\n",
    "# 2. Importar config PRIMERO - esto configura autom√°ticamente los paths\n",
    "from config import cfg\n",
    "\n",
    "# 3. Cargar variante ANTES de importar otros m√≥dulos\n",
    "VARIANT = 'L'  # Cambiar a 'M' para modelo Medium\n",
    "CHECKPOINT_EPOCH = 83  # ‚Üê AJUSTAR seg√∫n tu checkpoint\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"  Testing ConvNeXtPose-{VARIANT}\")\n",
    "print(f\"{'='*60}\\n\")\n",
    "\n",
    "cfg.load_variant_config(VARIANT)\n",
    "cfg.set_args('0')  # GPU 0\n",
    "\n",
    "# 4. AHORA importar los dem√°s m√≥dulos (config ya configur√≥ sys.path)\n",
    "import torch\n",
    "import torch.backends.cudnn as cudnn\n",
    "from base import Tester\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "cudnn.benchmark = True\n",
    "cudnn.deterministic = False\n",
    "cudnn.enabled = True\n",
    "\n",
    "# 5. Crear tester y ejecutar\n",
    "tester = Tester()\n",
    "tester._make_batch_generator()\n",
    "tester._make_model(CHECKPOINT_EPOCH)\n",
    "\n",
    "print(f\"\\nüöÄ Ejecutando testing en epoch {CHECKPOINT_EPOCH}...\\n\")\n",
    "\n",
    "preds = []\n",
    "with torch.no_grad():\n",
    "    for itr, input_img in enumerate(tqdm(tester.batch_generator)):\n",
    "        coord_out = tester.model(input_img)\n",
    "        \n",
    "        if cfg.flip_test:\n",
    "            from utils.pose_utils import flip\n",
    "            flipped_input_img = flip(input_img, dims=3)\n",
    "            flipped_coord_out = tester.model(flipped_input_img)\n",
    "            flipped_coord_out[:, :, 0] = cfg.output_shape[1] - flipped_coord_out[:, :, 0] - 1\n",
    "            for pair in tester.flip_pairs:\n",
    "                flipped_coord_out[:, pair[0], :], flipped_coord_out[:, pair[1], :] = \\\n",
    "                    flipped_coord_out[:, pair[1], :].clone(), flipped_coord_out[:, pair[0], :].clone()\n",
    "            coord_out = (coord_out + flipped_coord_out)/2.\n",
    "        \n",
    "        coord_out = coord_out.cpu().numpy()\n",
    "        preds.append(coord_out)\n",
    "\n",
    "# Evaluar\n",
    "preds = np.concatenate(preds, axis=0)\n",
    "print(f\"\\nüìä Evaluando {len(preds)} predicciones...\\n\")\n",
    "tester._evaluate(preds, cfg.result_dir)\n",
    "\n",
    "print(f\"\\n‚úÖ Testing completado!\")\n",
    "print(f\"üìÇ Resultados guardados en: {cfg.result_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86f7ffa5",
   "metadata": {},
   "source": [
    "### Modelo M (Medium) - Opcional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8944d98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Si tienes el checkpoint del modelo M, ejecuta esto:\n",
    "# !python test.py --gpu 0 --epochs {CHECKPOINT_EPOCH} --variant M"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1df8cdae",
   "metadata": {},
   "source": [
    "## üìä PASO 5: Verificar Resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8200f20f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ver resultados generados\n",
    "%cd ..\n",
    "!ls -lh output/result/\n",
    "\n",
    "# Leer log de resultados\n",
    "import glob\n",
    "log_files = glob.glob('output/log/*.log')\n",
    "if log_files:\n",
    "    latest_log = max(log_files, key=os.path.getctime)\n",
    "    print(f\"\\nüìÑ √öltimas l√≠neas del log ({os.path.basename(latest_log)}):\")\n",
    "    !tail -n 20 {latest_log}\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  No se encontraron logs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14f8e2af",
   "metadata": {},
   "source": [
    "## üìà PASO 6: An√°lisis de Resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "145e14d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def extract_mpjpe_from_log(log_path):\n",
    "    \"\"\"Extrae el MPJPE del log\"\"\"\n",
    "    with open(log_path, 'r') as f:\n",
    "        content = f.read()\n",
    "    \n",
    "    # Buscar patr√≥n de MPJPE\n",
    "    pattern = r'MPJPE.*?(\\d+\\.\\d+)'\n",
    "    matches = re.findall(pattern, content)\n",
    "    \n",
    "    if matches:\n",
    "        return float(matches[-1])  # √öltimo valor\n",
    "    return None\n",
    "\n",
    "# Extraer resultados\n",
    "log_files = glob.glob('output/log/*.log')\n",
    "if log_files:\n",
    "    latest_log = max(log_files, key=os.path.getctime)\n",
    "    mpjpe = extract_mpjpe_from_log(latest_log)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"  üìä RESULTADOS FINALES\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    if mpjpe:\n",
    "        print(f\"\\n  MPJPE (Protocol 2): {mpjpe:.2f} mm\")\n",
    "        \n",
    "        # Comparar con paper\n",
    "        expected = {\n",
    "            'L': 42.3,\n",
    "            'M': 44.6\n",
    "        }\n",
    "        \n",
    "        # Determinar variante\n",
    "        for variant, expected_val in expected.items():\n",
    "            diff = abs(mpjpe - expected_val)\n",
    "            if diff < 5:\n",
    "                print(f\"\\n  Variante detectada: {variant}\")\n",
    "                print(f\"  Valor del paper: {expected_val:.2f} mm\")\n",
    "                print(f\"  Diferencia: {mpjpe - expected_val:+.2f} mm\")\n",
    "                \n",
    "                if diff < 2:\n",
    "                    print(\"  ‚úÖ Resultado excelente (dentro de ¬±2mm)\")\n",
    "                elif diff < 5:\n",
    "                    print(\"  ‚úì Resultado aceptable (dentro de ¬±5mm)\")\n",
    "                break\n",
    "    else:\n",
    "        print(\"\\n‚ö†Ô∏è  No se pudo extraer MPJPE del log\")\n",
    "        print(\"Revisa el log manualmente en output/log/\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "else:\n",
    "    print(\"‚ùå No se encontraron logs de testing\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76e80fd2",
   "metadata": {},
   "source": [
    "## üíæ PASO 7: Guardar Outputs\n",
    "\n",
    "Kaggle guarda autom√°ticamente todo en `/kaggle/working/`. Opcionalmente puedes copiar resultados espec√≠ficos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2acfddb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear resumen de resultados\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "summary = {\n",
    "    'timestamp': datetime.now().isoformat(),\n",
    "    'model': 'ConvNeXtPose-L',  # Cambiar seg√∫n modelo testeado\n",
    "    'checkpoint_epoch': CHECKPOINT_EPOCH,\n",
    "    'dataset': 'Human3.6M Protocol 2',\n",
    "    'mpjpe_mm': mpjpe if 'mpjpe' in locals() else None,\n",
    "    'pytorch_version': torch.__version__,\n",
    "    'cuda_version': torch.version.cuda if torch.cuda.is_available() else None\n",
    "}\n",
    "\n",
    "# Guardar resumen\n",
    "with open('output/result/test_summary.json', 'w') as f:\n",
    "    json.dump(summary, f, indent=2)\n",
    "\n",
    "print(\"‚úì Resumen guardado en output/result/test_summary.json\")\n",
    "print(\"\\nüìÑ Contenido:\")\n",
    "print(json.dumps(summary, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23f63b45",
   "metadata": {},
   "source": [
    "## üéâ Testing Completado!\n",
    "\n",
    "Los resultados est√°n en:\n",
    "- **Logs**: `output/log/`\n",
    "- **Resultados**: `output/result/`\n",
    "- **Resumen JSON**: `output/result/test_summary.json`\n",
    "\n",
    "Todos los archivos en `/kaggle/working/ConvNeXtPose/output/` se guardar√°n autom√°ticamente cuando hagas commit del notebook."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
