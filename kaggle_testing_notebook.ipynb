{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "17daf72a",
   "metadata": {},
   "source": [
    "# üéØ ConvNeXtPose Testing en Kaggle - Modelos L y M\n",
    "\n",
    "Este notebook eval√∫a los modelos ConvNeXtPose L y M en Human3.6M Protocol 2.\n",
    "\n",
    "**Datasets requeridos:**\n",
    "- Human3.6M Dataset (con S9_ACT2_i6, S11_ACT2_i6, annotations)\n",
    "- ConvNeXtPose Pre-trained Models (checkpoints .tar)\n",
    "\n",
    "**GPU recomendada:** T4 x2 o P100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9ee4c88",
   "metadata": {},
   "source": [
    "## üì¶ PASO 1: Setup Inicial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eba1510",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clonar repositorio\n",
    "!git clone https://github.com/EstebanCabreraArbizu/ConvNeXtPose.git\n",
    "%cd ConvNeXtPose\n",
    "\n",
    "# Verificar versiones\n",
    "import torch\n",
    "print(f\"PyTorch: {torch.__version__}\")\n",
    "print(f\"CUDA disponible: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"Memoria GPU: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1dc19b2",
   "metadata": {},
   "source": [
    "## üóÇÔ∏è PASO 2: Configurar Dataset Human3.6M\n",
    "\n",
    "**IMPORTANTE:** Solo enlazamos el contenido del dataset dentro de `data/Human36M/`.\n",
    "Los m√≥dulos Python originales (`dataset.py`, `Human36M.py`) permanecen intactos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59e3a6e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORTANTE: Ajusta este path al nombre real de tu dataset en Kaggle\n",
    "KAGGLE_DATASET_PATH = '/kaggle/input/human36m-dataset'  # ‚Üê CAMBIAR SEG√öN TU DATASET\n",
    "\n",
    "# Verificar que el dataset existe\n",
    "import os\n",
    "if not os.path.exists(KAGGLE_DATASET_PATH):\n",
    "    print(f\"‚ùå Dataset no encontrado en {KAGGLE_DATASET_PATH}\")\n",
    "    print(\"\\nüìÇ Datasets disponibles:\")\n",
    "    !ls /kaggle/input/\n",
    "    raise FileNotFoundError(\"Verifica el nombre del dataset y actualiza KAGGLE_DATASET_PATH\")\n",
    "else:\n",
    "    print(f\"‚úì Dataset encontrado en {KAGGLE_DATASET_PATH}\")\n",
    "    print(\"\\nüìÇ Contenido:\")\n",
    "    !ls {KAGGLE_DATASET_PATH}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "593fc699",
   "metadata": {},
   "source": [
    "### (Opcional) Diagnosticar Estructura del Dataset\n",
    "\n",
    "Si tienes dudas sobre la estructura de tu dataset, ejecuta esta celda primero para ver c√≥mo est√°n organizadas las carpetas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5a35b87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Diagnosticar estructura del dataset (√∫til para identificar carpetas anidadas)\n",
    "!python diagnose_kaggle_dataset.py {KAGGLE_DATASET_PATH}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43d76eb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejecutar script de configuraci√≥n\n",
    "# IMPORTANTE: Esto enlaza el dataset DENTRO de data/Human36M/, NO reemplaza data/\n",
    "# El script detecta autom√°ticamente carpetas anidadas (ej: annotations (1)/annotations/)\n",
    "!python setup_kaggle_dataset.py --kaggle-input {KAGGLE_DATASET_PATH} --project-root /kaggle/working/ConvNeXtPose\n",
    "\n",
    "print(\"\\n‚úÖ Dataset enlazado en data/Human36M/\")\n",
    "print(\"‚úÖ M√≥dulos Python originales intactos en data/\")\n",
    "print(\"‚úÖ Carpetas anidadas detectadas autom√°ticamente\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2fdaddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verificar que la estructura es correcta\n",
    "!python setup_kaggle_dataset.py --verify /kaggle/working/ConvNeXtPose"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79560116",
   "metadata": {},
   "source": [
    "## üéØ PASO 3: Preparar Checkpoints\n",
    "\n",
    "Extraer los modelos pre-entrenados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ace898e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORTANTE: Si usas gdown, ajusta la ruta al directorio donde descargaste\n",
    "# Si usas dataset de Kaggle, usa: MODELS_DATASET_PATH = '/kaggle/input/convnextpose-models'\n",
    "MODELS_DATASET_PATH = '/kaggle/working/ConvNeXtPose/models_tar'  # ‚Üê CAMBIAR SEG√öN TU CASO\n",
    "\n",
    "# Verificar modelos disponibles\n",
    "if os.path.exists(MODELS_DATASET_PATH):\n",
    "    print(\"üì¶ Modelos disponibles:\")\n",
    "    !ls -lh {MODELS_DATASET_PATH}/*.tar\n",
    "else:\n",
    "    print(f\"‚ùå Dataset de modelos no encontrado: {MODELS_DATASET_PATH}\")\n",
    "    print(\"\\nüìÇ Rutas disponibles:\")\n",
    "    !ls -lh /kaggle/working/ConvNeXtPose/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b531a3fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import zipfile\n",
    "import shutil\n",
    "import tarfile\n",
    "import torch\n",
    "\n",
    "# Crear directorio para modelos\n",
    "os.makedirs('output/model_dump', exist_ok=True)\n",
    "\n",
    "# Funci√≥n para extraer y convertir checkpoints\n",
    "def extract_checkpoint(tar_path, model_name, expected_epoch=None):\n",
    "    \"\"\"Extrae checkpoint desde .tar/.zip y lo convierte en archivo .pth v√°lido\n",
    "    \n",
    "    Los archivos .tar de ConvNeXtPose son ZIP con estructura de directorio en formato legacy.\n",
    "    Usamos una conversi√≥n simple: empaquetar el directorio como TAR que PyTorch puede leer.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(tar_path):\n",
    "        print(f\"‚ö†Ô∏è  Modelo {model_name} no encontrado en {tar_path}\")\n",
    "        return None\n",
    "    \n",
    "    print(f\"üì¶ Extrayendo modelo {model_name} desde {tar_path}...\")\n",
    "    \n",
    "    # Verificar tama√±o del archivo\n",
    "    file_size = os.path.getsize(tar_path)\n",
    "    print(f\"   üìè Tama√±o del archivo: {file_size / (1024*1024):.2f} MB\")\n",
    "    \n",
    "    # Leer primeros bytes para diagnosticar el formato\n",
    "    with open(tar_path, 'rb') as f:\n",
    "        header = f.read(512)\n",
    "        print(f\"   üîç Primeros bytes (hex): {header[:50].hex()}\")\n",
    "    \n",
    "    # Si el archivo es muy peque√±o o empieza con HTML, es un error de descarga\n",
    "    if file_size < 1000 or header.startswith(b'<!DOCTYPE') or header.startswith(b'<html'):\n",
    "        print(f\"   ‚ùå ERROR: El archivo parece ser HTML (error de descarga)\")\n",
    "        print(f\"   üí° Soluci√≥n: Revisa que el folder de Google Drive sea p√∫blico\")\n",
    "        return None\n",
    "    \n",
    "    # Extraer a directorio temporal\n",
    "    temp_dir = 'output/model_dump/temp_extract'\n",
    "    os.makedirs(temp_dir, exist_ok=True)\n",
    "    \n",
    "    # Detectar formato: intentar ZIP primero, luego TAR\n",
    "    extracted = False\n",
    "    try:\n",
    "        with zipfile.ZipFile(tar_path, 'r') as zip_ref:\n",
    "            zip_ref.extractall(temp_dir)\n",
    "            files = zip_ref.namelist()\n",
    "            print(f\"   ‚úì Formato: ZIP - Extra√≠do: {len(files)} archivos\")\n",
    "            extracted = True\n",
    "    except zipfile.BadZipFile:\n",
    "        try:\n",
    "            with tarfile.open(tar_path, 'r') as tar_ref:\n",
    "                tar_ref.extractall(temp_dir)\n",
    "                files = tar_ref.getnames()\n",
    "                print(f\"   ‚úì Formato: TAR - Extra√≠do: {len(files)} archivos\")\n",
    "                extracted = True\n",
    "        except (tarfile.ReadError, Exception) as e:\n",
    "            print(f\"   ‚ùå ERROR: No se pudo extraer el archivo\")\n",
    "            print(f\"   üí° Formato no reconocido: {type(e).__name__}: {e}\")\n",
    "            shutil.rmtree(temp_dir, ignore_errors=True)\n",
    "            return None\n",
    "    \n",
    "    if not extracted:\n",
    "        shutil.rmtree(temp_dir, ignore_errors=True)\n",
    "        return None\n",
    "    \n",
    "    # Buscar la carpeta del checkpoint (snapshot_XX.pth/ o archive/)\n",
    "    found_checkpoints = []\n",
    "    for item in os.listdir(temp_dir):\n",
    "        item_path = os.path.join(temp_dir, item)\n",
    "        if os.path.isdir(item_path):\n",
    "            # Verificar que tenga data.pkl (indicador de checkpoint PyTorch)\n",
    "            if os.path.exists(os.path.join(item_path, 'data.pkl')):\n",
    "                found_checkpoints.append((item, item_path))\n",
    "                print(f\"   ‚úì Checkpoint encontrado: {item}/\")\n",
    "    \n",
    "    if not found_checkpoints:\n",
    "        print(f\"   ‚ùå No se encontr√≥ estructura de checkpoint v√°lida\")\n",
    "        print(f\"   üìÇ Contenido extra√≠do:\")\n",
    "        for item in os.listdir(temp_dir)[:10]:\n",
    "            print(f\"      - {item}\")\n",
    "        shutil.rmtree(temp_dir, ignore_errors=True)\n",
    "        return None\n",
    "    \n",
    "    # Convertir el checkpoint de directorio a archivo .pth.tar\n",
    "    epoch = None\n",
    "    for ckpt_name, ckpt_path in found_checkpoints:\n",
    "        # Determinar el epoch desde el nombre\n",
    "        import re\n",
    "        match = re.search(r'snapshot_(\\d+)', ckpt_name)\n",
    "        if match:\n",
    "            epoch = match.group(1)\n",
    "            final_name = f'snapshot_{epoch}.pth.tar'\n",
    "        else:\n",
    "            # Si no tiene snapshot_XX, usar archive/ con epoch esperado\n",
    "            if expected_epoch:\n",
    "                epoch = str(expected_epoch)\n",
    "                final_name = f'snapshot_{epoch}.pth.tar'\n",
    "            else:\n",
    "                epoch = '0'\n",
    "                final_name = f'{model_name}_checkpoint.pth.tar'\n",
    "        \n",
    "        dest_path = os.path.join('output/model_dump', final_name)\n",
    "        \n",
    "        print(f\"   üîÑ Convirtiendo directorio ‚Üí archivo .pth.tar (formato TAR)...\")\n",
    "        \n",
    "        try:\n",
    "            # SOLUCI√ìN: Re-empaquetar como TAR (formato legacy que PyTorch entiende)\n",
    "            # PyTorch espera que los archivos est√©n en la ra√≠z del TAR, no en subdirectorio\n",
    "            \n",
    "            if os.path.exists(dest_path):\n",
    "                os.remove(dest_path)\n",
    "            \n",
    "            # Crear archivo TAR con el contenido del directorio en la ra√≠z\n",
    "            import tarfile\n",
    "            with tarfile.open(dest_path, 'w') as tar:\n",
    "                # Agregar cada archivo/carpeta del checkpoint a la ra√≠z del TAR\n",
    "                for item in os.listdir(ckpt_path):\n",
    "                    item_path = os.path.join(ckpt_path, item)\n",
    "                    tar.add(item_path, arcname=item)\n",
    "            \n",
    "            # Verificar tama√±o del archivo creado\n",
    "            size_mb = os.path.getsize(dest_path) / (1024 * 1024)\n",
    "            print(f\"   ‚úì Archivo creado: {final_name} ({size_mb:.1f} MB)\")\n",
    "            \n",
    "            # Verificar que PyTorch puede cargar el archivo\n",
    "            # Nota: La verificaci√≥n puede fallar pero el modelo cargar√° correctamente\n",
    "            try:\n",
    "                test_load = torch.load(dest_path, map_location='cpu', weights_only=False)\n",
    "                keys = list(test_load.keys())\n",
    "                print(f\"   ‚úì Verificaci√≥n exitosa - Keys: {keys}\")\n",
    "            except KeyError as e:\n",
    "                # KeyError es esperado en algunos casos - el modelo lo manejar√° correctamente\n",
    "                if 'storages' in str(e) or 'filename' in str(e):\n",
    "                    print(f\"   ‚ö†Ô∏è  Advertencia (esperada): {type(e).__name__}: {str(e)[:100]}\")\n",
    "                    print(f\"   üí° Esto es normal - el modelo usar√° su propio cargador legacy\")\n",
    "                else:\n",
    "                    raise\n",
    "            except Exception as e:\n",
    "                print(f\"   ‚ö†Ô∏è  Advertencia al verificar: {type(e).__name__}: {str(e)[:100]}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"   ‚ùå ERROR al convertir checkpoint: {type(e).__name__}\")\n",
    "            print(f\"      {str(e)[:200]}\")\n",
    "            import traceback\n",
    "            print(f\"   üîç Traceback: {traceback.format_exc()[:300]}\")\n",
    "            shutil.rmtree(temp_dir, ignore_errors=True)\n",
    "            return None\n",
    "    \n",
    "    # Limpiar temporal\n",
    "    shutil.rmtree(temp_dir, ignore_errors=True)\n",
    "    print(f\"   ‚úì Conversi√≥n completada\")\n",
    "    \n",
    "    return epoch\n",
    "\n",
    "# Extraer modelo L (epoch 83 seg√∫n el paper)\n",
    "print(\"=\"*60)\n",
    "model_l_path = f'{MODELS_DATASET_PATH}/ConvNeXtPose_L.tar'\n",
    "epoch_l = extract_checkpoint(model_l_path, 'L', expected_epoch=83)\n",
    "\n",
    "print()\n",
    "\n",
    "# Extraer modelo M (epoch 70 seg√∫n el paper)\n",
    "model_m_path = f'{MODELS_DATASET_PATH}/ConvNeXtPose_M.tar'\n",
    "epoch_m = extract_checkpoint(model_m_path, 'M', expected_epoch=70)\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Verificar checkpoints extra√≠dos\n",
    "print(\"\\nüìÇ Checkpoints disponibles:\")\n",
    "checkpoints = [f for f in os.listdir('output/model_dump') \n",
    "               if f.endswith('.pth.tar') and os.path.isfile(os.path.join('output/model_dump', f))]\n",
    "\n",
    "if checkpoints:\n",
    "    for ckpt in sorted(checkpoints):\n",
    "        ckpt_path = os.path.join('output/model_dump', ckpt)\n",
    "        size_mb = os.path.getsize(ckpt_path) / (1024 * 1024)\n",
    "        print(f\"  ‚úì {ckpt} ({size_mb:.1f} MB)\")\n",
    "        \n",
    "        # Verificar contenido\n",
    "        try:\n",
    "            test_load = torch.load(ckpt_path, map_location='cpu')\n",
    "            keys = list(test_load.keys())\n",
    "            print(f\"    ‚Üí Keys: {keys}\")\n",
    "            if 'network' in test_load:\n",
    "                print(f\"    ‚Üí ‚úÖ Formato v√°lido para torch.load()\")\n",
    "            else:\n",
    "                print(f\"    ‚Üí ‚ö†Ô∏è  Falta key 'network'\")\n",
    "        except Exception as e:\n",
    "            print(f\"    ‚Üí ‚ùå Error al verificar: {type(e).__name__}: {str(e)[:80]}\")\n",
    "\n",
    "# Mostrar informaci√≥n de epochs\n",
    "if epoch_l:\n",
    "    print(f\"\\nüí° Modelo L: Usa CHECKPOINT_EPOCH = {epoch_l}\")\n",
    "if epoch_m:\n",
    "    print(f\"üí° Modelo M: Usa CHECKPOINT_EPOCH = {epoch_m}\")\n",
    "\n",
    "if epoch_l or epoch_m:\n",
    "    print(\"\\n‚úÖ Los checkpoints est√°n listos para torch.load()\")\n",
    "    print(\"   Formato: .pth.tar (TAR con estructura legacy)\")\n",
    "else:\n",
    "    print(\"\\n‚ùå No se pudieron extraer los checkpoints\")\n",
    "    print(\"üí° Verifica que los archivos .tar se descargaron correctamente\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b098aaa5",
   "metadata": {},
   "source": [
    "### ‚ö†Ô∏è Nota sobre Extracci√≥n de Checkpoints\n",
    "\n",
    "Los archivos `.tar` de ConvNeXtPose son en realidad **archivos ZIP** con estructura anidada:\n",
    "```\n",
    "ConvNeXtPose_L.tar (archivo zip)\n",
    "‚îî‚îÄ‚îÄ snapshot_83.pth/        ‚Üê Directorio (formato legacy de PyTorch)\n",
    "    ‚îú‚îÄ‚îÄ data.pkl            ‚Üê Metadatos del modelo (epoch, network, etc.)\n",
    "    ‚îú‚îÄ‚îÄ version             ‚Üê Versi√≥n de PyTorch\n",
    "    ‚îî‚îÄ‚îÄ data/               ‚Üê Tensors del modelo\n",
    "        ‚îú‚îÄ‚îÄ 0, 1, 2...      ‚Üê Fragmentos binarios\n",
    "```\n",
    "\n",
    "**Problema:** `torch.load()` en PyTorch moderno **NO acepta directorios**, solo archivos.\n",
    "\n",
    "**Soluci√≥n:** La celda anterior:\n",
    "1. Extrae el contenido del ZIP\n",
    "2. Lee `data.pkl` del directorio\n",
    "3. Convierte y guarda como archivo `.pth` est√°ndar que `torch.load()` puede abrir\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbe7254a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verificar checkpoints extra√≠dos y detectar epoch\n",
    "import glob\n",
    "import re\n",
    "import os\n",
    "import torch\n",
    "\n",
    "print(\"üîç Verificaci√≥n de Checkpoints:\\n\")\n",
    "\n",
    "# Buscar ARCHIVOS .pth.tar\n",
    "checkpoint_dir = 'output/model_dump'\n",
    "if os.path.exists(checkpoint_dir):\n",
    "    checkpoints = [f for f in os.listdir(checkpoint_dir) \n",
    "                   if f.endswith('.pth.tar') and os.path.isfile(os.path.join(checkpoint_dir, f))]\n",
    "else:\n",
    "    checkpoints = []\n",
    "\n",
    "if checkpoints:\n",
    "    print(\"‚úÖ Checkpoints encontrados:\\n\")\n",
    "    for ckpt in sorted(checkpoints):\n",
    "        ckpt_path = os.path.join(checkpoint_dir, ckpt)\n",
    "        size_mb = os.path.getsize(ckpt_path) / (1024 * 1024)\n",
    "        \n",
    "        # Extraer epoch\n",
    "        match = re.search(r'snapshot_(\\d+)', ckpt)\n",
    "        if match:\n",
    "            epoch = match.group(1)\n",
    "            print(f\"  ‚úì {ckpt} ({size_mb:.1f} MB)\")\n",
    "            print(f\"    ‚Üí Usa CHECKPOINT_EPOCH = {epoch}\")\n",
    "            \n",
    "            # Verificar contenido del checkpoint\n",
    "            try:\n",
    "                test_load = torch.load(ckpt_path, map_location='cpu', weights_only=False)\n",
    "                keys = list(test_load.keys())\n",
    "                print(f\"    ‚Üí Keys: {keys}\")\n",
    "                \n",
    "                if 'network' in test_load:\n",
    "                    # Contar par√°metros del modelo\n",
    "                    num_params = sum(p.numel() for p in test_load['network'].values() \n",
    "                                     if isinstance(p, torch.Tensor))\n",
    "                    print(f\"    ‚Üí ‚úÖ Formato v√°lido ({num_params:,} par√°metros)\")\n",
    "                else:\n",
    "                    print(f\"    ‚Üí ‚ö†Ô∏è  Falta key 'network'\")\n",
    "                    \n",
    "            except KeyError as e:\n",
    "                # KeyError con 'storages' es esperado - el modelo lo manejar√°\n",
    "                if 'storages' in str(e) or 'filename' in str(e):\n",
    "                    print(f\"    ‚Üí ‚ö†Ô∏è  KeyError (esperado para formato legacy)\")\n",
    "                    print(f\"    ‚Üí üí° El modelo cargar√° esto correctamente en base.py\")\n",
    "                else:\n",
    "                    print(f\"    ‚Üí ‚ùå Error inesperado: {type(e).__name__}: {str(e)[:50]}\")\n",
    "            except Exception as e:\n",
    "                print(f\"    ‚Üí ‚ùå Error al verificar: {type(e).__name__}: {str(e)[:50]}\")\n",
    "            print()\n",
    "else:\n",
    "    print(\"‚ùå No se encontraron checkpoints .pth.tar v√°lidos\\n\")\n",
    "    \n",
    "    # Diagnosticar problema\n",
    "    if os.path.exists(checkpoint_dir):\n",
    "        all_items = os.listdir(checkpoint_dir)\n",
    "        if all_items:\n",
    "            print(\"üìÇ Contenido de output/model_dump/:\")\n",
    "            for item in all_items[:15]:\n",
    "                item_path = os.path.join(checkpoint_dir, item)\n",
    "                if os.path.isdir(item_path):\n",
    "                    print(f\"    üìÅ {item}/ (directorio - no v√°lido)\")\n",
    "                else:\n",
    "                    size_mb = os.path.getsize(item_path) / (1024 * 1024)\n",
    "                    print(f\"    üìÑ {item} ({size_mb:.1f} MB)\")\n",
    "            \n",
    "            print(\"\\nüí° Soluci√≥n: Re-ejecuta la celda anterior de extracci√≥n\")\n",
    "        else:\n",
    "            print(\"üí° Directorio vac√≠o - verifica MODELS_DATASET_PATH\")\n",
    "    else:\n",
    "        print(\"üí° Soluci√≥n: Verifica que MODELS_DATASET_PATH es correcto\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üí° IMPORTANTE: Los checkpoints deben ser archivos .pth.tar\")\n",
    "print(\"   (TAR con formato legacy que PyTorch puede leer)\")\n",
    "print(\"=\"*60)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "177530d1",
   "metadata": {},
   "source": [
    "**‚ö†Ô∏è IMPORTANTE:** Ajusta el valor de `CHECKPOINT_EPOCH` en las siguientes celdas seg√∫n el epoch de tu checkpoint extra√≠do. En este caso detectamos `snapshot_83.pth`, as√≠ que usa `CHECKPOINT_EPOCH = 83`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f63761ce",
   "metadata": {},
   "source": [
    "## üöÄ PASO 4: Ejecutar Testing\n",
    "\n",
    "### Modelo L (Large)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58509a2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Diagn√≥stico: Verificar que todos los componentes est√°n listos\n",
    "import sys\n",
    "import os\n",
    "\n",
    "print(\"üîç Verificaci√≥n de Componentes:\\n\")\n",
    "\n",
    "# 1. Dataset - Verificar estructura DENTRO del proyecto\n",
    "project_root = '/kaggle/working/ConvNeXtPose'\n",
    "data_dir = os.path.join(project_root, 'data')\n",
    "h36m_path = os.path.join(data_dir, 'Human36M')\n",
    "\n",
    "print(f\"1. Dataset (estructura del proyecto):\")\n",
    "print(f\"   Project root: {project_root}\")\n",
    "print(f\"   data/ exists: {os.path.exists(data_dir)}\")\n",
    "print(f\"   data/dataset.py: {os.path.exists(os.path.join(data_dir, 'dataset.py'))}\")\n",
    "print(f\"   data/Human36M/ exists: {os.path.exists(h36m_path)}\")\n",
    "\n",
    "if os.path.exists(h36m_path):\n",
    "    print(f\"   - Human36M.py: {os.path.exists(os.path.join(h36m_path, 'Human36M.py'))}\")\n",
    "    print(f\"   - annotations: {os.path.exists(os.path.join(h36m_path, 'annotations'))}\")\n",
    "    print(f\"   - images/S9: {os.path.exists(os.path.join(h36m_path, 'images', 'S9'))}\")\n",
    "    print(f\"   - images/S11: {os.path.exists(os.path.join(h36m_path, 'images', 'S11'))}\")\n",
    "\n",
    "# 2. Checkpoints\n",
    "checkpoint_dir = os.path.join(project_root, 'output/model_dump')\n",
    "print(f\"\\n2. Checkpoints: {checkpoint_dir}\")\n",
    "if os.path.exists(checkpoint_dir):\n",
    "    checkpoints = [f for f in os.listdir(checkpoint_dir) if f.startswith('snapshot_')]\n",
    "    if checkpoints:\n",
    "        print(f\"   Disponibles: {', '.join(checkpoints)}\")\n",
    "        # Extraer epoch del checkpoint\n",
    "        import re\n",
    "        for ckpt in checkpoints:\n",
    "            match = re.search(r'snapshot_(\\d+)', ckpt)\n",
    "            if match:\n",
    "                epoch = match.group(1)\n",
    "                print(f\"   üí° Usa CHECKPOINT_EPOCH = {epoch} en la siguiente celda\")\n",
    "    else:\n",
    "        print(\"   ‚ö†Ô∏è  No se encontraron checkpoints\")\n",
    "else:\n",
    "    print(\"   ‚ùå Directorio no existe\")\n",
    "\n",
    "# 3. Estructura del proyecto\n",
    "print(f\"\\n3. Estructura del proyecto:\")\n",
    "critical_files = ['main/config.py', 'common/base.py', 'data/dataset.py', 'data/Human36M/Human36M.py']\n",
    "for file_path in critical_files:\n",
    "    full_path = os.path.join(project_root, file_path)\n",
    "    exists = os.path.exists(full_path)\n",
    "    status = \"‚úì\" if exists else \"‚ùå\"\n",
    "    print(f\"   {status} {file_path}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "if all(os.path.exists(os.path.join(project_root, f)) for f in critical_files):\n",
    "    print(\"‚úÖ Todos los checks pasaron - Listo para testing\")\n",
    "else:\n",
    "    print(\"‚ùå Algunos archivos faltan - Revisa la configuraci√≥n\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70c873e6",
   "metadata": {},
   "source": [
    "### Ejecutar Testing desde Python\n",
    "\n",
    "**Estructura correcta:**\n",
    "- ‚úÖ M√≥dulos Python originales en `data/` (dataset.py, Human36M.py)\n",
    "- ‚úÖ Dataset de Kaggle enlazado en `data/Human36M/` (images, annotations)\n",
    "- ‚úÖ `config.py` configura autom√°ticamente los paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6224a3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing con estructura correcta del proyecto\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# 1. Cambiar al directorio main (donde est√° config.py)\n",
    "os.chdir('/kaggle/working/ConvNeXtPose/main')\n",
    "\n",
    "# 2. Importar config PRIMERO - esto configura autom√°ticamente los paths\n",
    "from config import cfg\n",
    "\n",
    "# 3. Cargar variante ANTES de importar otros m√≥dulos\n",
    "VARIANT = 'L'  # Cambiar a 'M' para modelo Medium\n",
    "CHECKPOINT_EPOCH = 83  # ‚Üê AJUSTAR seg√∫n tu checkpoint\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"  Testing ConvNeXtPose-{VARIANT}\")\n",
    "print(f\"{'='*60}\\n\")\n",
    "\n",
    "cfg.load_variant_config(VARIANT)\n",
    "cfg.set_args('0')  # GPU 0\n",
    "\n",
    "# 4. AHORA importar los dem√°s m√≥dulos (config ya configur√≥ sys.path)\n",
    "import torch\n",
    "import torch.backends.cudnn as cudnn\n",
    "from base import Tester\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "cudnn.benchmark = True\n",
    "cudnn.deterministic = False\n",
    "cudnn.enabled = True\n",
    "\n",
    "# 5. Crear tester y ejecutar\n",
    "tester = Tester()\n",
    "tester._make_batch_generator()\n",
    "tester._make_model(CHECKPOINT_EPOCH)\n",
    "\n",
    "print(f\"\\nüöÄ Ejecutando testing en epoch {CHECKPOINT_EPOCH}...\\n\")\n",
    "\n",
    "preds = []\n",
    "with torch.no_grad():\n",
    "    for itr, input_img in enumerate(tqdm(tester.batch_generator)):\n",
    "        coord_out = tester.model(input_img)\n",
    "        \n",
    "        if cfg.flip_test:\n",
    "            from utils.pose_utils import flip\n",
    "            flipped_input_img = flip(input_img, dims=3)\n",
    "            flipped_coord_out = tester.model(flipped_input_img)\n",
    "            flipped_coord_out[:, :, 0] = cfg.output_shape[1] - flipped_coord_out[:, :, 0] - 1\n",
    "            for pair in tester.flip_pairs:\n",
    "                flipped_coord_out[:, pair[0], :], flipped_coord_out[:, pair[1], :] = \\\n",
    "                    flipped_coord_out[:, pair[1], :].clone(), flipped_coord_out[:, pair[0], :].clone()\n",
    "            coord_out = (coord_out + flipped_coord_out)/2.\n",
    "        \n",
    "        coord_out = coord_out.cpu().numpy()\n",
    "        preds.append(coord_out)\n",
    "\n",
    "# Evaluar\n",
    "preds = np.concatenate(preds, axis=0)\n",
    "print(f\"\\nüìä Evaluando {len(preds)} predicciones...\\n\")\n",
    "tester._evaluate(preds, cfg.result_dir)\n",
    "\n",
    "print(f\"\\n‚úÖ Testing completado!\")\n",
    "print(f\"üìÇ Resultados guardados en: {cfg.result_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86f7ffa5",
   "metadata": {},
   "source": [
    "### Modelo M (Medium) - Opcional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8944d98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Si tienes el checkpoint del modelo M, ejecuta esto:\n",
    "# !python test.py --gpu 0 --epochs {CHECKPOINT_EPOCH} --variant M"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1df8cdae",
   "metadata": {},
   "source": [
    "## üìä PASO 5: Verificar Resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8200f20f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ver resultados generados\n",
    "%cd ..\n",
    "!ls -lh output/result/\n",
    "\n",
    "# Leer log de resultados\n",
    "import glob\n",
    "log_files = glob.glob('output/log/*.log')\n",
    "if log_files:\n",
    "    latest_log = max(log_files, key=os.path.getctime)\n",
    "    print(f\"\\nüìÑ √öltimas l√≠neas del log ({os.path.basename(latest_log)}):\")\n",
    "    !tail -n 20 {latest_log}\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  No se encontraron logs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14f8e2af",
   "metadata": {},
   "source": [
    "## üìà PASO 6: An√°lisis de Resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "145e14d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def extract_mpjpe_from_log(log_path):\n",
    "    \"\"\"Extrae el MPJPE del log\"\"\"\n",
    "    with open(log_path, 'r') as f:\n",
    "        content = f.read()\n",
    "    \n",
    "    # Buscar patr√≥n de MPJPE\n",
    "    pattern = r'MPJPE.*?(\\d+\\.\\d+)'\n",
    "    matches = re.findall(pattern, content)\n",
    "    \n",
    "    if matches:\n",
    "        return float(matches[-1])  # √öltimo valor\n",
    "    return None\n",
    "\n",
    "# Extraer resultados\n",
    "log_files = glob.glob('output/log/*.log')\n",
    "if log_files:\n",
    "    latest_log = max(log_files, key=os.path.getctime)\n",
    "    mpjpe = extract_mpjpe_from_log(latest_log)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"  üìä RESULTADOS FINALES\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    if mpjpe:\n",
    "        print(f\"\\n  MPJPE (Protocol 2): {mpjpe:.2f} mm\")\n",
    "        \n",
    "        # Comparar con paper\n",
    "        expected = {\n",
    "            'L': 42.3,\n",
    "            'M': 44.6\n",
    "        }\n",
    "        \n",
    "        # Determinar variante\n",
    "        for variant, expected_val in expected.items():\n",
    "            diff = abs(mpjpe - expected_val)\n",
    "            if diff < 5:\n",
    "                print(f\"\\n  Variante detectada: {variant}\")\n",
    "                print(f\"  Valor del paper: {expected_val:.2f} mm\")\n",
    "                print(f\"  Diferencia: {mpjpe - expected_val:+.2f} mm\")\n",
    "                \n",
    "                if diff < 2:\n",
    "                    print(\"  ‚úÖ Resultado excelente (dentro de ¬±2mm)\")\n",
    "                elif diff < 5:\n",
    "                    print(\"  ‚úì Resultado aceptable (dentro de ¬±5mm)\")\n",
    "                break\n",
    "    else:\n",
    "        print(\"\\n‚ö†Ô∏è  No se pudo extraer MPJPE del log\")\n",
    "        print(\"Revisa el log manualmente en output/log/\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "else:\n",
    "    print(\"‚ùå No se encontraron logs de testing\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76e80fd2",
   "metadata": {},
   "source": [
    "## üíæ PASO 7: Guardar Outputs\n",
    "\n",
    "Kaggle guarda autom√°ticamente todo en `/kaggle/working/`. Opcionalmente puedes copiar resultados espec√≠ficos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2acfddb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear resumen de resultados\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "summary = {\n",
    "    'timestamp': datetime.now().isoformat(),\n",
    "    'model': 'ConvNeXtPose-L',  # Cambiar seg√∫n modelo testeado\n",
    "    'checkpoint_epoch': CHECKPOINT_EPOCH,\n",
    "    'dataset': 'Human3.6M Protocol 2',\n",
    "    'mpjpe_mm': mpjpe if 'mpjpe' in locals() else None,\n",
    "    'pytorch_version': torch.__version__,\n",
    "    'cuda_version': torch.version.cuda if torch.cuda.is_available() else None\n",
    "}\n",
    "\n",
    "# Guardar resumen\n",
    "with open('output/result/test_summary.json', 'w') as f:\n",
    "    json.dump(summary, f, indent=2)\n",
    "\n",
    "print(\"‚úì Resumen guardado en output/result/test_summary.json\")\n",
    "print(\"\\nüìÑ Contenido:\")\n",
    "print(json.dumps(summary, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23f63b45",
   "metadata": {},
   "source": [
    "## üéâ Testing Completado!\n",
    "\n",
    "Los resultados est√°n en:\n",
    "- **Logs**: `output/log/`\n",
    "- **Resultados**: `output/result/`\n",
    "- **Resumen JSON**: `output/result/test_summary.json`\n",
    "\n",
    "Todos los archivos en `/kaggle/working/ConvNeXtPose/output/` se guardar√°n autom√°ticamente cuando hagas commit del notebook."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
